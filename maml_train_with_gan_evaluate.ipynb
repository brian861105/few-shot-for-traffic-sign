{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5929202-b7f6-4f82-8ed8-a4970ebe46ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch, os\n",
    "import  numpy as np\n",
    "import  scipy.stats\n",
    "from    torch.utils.data import DataLoader\n",
    "from    torch.optim import lr_scheduler\n",
    "import  random, sys, pickle\n",
    "from utils.dataloader import train_data_gen , test_data_gen\n",
    "# from maml_meta import Meta\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from    torch import optim\n",
    "from maml_learner import Learner\n",
    "from    torch.nn import functional as F\n",
    "from    copy import deepcopy\n",
    "import json\n",
    "import shutil\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c7c1e33-f4eb-45d4-ae1c-23b001d0f7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 60000, 'n_way': 5, 'k_spt': 1, 'k_qry': 15, 'img_sz': 84, 'task_num': 5, 'img_c': 3, 'meta_lr': 0.001, 'update_lr': 0.01, 'update_step': 5, 'update_step_test': 10, 'loss': 'cross_entropy', 'min_learning_rate': 1e-15, 'number_of_training_steps_per_iter': 4, 'multi_step_loss_num_epochs': 15, 'spy_gan_num': 1, 'qry_gan_num': 5, 'num_distractor': 2, 'gan': 1, 'spy_distractor_num': 1, 'qry_distractor_num': 5, 'batch_for_gradient': 25, 'no_save': 0, 'learn_inner_lr': 0, 'create_graph': 0, 'msl': 0, 'single_fast_test': 0, 'consine_schedule': 0, 'save_path': '0704_5way1shot2distractor1gan_evaluation'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"maml_configs/0704_5way1shot2distractor1gan.json\") as json_file:\n",
    "    args = json.load(json_file)\n",
    "\n",
    "args[\"save_path\"] = args[\"save_path\"] + \"_evaluation\"\n",
    "print(args)\n",
    "writer = SummaryWriter(\"maml_runs/\" + args[\"save_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b615ddc1-6288-4689-b01e-c0ee0b47913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "        \n",
    "    if not os.path.exists(\"maml/\" + path):\n",
    "        os.makedirs(\"maml/\" + path)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a93a30ed-828a-4aed-a583-28a04069d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spt_size = args[\"k_spt\"] * args[\"n_way\"]\n",
    "qry_size = args[\"k_qry\"] * args[\"n_way\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e01d98-8bbb-4081-aaab-09f315aabeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = 42\n",
    "config = [\n",
    "    (\"conv2d\", [fm, 3, 3, 3, 1, 0]),\n",
    "    (\"relu\", [True]),\n",
    "    (\"bn\", [fm]),\n",
    "    (\"max_pool2d\", [2, 2, 0]),\n",
    "    (\"conv2d\", [fm, fm, 3, 3, 1, 0]),\n",
    "    (\"relu\", [True]),\n",
    "    (\"bn\", [fm]),\n",
    "    (\"max_pool2d\", [2, 2, 0]),\n",
    "    (\"conv2d\", [fm, fm, 3, 3, 1, 0]),\n",
    "    (\"relu\", [True]),\n",
    "    (\"bn\", [fm]),\n",
    "    (\"max_pool2d\", [2, 2, 0]),\n",
    "    (\"conv2d\", [fm, fm, 3, 3, 1, 0]),\n",
    "    (\"relu\", [True]),\n",
    "    (\"bn\", [fm]),\n",
    "    (\"max_pool2d\", [2, 1, 0]),\n",
    "    (\"flatten\", []),\n",
    "    (\"linear\", [6, fm * 5 * 5])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "569b93b1-04d0-4fb3-9842-ba36ce39271d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load datasets/BelgiumTSC\n",
      "load complete time 0.4219205379486084\n",
      "load datasets/ArTS\n",
      "load complete time 0.3905467987060547\n",
      "load datasets/chinese_traffic_sign\n",
      "load complete time 0.7185690402984619\n",
      "load datasets/CVL\n",
      "load complete time 0.5696113109588623\n",
      "load datasets/FullJCNN2013\n",
      "load complete time 0.27639055252075195\n",
      "load datasets/logo_2k\n",
      "load complete time 1.0268397331237793\n",
      "load datasets/GTSRB\n",
      "load complete time 0.09910392761230469\n",
      "load datasets/DFG\n",
      "load complete time 0.03455376625061035\n"
     ]
    }
   ],
   "source": [
    "train_data_generator = train_data_gen(args)\n",
    "test_data_generator = test_data_gen(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "566a21e6-8f21-4bdb-b221-e7346562648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "nc = 3\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "if args[\"gan\"]:\n",
    "    netG = Generator().to(device)\n",
    "    netG.load_state_dict(torch.load(\"GAN_save_models/generator/fixed/model_step_15.pt\")[\"model_state_dict\"])\n",
    "    # netG.apply(weights_init)\n",
    "    # summary(netG,(nz,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a76092fc-8af5-4786-973d-806b3b28942f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Meta(nn.Module):\n",
    "    \"\"\"\n",
    "    Meta Learner\n",
    "    \"\"\"\n",
    "    def __init__(self, args, config):\n",
    "        \"\"\"\n",
    "\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(Meta, self).__init__()\n",
    "\n",
    "        self.update_lr = args[\"update_lr\"]\n",
    "        self.meta_lr = args[\"meta_lr\"]\n",
    "        self.n_way = args[\"n_way\"]\n",
    "        self.k_spt = args[\"k_spt\"]\n",
    "        self.k_qry = args[\"k_qry\"]\n",
    "        self.task_num = args[\"task_num\"]\n",
    "        self.update_step = args[\"update_step\"]\n",
    "        self.update_step_test = args[\"update_step_test\"]\n",
    "        self.distractor = args[\"num_distractor\"]\n",
    "        self.net = Learner(config, args[\"img_c\"], args[\"img_sz\"])\n",
    "        self.meta_optim = optim.Adam(self.net.parameters(), lr=self.meta_lr)\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.gan = args[\"gan\"]\n",
    "    def forward(self, x_spt, y_spt, x_qry, y_qry, unlabel_spt_image=None, unlabel_qry_image=None,gan_spt=None, gan_qry=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x_spt:   [b, setsz, c_, h, w]\n",
    "        :param y_spt:   [b, setsz]\n",
    "        :param x_qry:   [b, querysz, c_, h, w]\n",
    "        :param y_qry:   [b, querysz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        task_num, setsz, c_, h, w = x_spt.size()\n",
    "        querysz = x_qry.size(1)\n",
    "        if self.gan :\n",
    "            gan_sptsz = gan_spt.size(1)\n",
    "            gan_qrysz = gan_qry.size(1)\n",
    "        \n",
    "        else:\n",
    "            gan_qrysz = 0\n",
    "        if self.distractor or gan_sptsz:\n",
    "            corrects = {}\n",
    "            corrects[\"total_query_nway\"] = np.zeros(self.update_step + 1)\n",
    "            if self.distractor:\n",
    "                unlabel_querysz = unlabel_qry.size(1)\n",
    "                corrects[\"query_nway_recall\"] = np.zeros(self.update_step + 1)\n",
    "                corrects[\"label_query_nway_recall\"] = np.zeros(self.update_step + 1)\n",
    "                corrects[\"distractor_query_nway_recall\"] = np.zeros(self.update_step + 1)\n",
    "            if self.gan :\n",
    "                corrects[\"gan_query_nway\"] = np.zeros(self.update_step + 1)\n",
    "        else:\n",
    "            corrects = {key: np.zeros(self.update_step + 1) for key in \n",
    "                [\n",
    "                \"query_nway_recall\",\n",
    "                \"label_query_nway_recall\"\n",
    "                ]}\n",
    "        losses_q = [0 for _ in range(self.update_step + 1)]  # losses_q[i] is the loss on step i\n",
    "\n",
    "        for i in range(task_num):\n",
    "            spt_image = x_spt[i]\n",
    "            spt_label = y_spt[i]\n",
    "            qry_image = x_qry[i]\n",
    "            qry_label = y_qry[i]\n",
    "            if self.distractor:\n",
    "                spt_image = torch.concat((spt_image,unlabel_spt[i]))\n",
    "                spt_unlabel_label = torch.full((unlabel_spt.size(1),), 5, dtype=torch.long,device=self.device)\n",
    "                spt_label = torch.cat((spt_label,spt_unlabel_label))\n",
    "                qry_image = torch.concat((qry_image,unlabel_qry[i]))\n",
    "                qry_unlabel_label = torch.full((unlabel_qry.size(1),), 5, dtype=torch.long,device=self.device)\n",
    "                qry_label = torch.cat((qry_label,qry_unlabel_label))\n",
    "            if self.gan :\n",
    "                spt_image = torch.concat((spt_image,gan_spt[i]))\n",
    "                spt_gan_label = torch.full((gan_spt.size(1),), 5, dtype=torch.long,device=self.device)\n",
    "                spt_label = torch.cat((spt_label,spt_gan_label))\n",
    "                qry_image = torch.concat((qry_image,gan_qry[i]))\n",
    "                qry_gan_label = torch.full((gan_qry.size(1),), 5, dtype=torch.long,device=self.device)\n",
    "                qry_label = torch.cat((qry_label,qry_gan_label))\n",
    "\n",
    "            # 1. run the i-th task and compute loss for k=0\n",
    "            logits = self.net(spt_image, vars=None, bn_training=True)\n",
    "            loss = F.cross_entropy(logits, spt_label)\n",
    "            grad = torch.autograd.grad(loss, self.net.parameters())\n",
    "            fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, self.net.parameters())))\n",
    "\n",
    "            # this is the loss and accuracy before first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                if self.distractor or gan_sptsz:\n",
    "                    total_logits_q = self.net(qry_image, self.net.parameters(), bn_training=False)\n",
    "                    total_pred_q = F.softmax(total_logits_q, dim=1).argmax(dim=1)\n",
    "                    total_q_correct = torch.eq(total_pred_q, qry_label).sum().item()\n",
    "                    corrects[\"total_query_nway\"][0] += total_q_correct\n",
    "                    loss_q = F.cross_entropy(total_logits_q, qry_label)\n",
    "                    losses_q[0] += loss_q\n",
    "                    if self.distractor:\n",
    "                        label_logits_q = self.net(x_qry[i], self.net.parameters(), bn_training=False)\n",
    "                        label_pred_q = F.softmax(label_logits_q, dim=1).argmax(dim=1)\n",
    "                        label_pred_q_correct = torch.eq(label_pred_q, y_qry[i]).sum().item()\n",
    "                        corrects[\"label_query_nway_recall\"][0] += label_pred_q_correct\n",
    "                        label_pred_q = F.softmax(label_logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                        label_pred_q_correct = torch.eq(label_pred_q, y_qry[i]).sum().item()\n",
    "                        corrects[\"query_nway_recall\"][0] += label_pred_q_correct\n",
    "                        \n",
    "\n",
    "                        unlabel_logits_q = self.net(unlabel_qry[i], self.net.parameters(), bn_training=False)\n",
    "                        unlabel_pred_q = F.softmax(unlabel_logits_q, dim=1).argmax(dim=1)\n",
    "                        other = torch.eq(unlabel_pred_q, qry_unlabel_label).sum().item()\n",
    "                        corrects[\"distractor_query_nway_recall\"][0] += other\n",
    "                    if self.gan :\n",
    "                        gan_logits_q = self.net(gan_qry[i], self.net.parameters(), bn_training=False)\n",
    "                        gan_pred_q = F.softmax(gan_logits_q, dim=1).argmax(dim=1)\n",
    "                        gan_counts = torch.eq(gan_pred_q, qry_gan_label).sum().item()\n",
    "                        corrects[\"gan_query_nway_recall\"][0] += gan_counts\n",
    "                else:\n",
    "                    logits_q = self.net(qry_image, self.net.parameters(), bn_training=True)\n",
    "                    loss_q = F.cross_entropy(logits_q, qry_label)\n",
    "                    pred_q = F.softmax(logits_q[:,-1], dim=1).argmax(dim=1)\n",
    "                    q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                    corrects[\"query_nway_recall\"][0] += q_discrim_correct\n",
    "                    pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                    q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                    corrects[\"label_query_nway_recall\"][0] += q_discrim_correct\n",
    "                    losses_q[0] += loss_q\n",
    "            # this is the loss and accuracy after the first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                if self.distractor or gan_sptsz:\n",
    "                    \n",
    "                    total_logits_q = self.net(qry_image,fast_weights , bn_training=False)\n",
    "                    total_pred_q = F.softmax(total_logits_q, dim=1).argmax(dim=1)\n",
    "                    total_q_correct = torch.eq(total_pred_q, qry_label).sum().item()\n",
    "                    corrects[\"total_query_nway\"][1] += total_q_correct\n",
    "                    loss_q = F.cross_entropy(total_logits_q, qry_label)\n",
    "                    losses_q[1] += loss_q\n",
    "                    if self.distractor:\n",
    "                        label_logits_q = self.net(x_qry[i], fast_weights, bn_training=False)\n",
    "                        label_pred_q = F.softmax(label_logits_q, dim=1).argmax(dim=1)\n",
    "                        label_pred_q_correct = torch.eq(label_pred_q, y_qry[i]).sum().item()\n",
    "                        corrects[\"label_query_nway_recall\"][1] += label_pred_q_correct\n",
    "                        label_pred_q = F.softmax(label_logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                        label_pred_q_correct = torch.eq(label_pred_q, y_qry[i]).sum().item()\n",
    "                        corrects[\"query_nway_recall\"][1] += label_pred_q_correct\n",
    "                        \n",
    "                        unlabel_logits_q = self.net(unlabel_qry[i], fast_weights, bn_training=False)\n",
    "                        unlabel_pred_q = F.softmax(unlabel_logits_q, dim=1).argmax(dim=1)\n",
    "                        other = torch.eq(unlabel_pred_q, qry_unlabel_label).sum().item()\n",
    "                        corrects[\"distractor_query_nway_recall\"][1] += other\n",
    "                    if self.gan :\n",
    "                        gan_logits_q = self.net(gan_qry[i], fast_weights, bn_training=False)\n",
    "                        gan_pred_q = F.softmax(gan_logits_q, dim=1).argmax(dim=1)\n",
    "                        gan_counts = torch.eq(gan_pred_q, qry_gan_label).sum().item()\n",
    "                        corrects[\"gan_query_nway_recall\"][1] += gan_counts\n",
    "                else:\n",
    "                    logits_q = self.net(qry_image, fast_weights, bn_training=False)\n",
    "                    loss_q = F.cross_entropy(logits_q, qry_label)\n",
    "                    pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                    q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                    corrects[\"query_nway_recall\"][1] += q_discrim_correct\n",
    "                    pred_q = F.softmax(logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                    q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                    corrects[\"label_query_nway_recall\"][1] += q_discrim_correct\n",
    "                    losses_q[1] += loss_q\n",
    "\n",
    "            for k in range(1, self.update_step):\n",
    "                # 1. run the i-th task and compute loss for k=1~K-1\n",
    "                logits = self.net(spt_image, fast_weights, bn_training=True)\n",
    "                loss = F.cross_entropy(logits, spt_label)\n",
    "                # 2. compute grad on theta_pi\n",
    "                grad = torch.autograd.grad(loss, fast_weights)\n",
    "                # 3. theta_pi = theta_pi - train_lr * grad\n",
    "                fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "                logits_q = self.net(qry_image, fast_weights, bn_training=True)\n",
    "                # loss_q will be overwritten and just keep the loss_q on last update step.\n",
    "                loss_q = F.cross_entropy(logits_q, qry_label)\n",
    "                losses_q[k + 1] += loss_q\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if self.distractor or gan_sptsz:\n",
    "                        total_logits_q = self.net(qry_image, fast_weights, bn_training=False)\n",
    "                        total_pred_q = F.softmax(total_logits_q, dim=1).argmax(dim=1)\n",
    "                        total_q_correct = torch.eq(total_pred_q, qry_label).sum().item()\n",
    "                        corrects[\"total_query_nway\"][k+1] += total_q_correct\n",
    "                        if self.distractor:\n",
    "                            label_logits_q = self.net(x_qry[i], fast_weights, bn_training=False)\n",
    "                            label_pred_q = F.softmax(label_logits_q, dim=1).argmax(dim=1)\n",
    "                            label_pred_q_correct = torch.eq(label_pred_q, y_qry[i]).sum().item()\n",
    "                            corrects[\"label_query_nway\"][k+1] += label_pred_q_correct\n",
    "                            label_pred_q = F.softmax(label_logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                            label_pred_q_correct = torch.eq(label_pred_q, y_qry[i]).sum().item()\n",
    "                            corrects[\"query_nway_recall\"][k+1] += label_pred_q_correct\n",
    "                            \n",
    "                            unlabel_logits_q = self.net(unlabel_qry[i], fast_weights, bn_training=False)\n",
    "                            unlabel_pred_q = F.softmax(unlabel_logits_q, dim=1).argmax(dim=1)\n",
    "                            other = torch.eq(unlabel_pred_q, qry_unlabel_label).sum().item()\n",
    "                            corrects[\"distractor_query_nway_recall\"][k+1] += other\n",
    "                        if self.gan :\n",
    "                            gan_logits_q = self.net(gan_qry[i], fast_weights, bn_training=False)\n",
    "                            gan_pred_q = F.softmax(gan_logits_q, dim=1).argmax(dim=1)\n",
    "                            gan_counts = torch.eq(gan_pred_q, qry_gan_label).sum().item()\n",
    "                            corrects[\"gan_query_nway\"][k+1] += gan_counts\n",
    "                    else:\n",
    "                        pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                        q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                        corrects[\"query_nway_recall\"][k+1] += q_discrim_correct\n",
    "                        pred_q = F.softmax(logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                        q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                        corrects[\"label_query_nway_recall\"][k+1] += q_discrim_correct\n",
    "\n",
    "        # end of all tasks\n",
    "        # sum over all losses on query set across all tasks\n",
    "        loss_q = losses_q[-1] / task_num\n",
    "        # optimize theta parameters\n",
    "        self.meta_optim.zero_grad()\n",
    "        loss_q.backward()\n",
    "\n",
    "        self.meta_optim.step()\n",
    "        \n",
    "        accs = {}\n",
    "        if (self.distractor or self.gan):\n",
    "            accs[\"total_query_nway\"] = corrects[\"total_query_nway\"] / (task_num * (querysz + unlabel_querysz + gan_qrysz))\n",
    "            if self.distractor:\n",
    "                accs[\"label_query_nway_recall\"] = corrects[\"label_query_nway_recall\"] / (task_num * querysz)\n",
    "                accs[\"query_nway_recall\"] = corrects[\"query_nway_recall\"] / (task_num * querysz)\n",
    "                accs[\"distractor_query_nway_recall\"] = corrects[\"distractor_query_nway_recall\"] / (task_num * unlabel_querysz)\n",
    "            if gan_qrysz:\n",
    "                accs[\"gan_query_nway\"] = corrects[\"gan_query_nway\"] / (task_num * gan_qrysz)\n",
    "        else:\n",
    "            accs[\"query_nway_recall\"] = corrects[\"query_nway_recall\"] / (task_num * querysz)\n",
    "            accs[\"label_query_nway_recall\"] = corrects[\"label_query_nway_recall\"] / (task_num * querysz)\n",
    "        return accs,loss_q\n",
    "\n",
    "\n",
    "    def finetunning(self, x_spt, y_spt, x_qry, y_qry, unlabel_spt=None, unlabel_qry=None, gan_spt=None, gan_qry=None):\n",
    "\n",
    "        assert len(x_spt.shape) == 4\n",
    "        querysz = x_qry.size(0)\n",
    "        if self.gan :\n",
    "            gan_sptsz = gan_spt.size(0)\n",
    "            gan_qrysz = gan_qry.size(0)\n",
    "        else:\n",
    "            gan_sptsz = gan_qrysz = 0\n",
    "        if self.distractor or self.gan:\n",
    "            corrects = {}\n",
    "            corrects[\"total_query_nway\"] = np.zeros(self.update_step_test + 1)\n",
    "            if self.distractor:\n",
    "                unlabel_querysz = unlabel_qry.size(0)\n",
    "                corrects[\"label_query_nway_recall\"] = np.zeros(self.update_step_test + 1)\n",
    "                corrects[\"query_nway_recall\"] = np.zeros(self.update_step_test + 1)\n",
    "                corrects[\"distractor_query_nway_recall\"] = np.zeros(self.update_step_test + 1)\n",
    "            if self.gan:\n",
    "                corrects[\"gan_query_nway\"] = np.zeros(self.update_step_test + 1)\n",
    "        else:\n",
    "            corrects = {key: np.zeros(self.update_step_test + 1) for key in \n",
    "                            [\n",
    "                            \"query_nway_recall\",\n",
    "                            \"label_query_nway_recall\"\n",
    "                            ]}\n",
    "        # in order to not ruin the state of running_mean/variance and bn_weight/bias\n",
    "        # we finetunning on the copied model instead of self.net\n",
    "        net = deepcopy(self.net)\n",
    "        spt_image = x_spt\n",
    "        spt_label = y_spt\n",
    "        qry_image = x_qry\n",
    "        qry_label = y_qry\n",
    "        if self.distractor:\n",
    "            spt_image = torch.concat((spt_image,unlabel_spt))\n",
    "            spt_unlabel_label = torch.full((unlabel_spt.size(0),), 5, dtype=torch.long,device=self.device)\n",
    "            spt_label = torch.cat((spt_label,spt_unlabel_label))\n",
    "            qry_image = torch.concat((qry_image,unlabel_qry))\n",
    "            qry_unlabel_label = torch.full((unlabel_qry.size(0),), 5, dtype=torch.long,device=self.device)\n",
    "            qry_label = torch.cat((qry_label,qry_unlabel_label))\n",
    "        if self.gan :\n",
    "            spt_image = torch.concat((spt_image,gan_spt))\n",
    "            spt_gan_label = torch.full((gan_spt.size(0),), 5, dtype=torch.long,device=self.device)\n",
    "            spt_label = torch.cat((spt_label,spt_gan_label))\n",
    "            qry_image = torch.concat((qry_image,gan_qry))\n",
    "            qry_gan_label = torch.full((gan_qry.size(0),), 5, dtype=torch.long,device=self.device)\n",
    "            qry_label = torch.cat((qry_label,qry_gan_label))\n",
    "        \n",
    "        # 1. run the i-th task and compute loss for k=0\n",
    "        logits = net(spt_image)\n",
    "        loss = F.cross_entropy(logits, spt_label)\n",
    "\n",
    "        grad = torch.autograd.grad(loss, net.parameters())\n",
    "        fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, net.parameters())))\n",
    "\n",
    "        # this is the loss and accuracy before first update\n",
    "        with torch.no_grad():\n",
    "            if self.distractor or gan_sptsz:\n",
    "                total_logits_q = self.net(qry_image, self.net.parameters(), bn_training=False)\n",
    "                total_pred_q = F.softmax(total_logits_q, dim=1).argmax(dim=1)\n",
    "                total_q_correct = torch.eq(total_pred_q, qry_label).sum().item()\n",
    "                corrects[\"total_query_nway\"][0] += total_q_correct\n",
    "                loss_q = F.cross_entropy(total_logits_q, qry_label)\n",
    "                if self.distractor:\n",
    "                    label_logits_q = self.net(x_qry, self.net.parameters(), bn_training=False)\n",
    "                    label_pred_q = F.softmax(label_logits_q, dim=1).argmax(dim=1)\n",
    "                    label_pred_q_correct = torch.eq(label_pred_q, y_qry).sum().item()\n",
    "                    corrects[\"label_query_nway_recall\"][0] += label_pred_q_correct\n",
    "                    \n",
    "                    label_logits_q = self.net(x_qry, self.net.parameters(), bn_training=False)\n",
    "                    label_pred_q = F.softmax(label_logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                    label_pred_q_correct = torch.eq(label_pred_q, y_qry).sum().item()\n",
    "                    corrects[\"query_nway_recall\"][0] += label_pred_q_correct\n",
    "                    \n",
    "                    unlabel_logits_q = self.net(unlabel_qry, self.net.parameters(), bn_training=False)\n",
    "                    unlabel_pred_q = F.softmax(unlabel_logits_q, dim=1).argmax(dim=1)\n",
    "                    other = torch.eq(unlabel_pred_q, qry_unlabel_label).sum().item()\n",
    "                    corrects[\"distractor_query_nway_recall\"][0] += other\n",
    "                if self.gan :\n",
    "                    gan_logits_q = self.net(gan_qry, self.net.parameters(), bn_training=False)\n",
    "                    gan_pred_q = F.softmax(gan_logits_q, dim=1).argmax(dim=1)\n",
    "                    gan_counts = torch.eq(gan_pred_q, qry_gan_label).sum().item()\n",
    "                    corrects[\"gan_query_nway\"][0] += gan_counts\n",
    "            else:\n",
    "                logits_q = self.net(qry_image, self.net.parameters(), bn_training=True)\n",
    "                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                corrects[\"query_nway_recall\"][0] += q_discrim_correct\n",
    "                pred_q = F.softmax(logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                corrects[\"label_query_nway_recall\"][0] += q_discrim_correct\n",
    "        # this is the loss and accuracy after the first update\n",
    "        with torch.no_grad():\n",
    "            if self.distractor or gan_sptsz:\n",
    "\n",
    "                total_logits_q = self.net(qry_image,fast_weights , bn_training=False)\n",
    "                total_pred_q = F.softmax(total_logits_q, dim=1).argmax(dim=1)\n",
    "                total_q_correct = torch.eq(total_pred_q, qry_label).sum().item()\n",
    "                corrects[\"total_query_nway\"][1] += total_q_correct\n",
    "                loss_q = F.cross_entropy(total_logits_q, qry_label)\n",
    "                if self.distractor:\n",
    "                    label_logits_q = self.net(x_qry, fast_weights, bn_training=False)\n",
    "                    label_pred_q = F.softmax(label_logits_q, dim=1).argmax(dim=1)\n",
    "                    label_pred_q_correct = torch.eq(label_pred_q, y_qry).sum().item()\n",
    "                    corrects[\"label_query_nway_recall\"][1] += label_pred_q_correct\n",
    "                    \n",
    "                    label_logits_q = self.net(x_qry, fast_weights, bn_training=False)\n",
    "                    label_pred_q = F.softmax(label_logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                    label_pred_q_correct = torch.eq(label_pred_q, y_qry).sum().item()\n",
    "                    corrects[\"query_nway_recall\"][1] += label_pred_q_correct\n",
    "                    \n",
    "                    unlabel_logits_q = self.net(unlabel_qry, fast_weights, bn_training=False)\n",
    "                    unlabel_pred_q = F.softmax(unlabel_logits_q, dim=1).argmax(dim=1)\n",
    "                    other = torch.eq(unlabel_pred_q, qry_unlabel_label).sum().item()\n",
    "                    corrects[\"distractor_query_nway_recall\"][1] += other\n",
    "                if self.gan :\n",
    "                    gan_logits_q = self.net(gan_qry, fast_weights, bn_training=False)\n",
    "                    gan_pred_q = F.softmax(gan_logits_q, dim=1).argmax(dim=1)\n",
    "                    gan_counts = torch.eq(gan_pred_q, qry_gan_label).sum().item()\n",
    "                    corrects[\"gan_query_nway\"][1] += gan_counts\n",
    "            else:\n",
    "                logits_q = self.net(qry_image, fast_weights, bn_training=True)\n",
    "                \n",
    "                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                corrects[\"query_nway_recall\"][1] += q_discrim_correct\n",
    "                pred_q = F.softmax(logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                corrects[\"label_query_nway_recall\"][1] += q_discrim_correct\n",
    "        for k in range(1, self.update_step_test):\n",
    "            # 1. run the i-th task and compute loss for k=1~K-1\n",
    "            logits = net(spt_image, fast_weights, bn_training=True)\n",
    "            loss = F.cross_entropy(logits, spt_label)\n",
    "            # 2. compute grad on theta_pi\n",
    "            grad = torch.autograd.grad(loss, fast_weights)\n",
    "            # 3. theta_pi = theta_pi - train_lr * grad\n",
    "            fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "            logits_q = net(qry_image, fast_weights, bn_training=True)\n",
    "            # loss_q will be overwritten and just keep the loss_q on last update step.\n",
    "            loss_q = F.cross_entropy(logits_q, qry_label)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if self.distractor or gan_sptsz:\n",
    "                    total_logits_q = self.net(qry_image, fast_weights, bn_training=False)\n",
    "                    total_pred_q = F.softmax(total_logits_q, dim=1).argmax(dim=1)\n",
    "                    total_q_correct = torch.eq(total_pred_q, qry_label).sum().item()\n",
    "                    print(qry_image.size(),len(total_pred_q),total_pred_q,qry_label,F.softmax(total_logits_q, dim=1))\n",
    "                    corrects[\"total_query_nway\"][k+1] += total_q_correct\n",
    "                    if self.distractor:\n",
    "                        label_logits_q = self.net(x_qry, fast_weights, bn_training=False)\n",
    "                        label_pred_q = F.softmax(label_logits_q, dim=1).argmax(dim=1)\n",
    "                        label_pred_q_correct = torch.eq(label_pred_q, y_qry).sum().item()\n",
    "                        corrects[\"label_query_nway_recall\"][k+1] += label_pred_q_correct\n",
    "                        \n",
    "                        label_logits_q = self.net(x_qry, fast_weights, bn_training=False)\n",
    "                        label_pred_q = F.softmax(label_logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                        label_pred_q_correct = torch.eq(label_pred_q, y_qry).sum().item()\n",
    "                        corrects[\"query_nway_recall\"][k+1] += label_pred_q_correct\n",
    "\n",
    "                        unlabel_logits_q = self.net(unlabel_qry, fast_weights, bn_training=False)\n",
    "                        unlabel_pred_q = F.softmax(unlabel_logits_q, dim=1).argmax(dim=1)\n",
    "                        other = torch.eq(unlabel_pred_q, qry_unlabel_label).sum().item()\n",
    "                        corrects[\"distractor_query_nway_recall\"][k+1] += other\n",
    "                    if self.gan :\n",
    "                        gan_logits_q = self.net(gan_qry, fast_weights, bn_training=False)\n",
    "                        gan_pred_q = F.softmax(gan_logits_q, dim=1).argmax(dim=1)\n",
    "                        gan_counts = torch.eq(gan_pred_q, qry_gan_label).sum().item()\n",
    "                        corrects[\"gan_query_nway\"][k+1] += gan_counts\n",
    "                else:\n",
    "                    logits_q = self.net(qry_image, fast_weights, bn_training=True)\n",
    "                    pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                    q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                    corrects[\"query_nway_recall\"][k+1] += q_discrim_correct\n",
    "                    pred_q = F.softmax(logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                    q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                    corrects[\"label_query_nway_recall\"][k+1] += q_discrim_correct\n",
    "        \n",
    "        del net\n",
    "        accs = {}\n",
    "        if (self.distractor or self.gan):\n",
    "            # print(querysz,unlabel_querysz,gan_qrysz)\n",
    "            accs[\"total_query_nway\"] = corrects[\"total_query_nway\"] / (querysz + unlabel_querysz + gan_qrysz)\n",
    "            if self.distractor:\n",
    "                accs[\"label_query_nway_recall\"] = corrects[\"label_query_nway_recall\"] / querysz\n",
    "                accs[\"query_nway_recall\"] = corrects[\"query_nway_recall\"] / querysz\n",
    "                accs[\"distractor_query_nway_recall\"] = corrects[\"distractor_query_nway_recall\"] / (unlabel_querysz)\n",
    "            if self.gan:\n",
    "                accs[\"gan_query_nway\"] = corrects[\"gan_query_nway\"] / (gan_qrysz)\n",
    "        else:\n",
    "            accs[\"query_nway_recall\"] = corrects[\"query_nway_recall\"] / querysz\n",
    "            accs[\"label_query_nway_recall\"] = corrects[\"label_query_nway_recall\"] / querysz\n",
    "        return accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50a228-f15a-41bb-9fe9-60a4fb5f7617",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(10000,19200,100):\n",
    "    PATH = f\"maml/0704_5way1shot2distractor1gan/model_step{step}.pt\"\n",
    "    device = torch.device(\"cuda\")\n",
    "    maml = Meta(args, config).to(device)\n",
    "    maml.load_state_dict(torch.load(PATH))\n",
    "    db_test = DataLoader(test_data_generator, 1, shuffle=True, num_workers=1, pin_memory=True)\n",
    "    accs_all_test = {\n",
    "                    \"total_query_nway\":[],\n",
    "                    \"distractor_query_nway_recall\":[],\n",
    "                    \"query_nway_recall\":[],\n",
    "                    \"label_query_nway_recall\":[],\n",
    "                    \"gan_query_nway\":[]\n",
    "    }\n",
    "\n",
    "    for test_data in db_test:\n",
    "        if args[\"gan\"]:\n",
    "            gan_spt_noise = torch.randn(args[\"spy_gan_num\"], 100, 1, 1, device=device)\n",
    "            gan_spt = netG(gan_spt_noise).to(device)\n",
    "            gan_qry_noise = torch.randn(args[\"qry_gan_num\"], 100, 1, 1, device=device)\n",
    "            gan_qry = netG(gan_qry_noise).to(device)\n",
    "        if len(test_data) == 4:\n",
    "            x_spt, y_spt, x_qry, y_qry = test_data\n",
    "            x_spt, y_spt, x_qry, y_qry = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device), \\\n",
    "                                         x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device)\n",
    "            if args[\"gan\"]:\n",
    "                accs,loss_q = maml.finetunning(x_spt, y_spt, x_qry, y_qry,gan_spt=gan_spt, gan_qry=gan_qry)\n",
    "            else:\n",
    "                accs = maml.finetunning(x_spt, y_spt, x_qry, y_qry)\n",
    "\n",
    "        else:\n",
    "            x_spt, y_spt, x_qry, y_qry, unlabel_spt, unlabel_qry = test_data\n",
    "            x_spt, y_spt, x_qry, y_qry, unlabel_spt, unlabel_qry = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device), \\\n",
    "                                         x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device),\\\n",
    "                                        unlabel_spt.squeeze(0).to(device), unlabel_qry.squeeze(0).to(device)\n",
    "            if args[\"gan\"]:\n",
    "                accs = maml.finetunning(x_spt, y_spt, x_qry, y_qry, unlabel_spt, unlabel_qry,gan_spt=gan_spt, gan_qry=gan_qry)\n",
    "            else:\n",
    "                accs = maml.finetunning(x_spt, y_spt, x_qry, y_qry, unlabel_spt, unlabel_qry)\n",
    "        if \"total_query_nway\" in accs:\n",
    "            accs_all_test[\"total_query_nway\"].append(accs[\"total_query_nway\"])\n",
    "        if \"label_query_nway_recall\" in accs:\n",
    "            accs_all_test[\"label_query_nway_recall\"].append(accs[\"label_query_nway_recall\"])\n",
    "            accs_all_test[\"distractor_query_nway_recall\"].append(accs[\"distractor_query_nway_recall\"])\n",
    "        if \"gan_query_nway\" in accs:\n",
    "            accs_all_test[\"gan_query_nway\"].append(accs[\"gan_query_nway\"])\n",
    "        if \"query_nway_recall\" in accs:\n",
    "            accs_all_test[\"query_nway_recall\"].append(accs[\"query_nway_recall\"])\n",
    "    # [b, update_step+1]\n",
    "    if \"total_query_nway\" in accs:\n",
    "        accs[\"total_query_nway\"] = np.array(accs_all_test[\"total_query_nway\"]).mean(axis=0).astype(np.float16)\n",
    "        writer.add_scalar(\"Accuracy/test_total_query_nway_accuracy\", accs[\"total_query_nway\"][-1], step)\n",
    "    if \"label_query_nway_recall\" in accs:\n",
    "        accs[\"label_query_nway_recall\"] = np.array(accs_all_test[\"label_query_nway_recall\"]).mean(axis=0).astype(np.float16)\n",
    "        accs[\"distractor_query_nway_recall\"] = np.array(accs_all_test[\"distractor_query_nway_recall\"]).mean(axis=0).astype(np.float16)\n",
    "        writer.add_scalar(\"Accuracy/test_label_query_nway_accuracy\", accs[\"label_query_nway_recall\"][-1], step)\n",
    "        writer.add_scalar(\"Accuracy/test_distractor_query_nway_recall_accuracy\", accs[\"distractor_query_nway_recall\"][-1], step)\n",
    "    if \"gan_query_nway\" in accs:\n",
    "        accs[\"gan_query_nway\"] = np.array(accs_all_test[\"gan_query_nway\"]).mean(axis=0).astype(np.float16)\n",
    "        writer.add_scalar(\"Accuracy/test_gan_query_nway_accuracy\", accs[\"gan_query_nway\"][-1], step)\n",
    "    if \"query_nway_recall\" in accs:\n",
    "        accs[\"query_nway_recall\"] = np.array(accs_all_test[\"query_nway_recall\"]).mean(axis=0).astype(np.float16)\n",
    "        writer.add_scalar(\"Accuracy/test_query_nway_accuracy\", accs[\"query_nway_recall\"][-1], step)\n",
    "    print(step)\n",
    "    print(\"Test acc:\", accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd5413a-3534-4aba-8801-3fe6753b4da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metagan",
   "language": "python",
   "name": "metagan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
