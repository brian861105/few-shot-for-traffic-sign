{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bb88118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyc/miniconda3/envs/metagan/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  torch, os\n",
    "import  numpy as np\n",
    "\n",
    "from    torch.utils.data import DataLoader\n",
    "from    torch.optim import lr_scheduler\n",
    "import  random, sys, pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "from meta_ganpp import MetaGAN\n",
    "from dataloader import train_data_generator,test_data_generator\n",
    "import tqdm\n",
    "import shutil\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.max_memory_allocated(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c24ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def save_train_accs(path, accs):\n",
    "    file = open(path +  '/q_nway_accuracies.txt', 'ab')\n",
    "    np.savetxt(file, np.array([accs[\"q_nway\"]]))\n",
    "    file.close()\n",
    "\n",
    "    file = open(path +  '/discrim_loss.txt', 'ab')\n",
    "    np.savetxt(file, np.array([accs[\"discrim_loss\"]]))\n",
    "    file.close()\n",
    "\n",
    "    file = open(path +  '/gen_nway_accuracies.txt', 'ab')\n",
    "    np.savetxt(file, np.array([accs[\"gen_nway\"]]))\n",
    "    file.close()\n",
    "\n",
    "    file = open(path +  '/gen_loss.txt', 'ab')\n",
    "    np.savetxt(file, np.array([accs[\"gen_loss\"]]))\n",
    "    file.close()\n",
    "\n",
    "def save_test_accs(path, accs):\n",
    "    file = open(path +  '/test_q_nway_accuracies.txt', 'ab')\n",
    "    np.savetxt(file, np.array([accs]))\n",
    "    file.close()\n",
    "\n",
    "def save_imgs(path, imgs, step):\n",
    "    # save raw txt files\n",
    "    img_f=open(path+\"/images_step\" + str(step) + \".txt\",'ab')\n",
    "    some_imgs = np.reshape(imgs, [imgs.shape[0]*imgs.shape[1], -1])[0:50]\n",
    "    np.savetxt(img_f,some_imgs)\n",
    "    img_f.close()\n",
    "\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "    # save png of imgs\n",
    "    i = 0\n",
    "    for flat_img in some_imgs:\n",
    "        img = flat_img.reshape(3,84,84).swapaxes(0,1).swapaxes(1,2)\n",
    "        im = ((img - np.min(img))*255/(np.max(img - np.min(img)))).astype(np.uint8)\n",
    "        if i < 49:\n",
    "            plt.subplot(7, 7, 1 + i)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(im)\n",
    "        i += 1\n",
    "    plt.savefig(path+\"/images_step\" + str(step) + \".png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fac93c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'epoch':24000,\n",
    "        'n_way':5,\n",
    "        'k_spt':1,\n",
    "        'k_qry':15,\n",
    "        'img_sz':84,\n",
    "        \"tasks_per_batch\":4,\n",
    "        'img_c':3,\n",
    "        'task_num': 4,\n",
    "        'meta_lr':1e-3,\n",
    "        'update_lr':1e-3,\n",
    "        'gan_update_lr':2e-4,\n",
    "        'update_steps':4,\n",
    "        'update_steps_test':10,\n",
    "        \"no_save\":False,\n",
    "        \"learn_inner_lr\":True,\n",
    "        'condition_discrim':False,\n",
    "        \"loss\":\"cross_entropy\",\n",
    "        \"create_graph\":False,\n",
    "        \"single_fast_test\":False,\n",
    "        \"consine_schedule\":True,\n",
    "        \"min_learning_rate\":1e-10,\n",
    "        \"number_of_training_steps_per_iter\":4,\n",
    "        \"multi_step_loss_num_epochs\":15,\n",
    "        'save_path':'0425_consine_mamlgan_plus2'\n",
    "       }\n",
    "\n",
    "if os.path.exists(args[\"save_path\"]):\n",
    "    shutil.rmtree(args[\"save_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb793c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset/BelgiumTSC\n",
      "load complete time 3.410439968109131\n",
      "load dataset/ArTS\n",
      "load complete time 3.38081955909729\n",
      "load dataset/chinese_traffic_sign\n",
      "load complete time 0.6300342082977295\n",
      "load dataset/CVL\n",
      "load complete time 0.4109306335449219\n",
      "load dataset/FullJCNN2013\n",
      "load complete time 0.2707638740539551\n",
      "load dataset/logo_2k\n",
      "load complete time 1.054626703262329\n",
      "load dataset/GTSRB\n",
      "load complete time 0.06203627586364746\n",
      "load dataset/DFG\n",
      "load complete time 0.02844381332397461\n"
     ]
    }
   ],
   "source": [
    "train_data_generator = train_data_generator(args)\n",
    "test_data_generator = test_data_generator(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99fa460",
   "metadata": {},
   "outputs": [],
   "source": [
    "spt_size = args[\"k_spt\"] * args[\"n_way\"]\n",
    "qry_size = args[\"k_qry\"] * args[\"n_way\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee394d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyc/miniconda3/envs/metagan/lib/python3.8/site-packages/torch/optim/adam.py:81: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  super(Adam, self).__init__(params, defaults)\n"
     ]
    }
   ],
   "source": [
    "shared_config = [\n",
    "    ('conv2d', [64, 3, 3, 3, 2, 0]),\n",
    "    ('leakyrelu', [.2, True]),\n",
    "    ('bn', [64]),\n",
    "    ('conv2d', [64, 64, 3, 3, 2, 0]),\n",
    "    ('leakyrelu', [.2, True]),\n",
    "    ('bn', [64]),\n",
    "]\n",
    "\n",
    "nway_config = [\n",
    "    ('conv2d', [64, 3, 3, 3, 2, 0]),\n",
    "    ('leakyrelu', [.2, True]),\n",
    "    ('bn', [64]),\n",
    "    ('conv2d', [128, 64, 3, 3, 2, 0]),\n",
    "    ('leakyrelu', [.2, True]),\n",
    "    ('bn', [128]),\n",
    "    ('conv2d', [256, 128, 3, 3, 2, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [256]),\n",
    "    ('conv2d', [512, 256, 3, 3, 2, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [512]),\n",
    "    ('flatten', []),\n",
    "    ('linear', [args[\"n_way\"], 8192])\n",
    "]\n",
    "\n",
    "discriminator_config = [\n",
    "    ('conv2d', [64, 3, 3, 3, 2, 0]),\n",
    "    ('leakyrelu', [.2, True]),\n",
    "    ('bn', [64]),\n",
    "    ('conv2d', [64, 64, 3, 3, 2, 0]),\n",
    "    ('leakyrelu', [.2, True]),\n",
    "    ('bn', [64]),\n",
    "    ('conv2d', [64, 64, 3, 3, 2, 0]),\n",
    "    ('leakyrelu', [.2, True]),\n",
    "    ('bn', [64]),\n",
    "    ('conv2d', [64, 64, 2, 2, 1, 0]),\n",
    "    ('leakyrelu', [.2, True]),\n",
    "    ('bn', [64]),\n",
    "    ('flatten', []),\n",
    "    ('linear', [1, 64 * 8 * 8])]\n",
    "\n",
    "# discriminator_config = [\n",
    "#     ('conv2d', [64, 3, 3, 3, 2, 0]),\n",
    "#     ('leakyrelu', [.2, True]),\n",
    "\n",
    "#     ('conv2d', [64, 64, 3, 3, 2, 0]),\n",
    "#     ('leakyrelu', [.2, True]),\n",
    "\n",
    "#     ('conv2d', [64, 64, 3, 3, 2, 0]),\n",
    "#     ('leakyrelu', [.2, True]),\n",
    "\n",
    "#     ('conv2d', [64, 64, 2, 2, 1, 0]),\n",
    "#     ('leakyrelu', [.2, True]),\n",
    "    \n",
    "#     ('flatten',[]),\n",
    "#     ('concat_y',[]),\n",
    "\n",
    "#     ('linear', [1, 64 * 8 * 8])]\n",
    "\n",
    "if args[\"condition_discrim\"]:\n",
    "    discriminator_config = [\n",
    "        ('conv2d', [32, 3, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 1, 0]),\n",
    "        ('condition', [512, 32, 5]),\n",
    "        ('leakyrelu', [0.2, True]),\n",
    "        ('flatten',[]),\n",
    "        ('linear', [1024, 1600]),\n",
    "        ('bn', [1024]),\n",
    "        ('linear', [1, 1024])\n",
    "        # don't use a sigmoid at the end\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "# gen_config = [\n",
    "#     ('convt2d', [3, 64, 3, 3, 1, 1]),\n",
    "#     ('leakyrelu', [.2, True]),\n",
    "#     ('bn', [64]),\n",
    "#     ('random_proj', [100, 84, 64]),\n",
    "#     ('convt2d', [128, 64, 3, 3, 1, 1]),\n",
    "#     #('convt2d', [1, 128, 4, 4, 2, 1]), # [ch_in, ch_out, kernel_sz, kernel_sz, stride, padding]\n",
    "#     ('relu', [.2, True]),\n",
    "#     ('bn', [64]),\n",
    "#     # ('encode', [1024, 64*28*28]),\n",
    "#     # ('decode', [64*28*28, 1024]),\n",
    "#     ('relu', [.2, True]),\n",
    "#     ('conv2d', [64, 64, 3, 3, 1, 1]),\n",
    "#     #('convt2d', [1, 128, 4, 4, 2, 1]), # [ch_in, ch_out, kernel_sz, kernel_sz, stride, padding]\n",
    "#     ('relu', [.2, True]),\n",
    "#     ('bn', [64]),\n",
    "# ]\n",
    "# gen_config = [\n",
    "#     ('conv2d', [32, 3, 4, 4, 2, 0]),\n",
    "#     ('leakyrelu', [.2, True]),\n",
    "#     ('bn', [32]),\n",
    "#     ('random_proj', [100, 41, 32]),\n",
    "\n",
    "#     ('convt2d', [64, 32, 3, 3, 1, 1]),\n",
    "#     #('convt2d', [1, 128, 4, 4, 2, 1]), # [ch_in, ch_out, kernel_sz, kernel_sz, stride, padding]\n",
    "#     ('relu', [.2, True]),\n",
    "#     ('bn', [32]),\n",
    "#     # ('encode', [1024, 64*28*28]),\n",
    "#     # # ('decode', [64*28*28, 1024]),\n",
    "#     ('relu', [.2, True]),\n",
    "#     ('conv2d', [3, 32, 3, 3, 1, 1]),\n",
    "#     ('convt2d', [3, 3, 4, 4, 2, 0]), # [ch_in, ch_out, kernel_sz, kernel_sz, stride, padding]\n",
    "#     ('relu', [.2, True]),\n",
    "#     ('bn', [3]),\n",
    "#     (\"sigmoid\",[])\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# gen_config = [\n",
    "#     ('c_gan',[100,21*21*32,100+5]), # [latent_dim, embedding_dim, ch_out, h_out/w_out]\n",
    "#     # img: (32, 21, 21)\n",
    "#     ('convt2d', [32, 16, 4, 4, 2, 1]), # [ch_in, ch_out, kernel_sz, kernel_sz, stride, padding]\n",
    "#     ('bn', [16]),\n",
    "#     ('relu', [True]),\n",
    "#     # img: (16, 42, 42)\n",
    "#     ('convt2d', [16, 3, 4, 4, 2, 1]),\n",
    "#     # # img: (3, 84, 84)\n",
    "#     ('sigmoid', [True])\n",
    "# ]\n",
    "# gen_config = [\n",
    "#     ('c_gan',[100,512,7,256]), # [latent_dim, embedding_dim, ch_out, h_out/w_out]\n",
    "#     # img: (32, 21, 21)\n",
    "#     ('convt2d', [32, 16, 4, 4, 2, 1]), # [ch_in, ch_out, kernel_sz, kernel_sz, stride, padding]\n",
    "#     ('bn', [16]),\n",
    "#     ('relu', [True]),\n",
    "#     # img: (16, 42, 42)\n",
    "#     ('convt2d', [16, 3, 4, 4, 2, 1]),\n",
    "#     # # img: (3, 84, 84)\n",
    "#     ('sigmoid', [True])\n",
    "# ]\n",
    "gen_config = [\n",
    "    # img: (256, 7, 7)\n",
    "    ('c_gan',[100,512,256,7]), # [latent_dim, embedding_dim, ch_out, h_out/w_out]\n",
    "    # img: (128, 14, 14)\n",
    "    ('convt2d', [256, 128, 4, 4, 2, 1]), # [ch_in, ch_out, kernel_sz, kernel_sz, stride, padding]\n",
    "    ('bn', [128]),\n",
    "    ('relu', [True]),\n",
    "    # img: (64, 28, 28)\n",
    "    ('convt2d', [128, 64, 4, 4, 2, 1]), # [ch_in, ch_out, kernel_sz, kernel_sz, stride, padding]\n",
    "    ('bn', [64]),\n",
    "    ('relu', [True]),\n",
    "    # img: (32, 84, 84)\n",
    "    ('convt2d', [64, 32, 3, 3, 3, 0]), # [ch_in, ch_out, kernel_sz, kernel_sz, stride, padding]\n",
    "    ('bn', [32]),\n",
    "    ('relu', [True]),\n",
    "    ('conv2d', [3, 32, 3, 3, 1, 1]),\n",
    "    # # # img: (3, 84, 84)\n",
    "    # ('sigmoid', [True])\n",
    "]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mamlGAN = MetaGAN(args, shared_config, nway_config, discriminator_config, gen_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01663d8f-2d08-4a57-84c6-da021d904a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import  torch\n",
    "# from    torch import nn\n",
    "# from    torch.nn import functional as F\n",
    "# import  numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# class Generator(nn.Module):\n",
    "#     \"\"\"\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, config, img_c, img_sz, num_classes):\n",
    "#         \"\"\"\n",
    "\n",
    "#         :param config: network config file, type:list of (string, list)\n",
    "#         :param img_c: 1 or 3\n",
    "#         :param img_sz:  28 or 84\n",
    "#         \"\"\"\n",
    "#         super(Generator, self).__init__()\n",
    "\n",
    "\n",
    "#         self.config = config\n",
    "\n",
    "#         self.num_classes = num_classes\n",
    "\n",
    "#         # this dict contains all tensors needed to be optimized\n",
    "#         self.vars = nn.ParameterList()\n",
    "#         # running_mean and running_var\n",
    "#         self.vars_bn = nn.ParameterList()\n",
    "\n",
    "#         for i, (name, param) in enumerate(self.config):\n",
    "#             if name == 'conv2d':\n",
    "#                 # [ch_out, ch_in, kernelsz, kernelsz]\n",
    "#                 w = nn.Parameter(torch.ones(*param[:4]))\n",
    "#                 # gain=1 according to cbfin's implementation\n",
    "#                 torch.nn.init.kaiming_normal_(w)\n",
    "#                 self.vars.append(w)\n",
    "#                 # [ch_out]\n",
    "#                 self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "\n",
    "#             elif name == 'convt2d':\n",
    "#                 # [ch_in, ch_out, kernel_sz, kernel_sz, stride, padding]\n",
    "#                 # output will be sz = stride * (input_sz) + kernel_sz\n",
    "#                 w = nn.Parameter(torch.ones(*param[:4]))\n",
    "#                 # gain=1 according to cbfin's implementation\n",
    "#                 torch.nn.init.kaiming_normal_(w)\n",
    "#                 self.vars.append(w)\n",
    "#                 # [ch_in, ch_out]\n",
    "#                 self.vars.append(nn.Parameter(torch.zeros(param[1])))\n",
    "\n",
    "#             elif name == 'linear':\n",
    "#                 # [ch_out, ch_in]\n",
    "#                 w = nn.Parameter(torch.ones(*param))\n",
    "#                 # gain=1 according to cbfinn's implementation\n",
    "#                 torch.nn.init.kaiming_normal_(w)\n",
    "#                 self.vars.append(w)\n",
    "#                 # [ch_out]\n",
    "#                 self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "#             elif name == 'encode':\n",
    "#                 # [ch_out, ch_in]\n",
    "#                 w = nn.Parameter(torch.ones(*param))\n",
    "#                 # gain=1 according to cbfinn's implementation\n",
    "#                 torch.nn.init.kaiming_normal_(w)\n",
    "#                 self.vars.append(w)\n",
    "#                 # [ch_out]\n",
    "#                 self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "#             elif name == 'decode':\n",
    "#                 # [ch_out, ch_in]\n",
    "#                 w = nn.Parameter(torch.ones(*param))\n",
    "#                 # gain=1 according to cbfinn's implementation\n",
    "#                 torch.nn.init.kaiming_normal_(w)\n",
    "#                 self.vars.append(w)\n",
    "#                 # [ch_out]\n",
    "#                 self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "#             elif name == 'bn':\n",
    "#                 # [ch_out]\n",
    "#                 w = nn.Parameter(torch.ones(param[0]))\n",
    "#                 self.vars.append(w)\n",
    "#                 # [ch_out]\n",
    "#                 self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "\n",
    "#                 # must set requires_grad=False\n",
    "#                 running_mean = nn.Parameter(torch.zeros(param[0]), requires_grad=False)\n",
    "#                 running_var = nn.Parameter(torch.ones(param[0]), requires_grad=False)\n",
    "#                 self.vars_bn.extend([running_mean, running_var])\n",
    "#             elif name == \"random_proj\":\n",
    "#                 # [ch_in, ch_out, img_sz]\n",
    "#                 # latent_dim, latent_ch_out, emb_dim, emb_ch_out, hw_out = param\n",
    "#                 emb_dim, emb_ch_out, hw_out = param\n",
    "#                 # latent projection params\n",
    "#                 # latent_dim, hw_out, rand_ch_out = param\n",
    "#                 w_lat = nn.Parameter(torch.ones(hw_out*hw_out*latent_ch_out, latent_dim))\n",
    "#                 torch.nn.init.kaiming_normal_(w_lat)\n",
    "      \n",
    "#             elif name == \"c_gan\":\n",
    "#                 w = nn.Parameter(torch.ones(param[2]*param[3]*param[3],param[0] + param[1]))\n",
    "#                 # gain=1 according to cbfinn's implementation\n",
    "#                 torch.nn.init.kaiming_normal_(w)\n",
    "#                 self.vars.append(w)\n",
    "#                 # [ch_out]\n",
    "#                 self.vars.append(nn.Parameter(torch.zeros(param[2]*param[3]*param[3])))\n",
    "                \n",
    "#                 w = nn.Parameter(torch.ones(param[2]*param[3]*param[3]))\n",
    "#                 self.vars.append(w)\n",
    "#                 # [ch_out]\n",
    "#                 self.vars.append(nn.Parameter(torch.zeros(param[2]*param[3]*param[3])))\n",
    "\n",
    "#                 # must set requires_grad=False\n",
    "#                 running_mean = nn.Parameter(torch.zeros(param[2]*param[3]*param[3]), requires_grad=False)\n",
    "#                 running_var = nn.Parameter(torch.ones(param[2]*param[3]*param[3]), requires_grad=False)\n",
    "#                 self.vars_bn.extend([running_mean, running_var])\n",
    "                \n",
    "#             elif name in ['tanh', 'relu', 'upsample', 'avg_pool2d', 'max_pool2d',\n",
    "#                           'flatten', 'reshape', 'leakyrelu', 'sigmoid', 'identity', 'update_identity', 'encode', 'decode']:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 raise NotImplementedError\n",
    "\n",
    "#     def forward(self, x, y, vars=None, bn_training=True):\n",
    "#         \"\"\"\n",
    "#         This function can be called by finetunning, however, in finetunning, we dont wish to update\n",
    "#         running_mean/running_var. Thought weights/bias of bn is updated, it has been separated by fast_weights.\n",
    "#         Indeed, to not update running_mean/running_var, we need set update_bn_statistics=False\n",
    "#         but weight/bias will be updated and not dirty initial theta parameters via fast_weiths.\n",
    "#         :param x: [b, 512]\n",
    "#         :param vars:\n",
    "#         :param bn_training: set False to not update\n",
    "#         :return: x, loss, likelihood, kld\n",
    "#         \"\"\"\n",
    "\n",
    "#         batch_sz = x.size()[0]\n",
    "\n",
    "#         x_orig = x\n",
    "\n",
    "#         if vars == None:\n",
    "#             vars = self.vars\n",
    "\n",
    "#         idx = 0\n",
    "#         bn_idx = 0\n",
    "\n",
    "#         # assert self.config[0][0] is 'random_proj'\n",
    "#         # need to start with the random projection\n",
    "#         for name, param in self.config:\n",
    "#             # print(name)\n",
    "#             if name == 'conv2d':\n",
    "#                 w, b = vars[idx], vars[idx + 1]\n",
    "#                 # remember to keep synchrozied of forward_encoder and forward_decoder!\n",
    "#                 x = F.conv2d(x, w, b, stride=param[4], padding=param[5])\n",
    "#                 idx += 2\n",
    "#                 # print(name, param, '\\tout:', x.shape)\n",
    "#             elif name == 'convt2d':\n",
    "#                 w, b = vars[idx], vars[idx + 1]\n",
    "#                 # remember to keep synchrozied of forward_encoder and forward_decoder!\n",
    "#                 x = F.conv_transpose2d(x, w, b, stride=param[4], padding=param[5])\n",
    "#                 idx += 2\n",
    "#                 # print(name, param, '\\tout:', x.shape)\n",
    "#             elif name == 'linear':\n",
    "#                 w, b = vars[idx], vars[idx + 1]\n",
    "#                 x = F.linear(x, w, b)\n",
    "#                 idx += 2\n",
    "#                 # print('forward:', idx, x.norm().item())\n",
    "#             elif name == 'bn':\n",
    "#                 w, b = vars[idx], vars[idx + 1]\n",
    "#                 running_mean, running_var = self.vars_bn[bn_idx], self.vars_bn[bn_idx+1]\n",
    "#                 x = F.batch_norm(x, running_mean, running_var, weight=w, bias=b, training=bn_training)\n",
    "#                 idx += 2\n",
    "#                 bn_idx += 2\n",
    "#             elif name == 'encode':\n",
    "#                 x = x.view(x.size(0), -1)\n",
    "#                 w, b = vars[idx], vars[idx + 1]\n",
    "#                 x = F.linear(x, w, b)\n",
    "#                 idx += 2\n",
    "#             elif name == 'decode':\n",
    "#                 w, b = vars[idx], vars[idx + 1]\n",
    "#                 x = F.linear(x, w, b)\n",
    "#                 x = x.view(x.size(0), 64,28,28)\n",
    "#                 idx += 2\n",
    "#             elif name == 'random_proj':\n",
    "\n",
    "#                 latent_dim, latent_ch_out, emb_dim, emb_ch_out, hw_out = param\n",
    "#                 # latent_dim, hw_out, rand_ch_out = param\n",
    "#                 cuda = torch.cuda.is_available()\n",
    "\n",
    "#                 # send random tensor to linear layer, reshape into noise channels\n",
    "#                 FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor \n",
    "#                 rand = FloatTensor((x.size(0),latent_dim))\n",
    "#                 torch.randn(x.size(0),latent_dim, out=rand, requires_grad=True)\n",
    "#                 # w_lat, b_lat = vars[idx], vars[idx + 1]\n",
    "#                 # rand = F.linear(rand, w_lat, b_lat)\n",
    "#                 # rand = F.leaky_relu(rand, 0.2)\n",
    "#                 # rand = rand.view(rand.size(0), rand_ch_out, hw_out, hw_out)\n",
    "#                 x = torch.cat((y, rand), 1)\n",
    "\n",
    "#                 # w_lat, b_lat = vars[idx], vars[idx + 1]\n",
    "\n",
    "#                 # rand = F.linear(rand, w_lat, b_lat)\n",
    "#                 # rand = F.leaky_relu(rand, 0.2)\n",
    "#                 # rand = rand.view(rand.size(0), latent_ch_out, hw_out, hw_out)\n",
    "\n",
    "#                 # send class embbeddings through a linear layer, reshape embeddings channels\n",
    "#                 # w_emb, b_emb = vars[idx+2], vars[idx + 3]\n",
    "#                 # x = F.conv2d(x, w, b, stride=param[4], padding=param[5])\n",
    "#                 idx += 2\n",
    "\n",
    "#                 x = F.linear(x, w_emb, b_emb)\n",
    "#                 x = F.leaky_relu(x, 0.2)\n",
    "#                 x = x.view(x.size(0), emb_ch_out, hw_out, hw_out)\n",
    "\n",
    "#                 # concatenate embeddings and projections\n",
    "                \n",
    "\n",
    "#                 idx += 2\n",
    "#             elif name == \"c_gan\":\n",
    "#                 latent_dim = param[0]\n",
    "#                 cuda = torch.cuda.is_available()\n",
    "#                 FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor \n",
    "#                 rand = FloatTensor(x.size(0),latent_dim)\n",
    "                \n",
    "#                 torch.randn(x.size(0),latent_dim, out=rand, requires_grad=False)\n",
    "                \n",
    "#                 x = torch.cat((x, rand), 1)\n",
    "\n",
    "#                 w, b = vars[idx], vars[idx + 1]\n",
    "\n",
    "#                 x = F.linear(x, w, b)\n",
    "#                 idx += 2\n",
    "#                 print(x.size())\n",
    "#                 w, b = vars[idx], vars[idx + 1]\n",
    "#                 print(w.size())\n",
    "#                 running_mean, running_var = self.vars_bn[bn_idx], self.vars_bn[bn_idx+1]\n",
    "#                 x = F.batch_norm(x, running_mean, running_var, weight=w, bias=b, training=bn_training)\n",
    "#                 idx += 2\n",
    "#                 bn_idx += 2\n",
    "                \n",
    "#                 x = x.view(x.size(0),param[2],param[3],param[3])\n",
    "                \n",
    "                \n",
    "#             elif name == 'update_identity':\n",
    "#                 x_orig = x\n",
    "#             elif name == 'identity':\n",
    "#                 # print(x.shape)\n",
    "#                 x += x_orig\n",
    "#             elif name == 'flatten':\n",
    "#                 # print(x.shape)\n",
    "#                 x = x.view(x.size(0), -1)\n",
    "#             elif name == 'reshape':\n",
    "#                 # [b, 8] => [b, 2, 2, 2]\n",
    "#                 x = x.view(x.size(0), *param)\n",
    "#             elif name == 'relu':\n",
    "#                 x = F.relu(x, inplace=param[0])\n",
    "#             elif name == 'leakyrelu':\n",
    "#                 x = F.leaky_relu(x, negative_slope=param[0], inplace=param[1])\n",
    "#             elif name == 'tanh':\n",
    "#                 x = F.tanh(x)\n",
    "#             elif name == 'sigmoid':\n",
    "#                 x = torch.sigmoid(x)\n",
    "#             elif name == 'upsample':\n",
    "#                 x = F.upsample_nearest(x, scale_factor=param[0])\n",
    "#             elif name == 'max_pool2d':\n",
    "#                 x = F.max_pool2d(x, param[0], param[1], param[2])\n",
    "#             elif name == 'avg_pool2d':\n",
    "#                 x = F.avg_pool2d(x, param[0], param[1], param[2])\n",
    "\n",
    "#             else:\n",
    "#                 raise NotImplementedError\n",
    "\n",
    "#         # make sure variable is used properly\n",
    "#         assert idx == len(vars)\n",
    "#         assert bn_idx == len(self.vars_bn)\n",
    "\n",
    "#         # right now still returning y so that we can easilly extend to generating diff nums of examples by adjusting y in here\n",
    "#         return x, y\n",
    "\n",
    "\n",
    "#     def zero_grad(self, vars=None):\n",
    "#         \"\"\"\n",
    "\n",
    "#         :param vars:\n",
    "#         :return:\n",
    "#         \"\"\"\n",
    "#         with torch.no_grad():\n",
    "#             if vars == None:\n",
    "#                 for p in self.vars:\n",
    "#                     if not p.grad == None:\n",
    "#                         p.grad.zero_()\n",
    "#             else:\n",
    "#                 for p in vars:\n",
    "#                     if not p.grad ==  None:\n",
    "#                         p.grad.zero_()\n",
    "\n",
    "#     def parameters(self):\n",
    "#         \"\"\"\n",
    "#         override this function since initial parameters will return with a generator.\n",
    "#         :return:\n",
    "#         \"\"\"\n",
    "#         return self.vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2015a151-32d3-43f5-bd7d-1015850e4487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from conditioner import Conditioner\n",
    "# gen_config = [\n",
    "#     # img: (256, 7, 7)\n",
    "#     ('c_gan',[100,512,256,7]), # [latent_dim, embedding_dim, ch_out, h_out/w_out]\n",
    "#     # img: (128, 14, 14)\n",
    "#     ('convt2d', [256, 128, 4, 4, 2, 1]), # [ch_in, ch_out, kernel_sz, kernel_sz, stride, padding]\n",
    "#     ('bn', [128]),\n",
    "#     ('relu', [True]),\n",
    "#     # img: (64, 28, 28)\n",
    "#     ('convt2d', [128, 64, 4, 4, 2, 1]), # [ch_in, ch_out, kernel_sz, kernel_sz, stride, padding]\n",
    "#     ('bn', [64]),\n",
    "#     ('relu', [True]),\n",
    "#     # img: (32, 84, 84)\n",
    "#     ('convt2d', [64, 32, 3, 3, 3, 0]), # [ch_in, ch_out, kernel_sz, kernel_sz, stride, padding]\n",
    "#     ('bn', [32]),\n",
    "#     ('relu', [True]),\n",
    "#     ('conv2d', [3, 32, 3, 3, 1, 1]),\n",
    "#     # # # img: (3, 84, 84)\n",
    "#     # ('sigmoid', [True])\n",
    "# ]\n",
    "# x_spt[0].size()\n",
    "# conditioner = Conditioner().cuda()\n",
    "# image_embeddings = conditioner(x_spt[0]).squeeze()\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# a = Generator(gen_config,3,84,5).to(device)\n",
    "\n",
    "# a(image_embeddings,y_spt[0])[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "922c35e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = filter(lambda x: x.requires_grad, mamlGAN.parameters())\n",
    "num = sum(map(lambda x: np.prod(x.shape), tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d015806",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = not args[\"no_save\"]\n",
    "if save_model:\n",
    "    path = args[\"save_path\"]\n",
    "    mkdir_p(path)\n",
    "    file = open(path +  '/architecture.txt', 'w+')\n",
    "    file.write(\"shared_config = \" + json.dumps(shared_config) + \"\\n\" + \n",
    "        \"nway_config = \" + json.dumps(nway_config) + \"\\n\" +\n",
    "        \"discriminator_config = \" + json.dumps(discriminator_config) + \"\\n\" + \n",
    "        \"gen_config = \" + json.dumps(gen_config)  + \"\\n\" + \n",
    "        \"learn_inner_lr = \" + str(args[\"learn_inner_lr\"])   + \"\\n\" + \n",
    "        \"condition_discrim = \" + str(args[\"condition_discrim\"])\n",
    "        )\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52cd5fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/24000 [00:00<?, ?it/s]../aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [3,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "  0%|                                                 | 0/24000 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_MAPPING_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, (x_spt, y_spt, x_qry, y_qry) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     14\u001b[0m     x_spt, y_spt, x_qry, y_qry \u001b[38;5;241m=\u001b[39m x_spt\u001b[38;5;241m.\u001b[39mto(device), y_spt\u001b[38;5;241m.\u001b[39mto(device), x_qry\u001b[38;5;241m.\u001b[39mto(device), y_qry\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 16\u001b[0m     accs \u001b[38;5;241m=\u001b[39m \u001b[43mmamlGAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_spt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_spt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_qry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_qry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(step))\n",
      "File \u001b[0;32m~/miniconda3/envs/metagan/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Lab/school/metaGAN/meta_ganpp.py:368\u001b[0m, in \u001b[0;36mMetaGAN.forward\u001b[0;34m(self, x_spt, y_spt, x_qry, y_qry, epoch)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    367\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mper_step_loss_importance_vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_per_step_loss_importance_vector()\n\u001b[0;32m--> 368\u001b[0m             loss_q_tmp, corrects_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_task_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_spt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_spt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_qry\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_qry\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m#             loss_q += loss_q_tmp\u001b[39;00m\n\u001b[1;32m    371\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(corrects_tmp\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(corrects\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/Lab/school/metaGAN/meta_ganpp.py:290\u001b[0m, in \u001b[0;36mMetaGAN.single_task_forward\u001b[0;34m(self, x_spt, y_spt, x_qry, y_qry, nets, images)\u001b[0m\n\u001b[1;32m    288\u001b[0m discrim_loss \u001b[38;5;241m=\u001b[39m (gen_discrim_loss \u001b[38;5;241m+\u001b[39m real_discrim_loss) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    289\u001b[0m shared_loss \u001b[38;5;241m=\u001b[39m nway_loss \u001b[38;5;241m+\u001b[39m discrim_loss  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m q_class_logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_qry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m loss_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_cross_entropy(q_class_logits, y_qry) \u001b[38;5;66;03m# doesn't use discrim loss\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mper_step_loss_importance_vectors[k\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m loss_q)\n",
      "File \u001b[0;32m~/Lab/school/metaGAN/meta_ganpp.py:114\u001b[0m, in \u001b[0;36mMetaGAN.pred\u001b[0;34m(self, x, weights, nets, nway, discrim, conditions)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# shared_layer = shared_net(x, vars=shared_weights, bn_training=True)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m discrim_logits \u001b[38;5;241m=\u001b[39m discrim_net(x, conditions\u001b[38;5;241m=\u001b[39mconditions, \u001b[38;5;28mvars\u001b[39m\u001b[38;5;241m=\u001b[39mdiscrim_weights, bn_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m discrim \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m class_logits \u001b[38;5;241m=\u001b[39m \u001b[43mnway_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnway_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m nway \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m class_logits, discrim_logits\n",
      "File \u001b[0;32m~/miniconda3/envs/metagan/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Lab/school/metaGAN/learner.py:157\u001b[0m, in \u001b[0;36mLearner.forward\u001b[0;34m(self, x, conditions, vars, bn_training)\u001b[0m\n\u001b[1;32m    155\u001b[0m w, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mvars\u001b[39m[idx], \u001b[38;5;28mvars\u001b[39m[idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# remember to keep synchrozied of forward_encoder and forward_decoder!\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# print(name, param, '\\tout:', x.shape)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_MAPPING_ERROR"
     ]
    }
   ],
   "source": [
    "path = args[\"save_path\"]\n",
    "step = 0\n",
    "best_epoch = 0\n",
    "best_acc = []\n",
    "with tqdm.tqdm(initial=step,\n",
    "                   total=int(args[\"epoch\"])) as pbar_train:\n",
    "    for _ in range(args[\"epoch\"]//6000):\n",
    "            # fetch meta_batchsz num of episode each time\n",
    "\n",
    "        train_dataloader = DataLoader(train_data_generator, args[\"tasks_per_batch\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "        for _, (x_spt, y_spt, x_qry, y_qry) in enumerate(train_dataloader):\n",
    "\n",
    "            x_spt, y_spt, x_qry, y_qry = x_spt.to(device), y_spt.to(device), x_qry.to(device), y_qry.to(device)\n",
    "\n",
    "            accs = mamlGAN(x_spt, y_spt, x_qry, y_qry, step)\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(\"step \" + str(step))\n",
    "                for key in accs.keys():\n",
    "                    print(key + \": \" + str(accs[key]))\n",
    "                save_train_accs(path, accs)\n",
    "            if step % 500 == 0:  # evaluation\n",
    "                db_test = DataLoader(test_data_generator, 1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "                accs_all_test = []\n",
    "                imgs_all_test = []\n",
    "                for x_spt, y_spt, x_qry, y_qry in db_test:\n",
    "                    x_spt, y_spt, x_qry, y_qry = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device), \\\n",
    "                                                 x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device)\n",
    "\n",
    "                    accs, imgs = mamlGAN.finetunning(x_spt, y_spt, x_qry, y_qry)\n",
    "                    torch.cuda.empty_cache()\n",
    "                    accs_all_test.append(accs)\n",
    "                    imgs_all_test.append(imgs.cpu().detach().numpy())\n",
    "\n",
    "                imgs_all_test = np.array(imgs_all_test)\n",
    "                # [b, update_step+1]\n",
    "                accs = np.array(accs_all_test).mean(axis=0).astype(np.float16)\n",
    "                print('Test acc:', accs)\n",
    "                save_test_accs(path, accs)\n",
    "                \n",
    "                if not len(best_acc):\n",
    "                    best_acc = accs\n",
    "                    best_epoch = step\n",
    "                    torch.save({'model_state_dict': mamlGAN.state_dict()}, path + \"/best\")\n",
    "                else:\n",
    "                    if max(accs) > max(best_acc):\n",
    "                        best_acc = accs\n",
    "                        best_epoch = step\n",
    "                        torch.save({'model_state_dict': mamlGAN.state_dict()}, path + \"/best\")\n",
    "                torch.save({'model_state_dict': mamlGAN.state_dict()}, path + \"/model_step\" + str(step))\n",
    "\n",
    "                save_imgs(path, imgs_all_test, step)\n",
    "            step = step + 1\n",
    "            pbar_train.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a6fe3-95c3-42d8-b958-895a7ae37510",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_all_test[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aea6c5-5d7b-46c3-8022-9ea191b017b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imgs_all_test[0][4].transpose((1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = np.array(([best_epoch] + list(accs))).astype(np.float8)\n",
    "save_test_accs(path,best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf5be1d-0f11-4e60-bf8b-1b0ec3b91b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "84*84*3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metagan",
   "language": "python",
   "name": "metagan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
