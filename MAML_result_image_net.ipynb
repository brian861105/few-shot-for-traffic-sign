{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4cf20f9-4ba4-4c03-a62a-39edf396473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch, os\n",
    "import  numpy as np\n",
    "import  scipy.stats\n",
    "from    torch.utils.data import DataLoader\n",
    "from    torch.optim import lr_scheduler\n",
    "import  random, sys, pickle\n",
    "from utils.results.dataloaders_mini_imagenet import train_data_gen , test_data_gen\n",
    "# from maml_meta import Meta\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from    torch import optim\n",
    "from maml_learner import Learner\n",
    "from    torch.nn import functional as F\n",
    "from    copy import deepcopy\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c183b5-bd2c-402f-9e8c-4972c3107350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 60000, 'n_way': 5, 'k_spt': 5, 'k_qry': 15, 'data_augmentation_num': 3, 'img_sz': 84, 'task_num': 4, 'img_c': 3, 'meta_lr': 0.001, 'update_lr': 0.01, 'update_step': 5, 'update_step_test': 5, 'loss': 'cross_entropy', 'number_of_training_steps_per_iter': 5, 'multi_step_loss_num_epochs': 10, 'spy_gan_num': 1, 'qry_gan_num': 5, 'num_distractor': 5, 'gan': 0, 'spy_distractor_num': 1, 'qry_distractor_num': 3, 'batch_for_gradient': 25, 'fm': 32, 'no_save': 0, 'learn_inner_lr': 0, 'create_graph': 0, 'msl': 1, 'single_fast_test': 0, 'consine_schedule': 15000, 'eta_min': 1e-06, 'save_path': 'mini_results2'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"results_configs/mini_results2.json\") as json_file:\n",
    "    args = json.load(json_file)\n",
    "print(args)\n",
    "if os.path.exists(\"maml_runs/\" + args[\"save_path\"]):\n",
    "    shutil.rmtree(\"maml_runs/\" + args[\"save_path\"])\n",
    "writer = SummaryWriter(\"results_runs/\" + args[\"save_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da9fa08a-b78c-44f9-8638-cb702894d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    if not os.path.exists(\"model_results/\" + path):\n",
    "        os.makedirs(\"model_results/\" + path)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d99e4cb-f139-4280-a099-4ebd2a9d2e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE\n",
    "fm = args[\"fm\"]\n",
    "config = [\n",
    "    (\"conv2d\", [fm, 3, 3, 3, 1, 0]),\n",
    "    (\"leakyrelu\", [0.2,True]),\n",
    "    (\"bn\", [fm]),\n",
    "    (\"max_pool2d\", [2, 2, 0]),\n",
    "    (\"conv2d\", [fm, fm, 3, 3, 1, 0]),\n",
    "    (\"leakyrelu\", [0.2,True]),\n",
    "    (\"bn\", [fm]),\n",
    "    (\"max_pool2d\", [2, 2, 0]),\n",
    "    (\"conv2d\", [fm, fm, 3, 3, 1, 0]),\n",
    "    (\"leakyrelu\", [0.2,True]),\n",
    "    (\"bn\", [fm]),\n",
    "    (\"max_pool2d\", [2, 2, 0]),\n",
    "    (\"conv2d\", [fm, fm, 3, 3, 1, 0]),\n",
    "    (\"leakyrelu\", [0.2,True]),\n",
    "    (\"bn\", [fm]),\n",
    "    (\"max_pool2d\", [2, 1, 0]),\n",
    "    (\"flatten\", []),\n",
    "    (\"linear\", [6, fm * 5 * 5])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cdf48fa-c580-4192-8d5a-a15e0caa9b8d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load datasets/mini_imagenet\n",
      "load complete time 1.764672040939331\n",
      "load datasets/mini_imagenet\n",
      "load complete time 0.038918495178222656\n"
     ]
    }
   ],
   "source": [
    "train_data_generator = train_data_gen(args)\n",
    "test_data_generator = test_data_gen(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e9a5621-204f-4563-92f0-f3550c850868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Meta(nn.Module):\n",
    "    \"\"\"\n",
    "    Meta Learner\n",
    "    \"\"\"\n",
    "    def __init__(self, args, config):\n",
    "        \"\"\"\n",
    "\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(Meta, self).__init__()\n",
    "\n",
    "        self.update_lr = args[\"update_lr\"]\n",
    "        self.meta_lr = args[\"meta_lr\"]\n",
    "        self.n_way = args[\"n_way\"]\n",
    "        self.k_spt = args[\"k_spt\"]\n",
    "        self.k_qry = args[\"k_qry\"]\n",
    "        self.task_num = args[\"task_num\"]\n",
    "        self.update_step = args[\"update_step\"]\n",
    "        self.update_step_test = args[\"update_step_test\"]\n",
    "        self.distractor = args[\"num_distractor\"]\n",
    "        self.net = Learner(config, args[\"img_c\"], args[\"img_sz\"])\n",
    "        self.meta_optim = optim.Adam(self.net.parameters(), lr=self.meta_lr)\n",
    "        if args[\"consine_schedule\"]:\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.meta_optim, T_max=args[\"consine_schedule\"], eta_min=args[\"eta_min\"])\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.gan = args[\"gan\"]\n",
    "        self.multi_step_loss_num_epochs = args[\"multi_step_loss_num_epochs\"]\n",
    "        \n",
    "    def get_per_step_loss_importance_vector(self):\n",
    "\n",
    "        loss_weights = np.ones(shape=(self.update_step)) * (\n",
    "                1.0 / self.update_step)\n",
    "        decay_rate = 1.0 / self.update_step / self.multi_step_loss_num_epochs\n",
    "        min_value_for_non_final_losses = 0.03 / self.update_step\n",
    "        for i in range(len(loss_weights) - 1):\n",
    "            curr_value = np.maximum(loss_weights[i] - (self.current_epoch * decay_rate), min_value_for_non_final_losses)\n",
    "            loss_weights[i] = curr_value\n",
    "\n",
    "        curr_value = np.minimum(\n",
    "            loss_weights[-1] + (self.current_epoch * (self.update_step - 1) * decay_rate),\n",
    "            1.0 - ((self.update_step - 1) * min_value_for_non_final_losses))\n",
    "        loss_weights[-1] = curr_value\n",
    "        loss_weights = torch.Tensor(loss_weights).to(device=self.device)\n",
    "        return loss_weights\n",
    "\n",
    "    def forward(self, x_spt, y_spt, x_qry, y_qry, current_epoch,unlabel_spt_image=None, unlabel_qry_image=None,gan_spt=None, gan_qry=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x_spt:   [b, setsz, c_, h, w]\n",
    "        :param y_spt:   [b, setsz]\n",
    "        :param x_qry:   [b, querysz, c_, h, w]\n",
    "        :param y_qry:   [b, querysz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.current_epoch = current_epoch\n",
    "        per_step_loss_importance_vectors = self.get_per_step_loss_importance_vector()\n",
    "        task_num, setsz, c_, h, w = x_spt.size()\n",
    "        querysz = x_qry.size(1)\n",
    "        if self.gan :\n",
    "            gan_sptsz = gan_spt.size(1)\n",
    "            gan_qrysz = gan_qry.size(1)\n",
    "        else:\n",
    "            gan_sptsz = 0\n",
    "            gan_qrysz = 0\n",
    "        if self.distractor or self.gan:\n",
    "            corrects = {}\n",
    "            corrects[\"total_query_nway\"] = np.zeros(self.update_step + 1)\n",
    "            if self.distractor:\n",
    "                unlabel_querysz = unlabel_qry.size(1)\n",
    "                corrects[\"query_nway_recall\"] = np.zeros(self.update_step + 1)\n",
    "                corrects[\"label_query_nway_recall\"] = np.zeros(self.update_step + 1)\n",
    "                corrects[\"distractor_query_nway_recall\"] = np.zeros(self.update_step + 1)\n",
    "            if self.gan :\n",
    "                corrects[\"gan_query_nway\"] = np.zeros(self.update_step + 1)\n",
    "        else:\n",
    "            corrects = {key: np.zeros(self.update_step + 1) for key in \n",
    "                [\n",
    "                \"query_nway_recall\",\n",
    "                \"label_query_nway_recall\"\n",
    "                ]}\n",
    "        losses_q = [0 for _ in range(self.update_step + 1)]  # losses_q[i] is the loss on step i\n",
    "\n",
    "        for i in range(task_num):\n",
    "            spt_image = x_spt[i]\n",
    "            spt_label = y_spt[i]\n",
    "            qry_image = x_qry[i]\n",
    "            qry_label = y_qry[i]\n",
    "            if self.distractor:\n",
    "                spt_image = torch.concat((spt_image,unlabel_spt[i]))\n",
    "                spt_unlabel_label = torch.full((unlabel_spt.size(1),), 5, dtype=torch.long,device=self.device)\n",
    "                spt_label = torch.cat((spt_label,spt_unlabel_label))\n",
    "                qry_image = torch.concat((qry_image,unlabel_qry[i]))\n",
    "                qry_unlabel_label = torch.full((unlabel_qry.size(1),), 5, dtype=torch.long,device=self.device)\n",
    "                qry_label = torch.cat((qry_label,qry_unlabel_label))\n",
    "            if self.gan :\n",
    "                spt_image = torch.concat((spt_image,gan_spt[i]))\n",
    "                spt_gan_label = torch.full((gan_spt.size(1),), 5, dtype=torch.long,device=self.device)\n",
    "                spt_label = torch.cat((spt_label,spt_gan_label))\n",
    "                qry_image = torch.concat((qry_image,gan_qry[i]))\n",
    "                qry_gan_label = torch.full((gan_qry.size(1),), 5, dtype=torch.long,device=self.device)\n",
    "                qry_label = torch.cat((qry_label,qry_gan_label))\n",
    "\n",
    "            # 1. run the i-th task and compute loss for k=0\n",
    "            logits = self.net(spt_image, vars=None, bn_training=True)\n",
    "            loss = F.cross_entropy(logits, spt_label)\n",
    "            grad = torch.autograd.grad(loss, self.net.parameters())\n",
    "            fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, self.net.parameters())))\n",
    "\n",
    "            # this is the loss and accuracy before first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                if self.distractor or self.gan:\n",
    "                    total_logits_q = self.net(qry_image, self.net.parameters(), bn_training=False)\n",
    "                    total_pred_q = F.softmax(total_logits_q, dim=1).argmax(dim=1)\n",
    "                    total_q_correct = torch.eq(total_pred_q, qry_label).sum().item()\n",
    "                    corrects[\"total_query_nway\"][0] += total_q_correct\n",
    "                    loss_q = F.cross_entropy(total_logits_q, qry_label)\n",
    "                    losses_q[0] += loss_q\n",
    "                    if self.distractor:\n",
    "                        label_logits_q = self.net(x_qry[i], self.net.parameters(), bn_training=False)\n",
    "                        label_pred_q = F.softmax(label_logits_q, dim=1).argmax(dim=1)\n",
    "                        label_pred_q_correct = torch.eq(label_pred_q, y_qry[i]).sum().item()\n",
    "                        corrects[\"label_query_nway_recall\"][0] += label_pred_q_correct\n",
    "                        label_pred_q = F.softmax(label_logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                        label_pred_q_correct = torch.eq(label_pred_q, y_qry[i]).sum().item()\n",
    "                        corrects[\"query_nway_recall\"][0] += label_pred_q_correct\n",
    "                        \n",
    "\n",
    "                        unlabel_logits_q = self.net(unlabel_qry[i], self.net.parameters(), bn_training=False)\n",
    "                        unlabel_pred_q = F.softmax(unlabel_logits_q, dim=1).argmax(dim=1)\n",
    "                        other = torch.eq(unlabel_pred_q, qry_unlabel_label).sum().item()\n",
    "                        corrects[\"distractor_query_nway_recall\"][0] += other\n",
    "                    if self.gan :\n",
    "                        gan_logits_q = self.net(gan_qry[i], self.net.parameters(), bn_training=False)\n",
    "                        gan_pred_q = F.softmax(gan_logits_q, dim=1).argmax(dim=1)\n",
    "                        gan_counts = torch.eq(gan_pred_q, qry_gan_label).sum().item()\n",
    "                        corrects[\"gan_query_nway\"][0] += gan_counts\n",
    "                else:\n",
    "                    logits_q = self.net(qry_image, self.net.parameters(), bn_training=True)\n",
    "                    loss_q = F.cross_entropy(logits_q, qry_label)\n",
    "                    pred_q = F.softmax(logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                    q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                    corrects[\"query_nway_recall\"][0] += q_discrim_correct\n",
    "                    pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                    q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                    corrects[\"label_query_nway_recall\"][0] += q_discrim_correct\n",
    "                    losses_q[0] += loss_q\n",
    "            # this is the loss and accuracy after the first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                if self.distractor or self.gan:\n",
    "                    \n",
    "                    total_logits_q = self.net(qry_image,fast_weights , bn_training=False)\n",
    "                    total_pred_q = F.softmax(total_logits_q, dim=1).argmax(dim=1)\n",
    "                    total_q_correct = torch.eq(total_pred_q, qry_label).sum().item()\n",
    "                    corrects[\"total_query_nway\"][1] += total_q_correct\n",
    "                    loss_q = F.cross_entropy(total_logits_q, qry_label)\n",
    "                    losses_q[1] += loss_q\n",
    "                    if self.distractor:\n",
    "                        label_logits_q = self.net(x_qry[i], fast_weights, bn_training=False)\n",
    "                        label_pred_q = F.softmax(label_logits_q, dim=1).argmax(dim=1)\n",
    "                        label_pred_q_correct = torch.eq(label_pred_q, y_qry[i]).sum().item()\n",
    "                        corrects[\"label_query_nway_recall\"][1] += label_pred_q_correct\n",
    "                        label_pred_q = F.softmax(label_logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                        label_pred_q_correct = torch.eq(label_pred_q, y_qry[i]).sum().item()\n",
    "                        corrects[\"query_nway_recall\"][1] += label_pred_q_correct\n",
    "                        \n",
    "                        unlabel_logits_q = self.net(unlabel_qry[i], fast_weights, bn_training=False)\n",
    "                        unlabel_pred_q = F.softmax(unlabel_logits_q, dim=1).argmax(dim=1)\n",
    "                        other = torch.eq(unlabel_pred_q, qry_unlabel_label).sum().item()\n",
    "                        corrects[\"distractor_query_nway_recall\"][1] += other\n",
    "                else:\n",
    "                    logits_q = self.net(qry_image, fast_weights, bn_training=False)\n",
    "                    loss_q = F.cross_entropy(logits_q, qry_label)\n",
    "                    pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                    q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                    corrects[\"query_nway_recall\"][1] += q_discrim_correct\n",
    "                    pred_q = F.softmax(logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                    q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                    corrects[\"label_query_nway_recall\"][1] += q_discrim_correct\n",
    "                    losses_q[1] += loss_q\n",
    "\n",
    "            for k in range(1, self.update_step):\n",
    "                # 1. run the i-th task and compute loss for k=1~K-1\n",
    "                logits = self.net(spt_image, fast_weights, bn_training=True)\n",
    "                loss = F.cross_entropy(logits, spt_label)\n",
    "                # 2. compute grad on theta_pi\n",
    "                grad = torch.autograd.grad(loss, fast_weights)\n",
    "                # 3. theta_pi = theta_pi - train_lr * grad\n",
    "                fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "                logits_q = self.net(qry_image, fast_weights, bn_training=True)\n",
    "                # loss_q will be overwritten and just keep the loss_q on last update step.\n",
    "                loss_q = F.cross_entropy(logits_q, qry_label)\n",
    "                losses_q[k + 1] += loss_q\n",
    "                with torch.no_grad():\n",
    "                    if self.distractor or self.gan:\n",
    "                        total_logits_q = self.net(qry_image, fast_weights, bn_training=False)\n",
    "                        total_pred_q = F.softmax(total_logits_q, dim=1).argmax(dim=1)\n",
    "                        total_q_correct = torch.eq(total_pred_q, qry_label).sum().item()\n",
    "                        corrects[\"total_query_nway\"][k+1] += total_q_correct\n",
    "                        if self.distractor:\n",
    "                            label_logits_q = self.net(x_qry[i], fast_weights, bn_training=False)\n",
    "                            label_pred_q = F.softmax(label_logits_q, dim=1).argmax(dim=1)\n",
    "                            label_pred_q_correct = torch.eq(label_pred_q, y_qry[i]).sum().item()\n",
    "                            corrects[\"label_query_nway_recall\"][k+1] += label_pred_q_correct\n",
    "                            label_pred_q = F.softmax(label_logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                            label_pred_q_correct = torch.eq(label_pred_q, y_qry[i]).sum().item()\n",
    "                            corrects[\"query_nway_recall\"][k+1] += label_pred_q_correct\n",
    "                            \n",
    "                            unlabel_logits_q = self.net(unlabel_qry[i], fast_weights, bn_training=False)\n",
    "                            unlabel_pred_q = F.softmax(unlabel_logits_q, dim=1).argmax(dim=1)\n",
    "                            other = torch.eq(unlabel_pred_q, qry_unlabel_label).sum().item()\n",
    "                            corrects[\"distractor_query_nway_recall\"][k+1] += other\n",
    "                    else:\n",
    "                        pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                        q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                        corrects[\"query_nway_recall\"][k+1] += q_discrim_correct\n",
    "                        pred_q = F.softmax(logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                        q_discrim_correct = torch.eq(pred_q, qry_label).sum().item()\n",
    "                        corrects[\"label_query_nway_recall\"][k+1] += q_discrim_correct\n",
    "        # end of all tasks\n",
    "        # sum over all losses on query set across all tasks\n",
    "        loss_q = 0\n",
    "\n",
    "        for num_step, loss in enumerate(losses_q[1:]):\n",
    "            loss_q = loss_q + per_step_loss_importance_vectors[num_step] * loss / task_num\n",
    "        # optimize theta parameters\n",
    "        self.meta_optim.zero_grad()\n",
    "        loss_q.backward()\n",
    "\n",
    "        self.meta_optim.step()\n",
    "        \n",
    "        accs = {}\n",
    "        if (self.distractor or self.gan):\n",
    "            accs[\"total_query_nway\"] = corrects[\"total_query_nway\"] / (task_num * (querysz + unlabel_querysz + gan_qrysz))\n",
    "            if self.distractor:\n",
    "                accs[\"label_query_nway_recall\"] = corrects[\"label_query_nway_recall\"] / (task_num * querysz)\n",
    "                accs[\"query_nway_recall\"] = corrects[\"query_nway_recall\"] / (task_num * querysz)\n",
    "                accs[\"distractor_query_nway_recall\"] = corrects[\"distractor_query_nway_recall\"] / (task_num * unlabel_querysz)\n",
    "            if gan_qrysz:\n",
    "                accs[\"gan_query_nway\"] = corrects[\"gan_query_nway\"] / (task_num * gan_qrysz)\n",
    "        else:\n",
    "            accs[\"query_nway_recall\"] = corrects[\"query_nway_recall\"] / (task_num * querysz)\n",
    "            accs[\"label_query_nway_recall\"] = corrects[\"label_query_nway_recall\"] / (task_num * querysz)\n",
    "        return accs,loss_q\n",
    "\n",
    "\n",
    "    def finetunning(self, x_spt, y_spt, x_qry, y_qry, unlabel_spt=None, unlabel_qry=None, gan_spt=None, gan_qry=None):\n",
    "\n",
    "        assert len(x_spt.shape) == 4\n",
    "        querysz = x_qry.size(0)\n",
    "\n",
    "        corrects = {}\n",
    "        corrects[\"total_query_nway\"] = np.zeros(self.update_step_test + 1)\n",
    "\n",
    "        unlabel_querysz = unlabel_qry.size(0)\n",
    "\n",
    "        corrects[\"query_nway_recall\"] = np.zeros(self.update_step_test + 1)\n",
    "        corrects[\"distractor_query_nway_recall\"] = np.zeros(self.update_step_test + 1)\n",
    "\n",
    "        # in order to not ruin the state of running_mean/variance and bn_weight/bias\n",
    "        # we finetunning on the copied model instead of self.net\n",
    "        net = deepcopy(self.net)\n",
    "        spt_image = x_spt\n",
    "        spt_label = y_spt\n",
    "        qry_image = x_qry\n",
    "        qry_label = y_qry\n",
    "        if self.distractor:\n",
    "            spt_image = torch.concat((spt_image,unlabel_spt))\n",
    "            spt_unlabel_label = torch.full((unlabel_spt.size(0),), 5, dtype=torch.long,device=self.device)\n",
    "            spt_label = torch.cat((spt_label,spt_unlabel_label))\n",
    "\n",
    "        qry_image = torch.concat((qry_image,unlabel_qry))\n",
    "        qry_unlabel_label = torch.full((unlabel_qry.size(0),), 5, dtype=torch.long,device=self.device)\n",
    "        qry_label = torch.cat((qry_label,qry_unlabel_label))\n",
    "\n",
    "        # 1. run the i-th task and compute loss for k=0\n",
    "        logits = net(spt_image)\n",
    "        loss = F.cross_entropy(logits, spt_label)\n",
    "\n",
    "        grad = torch.autograd.grad(loss, net.parameters())\n",
    "        fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, net.parameters())))\n",
    "\n",
    "        # this is the loss and accuracy before first update\n",
    "        with torch.no_grad():\n",
    "\n",
    "            total_logits_q = self.net(qry_image,self.net.parameters() , bn_training=False)\n",
    "            total_pred_q = F.softmax(total_logits_q, dim=1).argmax(dim=1)\n",
    "            total_q_correct = torch.eq(total_pred_q, qry_label).sum().item()\n",
    "            corrects[\"total_query_nway\"][0] += total_q_correct\n",
    "            loss_q = F.cross_entropy(total_logits_q, qry_label)\n",
    "\n",
    "            label_logits_q = self.net(x_qry, self.net.parameters(), bn_training=False)\n",
    "            label_pred_q = F.softmax(label_logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "            label_pred_q_correct = torch.eq(label_pred_q, y_qry).sum().item()\n",
    "            corrects[\"query_nway_recall\"][0] += label_pred_q_correct\n",
    "\n",
    "            unlabel_logits_q = self.net(unlabel_qry, self.net.parameters(), bn_training=False)\n",
    "            unlabel_pred_q = F.softmax(unlabel_logits_q, dim=1).argmax(dim=1)\n",
    "            other = torch.eq(unlabel_pred_q, qry_unlabel_label).sum().item()\n",
    "            corrects[\"distractor_query_nway_recall\"][0] += other\n",
    "\n",
    "\n",
    "        # this is the loss and accuracy after the first update\n",
    "        with torch.no_grad():\n",
    "\n",
    "            total_logits_q = self.net(qry_image,fast_weights , bn_training=False)\n",
    "            total_pred_q = F.softmax(total_logits_q, dim=1).argmax(dim=1)\n",
    "            \n",
    "            total_q_correct = torch.eq(total_pred_q, qry_label).sum().item()\n",
    "            corrects[\"total_query_nway\"][1] += total_q_correct\n",
    "            loss_q = F.cross_entropy(total_logits_q, qry_label)\n",
    "\n",
    "            label_logits_q = self.net(x_qry, fast_weights, bn_training=False)\n",
    "            label_pred_q = F.softmax(label_logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "            label_pred_q_correct = torch.eq(label_pred_q, y_qry).sum().item()\n",
    "            corrects[\"query_nway_recall\"][1] += label_pred_q_correct\n",
    "\n",
    "            unlabel_logits_q = self.net(unlabel_qry, fast_weights, bn_training=False)\n",
    "            \n",
    "            unlabel_pred_q = F.softmax(unlabel_logits_q, dim=1).argmax(dim=1)\n",
    "            other = torch.eq(unlabel_pred_q, qry_unlabel_label).sum().item()\n",
    "            corrects[\"distractor_query_nway_recall\"][1] += other\n",
    "\n",
    "\n",
    "        for k in range(1, self.update_step_test):\n",
    "            # 1. run the i-th task and compute loss for k=1~K-1\n",
    "            logits = net(spt_image, fast_weights, bn_training=True)\n",
    "            loss = F.cross_entropy(logits, spt_label)\n",
    "            # 2. compute grad on theta_pi\n",
    "            grad = torch.autograd.grad(loss, fast_weights)\n",
    "            # 3. theta_pi = theta_pi - train_lr * grad\n",
    "            fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "            logits_q = net(qry_image, fast_weights, bn_training=True)\n",
    "            # loss_q will be overwritten and just keep the loss_q on last update step.\n",
    "            loss_q = F.cross_entropy(logits_q, qry_label)\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                total_logits_q = self.net(qry_image,fast_weights , bn_training=False)\n",
    "\n",
    "                total_pred_q = F.softmax(total_logits_q, dim=1).argmax(dim=1)\n",
    "                total_q_correct = torch.eq(total_pred_q, qry_label).sum().item()\n",
    "                corrects[\"total_query_nway\"][k+1] += total_q_correct\n",
    "                loss_q = F.cross_entropy(total_logits_q, qry_label)\n",
    "\n",
    "                label_logits_q = self.net(x_qry, fast_weights, bn_training=False)\n",
    "                label_pred_q = F.softmax(label_logits_q[:,:-1], dim=1).argmax(dim=1)\n",
    "                label_pred_q_correct = torch.eq(label_pred_q, y_qry).sum().item()\n",
    "                corrects[\"query_nway_recall\"][k+1] += label_pred_q_correct\n",
    "\n",
    "                unlabel_logits_q = self.net(unlabel_qry, fast_weights, bn_training=False)\n",
    "                unlabel_pred_q = F.softmax(unlabel_logits_q, dim=1).argmax(dim=1)\n",
    "                other = torch.eq(unlabel_pred_q, qry_unlabel_label).sum().item()\n",
    "                corrects[\"distractor_query_nway_recall\"][k+1] += other\n",
    "        del net\n",
    "        accs = {}\n",
    "\n",
    "        # accs[\"total_query_nway\"] = corrects[\"total_query_nway\"] / (querysz)\n",
    "        accs[\"total_query_nway\"] = corrects[\"total_query_nway\"] / (querysz + unlabel_querysz)\n",
    "        accs[\"query_nway_recall\"] = corrects[\"query_nway_recall\"] / querysz\n",
    "        accs[\"distractor_query_nway_recall\"] = corrects[\"distractor_query_nway_recall\"] / (unlabel_querysz)\n",
    "\n",
    "        return accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb2d02e-c650-4a07-b2ce-5ab10edf5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "maml = Meta(args, config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecadcd4d-b5a1-4e2d-9648-e716f71db135",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = filter(lambda x: x.requires_grad, maml.parameters())\n",
    "num = sum(map(lambda x: np.prod(x.shape), tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38236a4e-eb80-4c04-9cd8-21792e4f05b1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 \ttraining acc: {'total_query_nway': array([0.16388889, 0.18888889, 0.225     , 0.24722222, 0.26111111,\n",
      "       0.275     ]), 'label_query_nway_recall': array([0.18666667, 0.21      , 0.25333333, 0.28      , 0.29      ,\n",
      "       0.3       ]), 'query_nway_recall': array([0.19666667, 0.24      , 0.27333333, 0.30333333, 0.31      ,\n",
      "       0.32333333]), 'distractor_query_nway_recall': array([0.05      , 0.08333333, 0.08333333, 0.08333333, 0.11666667,\n",
      "       0.13333333])}\n",
      "Test acc: {'total_query_nway': array([0.1694, 0.1777, 0.1888, 0.1917, 0.2028, 0.1973], dtype=float16), 'query_nway_recall': array([0.2234, 0.2267, 0.2367, 0.2367, 0.2534, 0.25  ], dtype=float16), 'distractor_query_nway_recall': array([0.05   , 0.1    , 0.1333 , 0.1333 , 0.1333 , 0.11664],\n",
      "      dtype=float16)}\n",
      "step: 100 \ttraining acc: {'total_query_nway': array([0.14444444, 0.22777778, 0.25833333, 0.26666667, 0.3       ,\n",
      "       0.3       ]), 'label_query_nway_recall': array([0.14333333, 0.23666667, 0.27333333, 0.29      , 0.32333333,\n",
      "       0.32666667]), 'query_nway_recall': array([0.16      , 0.29666667, 0.32333333, 0.33666667, 0.37      ,\n",
      "       0.38      ]), 'distractor_query_nway_recall': array([0.15      , 0.18333333, 0.18333333, 0.15      , 0.18333333,\n",
      "       0.16666667])}\n",
      "step: 200 \ttraining acc: {'total_query_nway': array([0.18333333, 0.27222222, 0.3       , 0.31111111, 0.32777778,\n",
      "       0.34166667]), 'label_query_nway_recall': array([0.18333333, 0.29      , 0.33      , 0.34666667, 0.36333333,\n",
      "       0.37333333]), 'query_nway_recall': array([0.24333333, 0.32333333, 0.35666667, 0.38      , 0.39333333,\n",
      "       0.40333333]), 'distractor_query_nway_recall': array([0.18333333, 0.18333333, 0.15      , 0.13333333, 0.15      ,\n",
      "       0.18333333])}\n",
      "step: 300 \ttraining acc: {'total_query_nway': array([0.16388889, 0.23888889, 0.31944444, 0.35      , 0.38333333,\n",
      "       0.38611111]), 'label_query_nway_recall': array([0.15      , 0.24333333, 0.33      , 0.37333333, 0.40666667,\n",
      "       0.41      ]), 'query_nway_recall': array([0.2       , 0.29333333, 0.37333333, 0.41666667, 0.43      ,\n",
      "       0.43666667]), 'distractor_query_nway_recall': array([0.23333333, 0.21666667, 0.26666667, 0.25      , 0.26666667,\n",
      "       0.26666667])}\n",
      "step: 400 \ttraining acc: {'total_query_nway': array([0.15833333, 0.26944444, 0.33055556, 0.35833333, 0.36111111,\n",
      "       0.37777778]), 'label_query_nway_recall': array([0.15666667, 0.29      , 0.37      , 0.4       , 0.39666667,\n",
      "       0.41666667]), 'query_nway_recall': array([0.20333333, 0.32      , 0.4       , 0.42      , 0.41666667,\n",
      "       0.43333333]), 'distractor_query_nway_recall': array([0.16666667, 0.16666667, 0.13333333, 0.15      , 0.18333333,\n",
      "       0.18333333])}\n",
      "step: 500 \ttraining acc: {'total_query_nway': array([0.16111111, 0.3       , 0.37222222, 0.40555556, 0.42222222,\n",
      "       0.41944444]), 'label_query_nway_recall': array([0.13333333, 0.29333333, 0.39      , 0.44      , 0.46333333,\n",
      "       0.46333333]), 'query_nway_recall': array([0.18666667, 0.37666667, 0.45666667, 0.49333333, 0.50333333,\n",
      "       0.5       ]), 'distractor_query_nway_recall': array([0.3       , 0.33333333, 0.3       , 0.23333333, 0.21666667,\n",
      "       0.18333333])}\n",
      "Test acc: {'total_query_nway': array([0.175 , 0.2778, 0.3472, 0.3528, 0.3638, 0.3777], dtype=float16), 'query_nway_recall': array([0.2067, 0.3267, 0.4167, 0.4233, 0.44  , 0.47  ], dtype=float16), 'distractor_query_nway_recall': array([0.2333, 0.25  , 0.2167, 0.2167, 0.2167, 0.1666], dtype=float16)}\n",
      "step: 600 \ttraining acc: {'total_query_nway': array([0.23333333, 0.30833333, 0.37777778, 0.40555556, 0.40833333,\n",
      "       0.41666667]), 'label_query_nway_recall': array([0.21666667, 0.32666667, 0.41666667, 0.44333333, 0.45333333,\n",
      "       0.46333333]), 'query_nway_recall': array([0.24666667, 0.35666667, 0.45333333, 0.48666667, 0.49333333,\n",
      "       0.50333333]), 'distractor_query_nway_recall': array([0.31666667, 0.2       , 0.18333333, 0.21666667, 0.18333333,\n",
      "       0.18333333])}\n",
      "step: 700 \ttraining acc: {'total_query_nway': array([0.20833333, 0.3       , 0.33055556, 0.36111111, 0.37222222,\n",
      "       0.38055556]), 'label_query_nway_recall': array([0.19      , 0.29333333, 0.34      , 0.38      , 0.39333333,\n",
      "       0.40333333]), 'query_nway_recall': array([0.23666667, 0.33      , 0.37333333, 0.42666667, 0.43      ,\n",
      "       0.44666667]), 'distractor_query_nway_recall': array([0.3       , 0.33333333, 0.28333333, 0.26666667, 0.26666667,\n",
      "       0.26666667])}\n",
      "step: 800 \ttraining acc: {'total_query_nway': array([0.12777778, 0.23888889, 0.31944444, 0.35555556, 0.36666667,\n",
      "       0.36111111]), 'label_query_nway_recall': array([0.10666667, 0.24      , 0.34333333, 0.38666667, 0.40666667,\n",
      "       0.40333333]), 'query_nway_recall': array([0.12666667, 0.28666667, 0.38      , 0.40333333, 0.42      ,\n",
      "       0.42      ]), 'distractor_query_nway_recall': array([0.23333333, 0.23333333, 0.2       , 0.2       , 0.16666667,\n",
      "       0.15      ])}\n",
      "step: 900 \ttraining acc: {'total_query_nway': array([0.15833333, 0.31388889, 0.38611111, 0.43888889, 0.46388889,\n",
      "       0.47777778]), 'label_query_nway_recall': array([0.12      , 0.29666667, 0.38333333, 0.44      , 0.48      ,\n",
      "       0.51      ]), 'query_nway_recall': array([0.2       , 0.40333333, 0.49666667, 0.52333333, 0.56      ,\n",
      "       0.58333333]), 'distractor_query_nway_recall': array([0.35      , 0.4       , 0.4       , 0.43333333, 0.38333333,\n",
      "       0.31666667])}\n",
      "step: 1000 \ttraining acc: {'total_query_nway': array([0.16111111, 0.33333333, 0.43055556, 0.45277778, 0.47222222,\n",
      "       0.47222222]), 'label_query_nway_recall': array([0.16      , 0.33666667, 0.46333333, 0.49333333, 0.51666667,\n",
      "       0.52333333]), 'query_nway_recall': array([0.19666667, 0.43666667, 0.54      , 0.56      , 0.57333333,\n",
      "       0.58      ]), 'distractor_query_nway_recall': array([0.16666667, 0.31666667, 0.26666667, 0.25      , 0.25      ,\n",
      "       0.21666667])}\n",
      "Test acc: {'total_query_nway': array([0.1278, 0.3083, 0.3472, 0.3555, 0.3833, 0.4028], dtype=float16), 'query_nway_recall': array([0.1533, 0.3667, 0.3967, 0.4167, 0.4434, 0.4766], dtype=float16), 'distractor_query_nway_recall': array([0.1666 , 0.1333 , 0.15   , 0.1333 , 0.15   , 0.11664],\n",
      "      dtype=float16)}\n",
      "step: 1100 \ttraining acc: {'total_query_nway': array([0.13333333, 0.36666667, 0.40833333, 0.45555556, 0.46666667,\n",
      "       0.47777778]), 'label_query_nway_recall': array([0.10666667, 0.39666667, 0.45      , 0.50333333, 0.52      ,\n",
      "       0.52666667]), 'query_nway_recall': array([0.15666667, 0.44666667, 0.48      , 0.52666667, 0.54666667,\n",
      "       0.55      ]), 'distractor_query_nway_recall': array([0.26666667, 0.21666667, 0.2       , 0.21666667, 0.2       ,\n",
      "       0.23333333])}\n"
     ]
    }
   ],
   "source": [
    "path = args[\"save_path\"]\n",
    "step = 0\n",
    "mkdir_p(path)\n",
    "for epoch in range(args[\"epoch\"]//1000):\n",
    "        # fetch meta_batchsz num of episode each time\n",
    "\n",
    "    train_dataloader = DataLoader(train_data_generator, args[\"task_num\"], shuffle=True, num_workers=1, pin_memory=True)\n",
    "    x_spt = y_spt = x_qry = y_qry = unlabel_spt = unlabel_qry = gan_qry = gan_spt = 0\n",
    "    for idx,data  in enumerate(train_dataloader):\n",
    "        if len(data) == 4:\n",
    "            (x_spt, y_spt, x_qry, y_qry) = data\n",
    "            x_spt, y_spt, x_qry, y_qry = x_spt.to(device), y_spt.to(device), x_qry.to(device), y_qry.to(device)\n",
    "            if args[\"gan\"]:\n",
    "                accs,loss_q = maml(x_spt, y_spt, x_qry, y_qry,step,gan_spt=gan_spt, gan_qry=gan_qry)\n",
    "            else:\n",
    "                accs,loss_q = maml(x_spt, y_spt, x_qry, y_qry,step)\n",
    "        else:\n",
    "            (x_spt, y_spt, x_qry, y_qry, unlabel_spt, unlabel_qry) = data\n",
    "            x_spt, y_spt, x_qry, y_qry, unlabel_spt, unlabel_qry = x_spt.to(device), y_spt.to(device), x_qry.to(device), y_qry.to(device), \\\n",
    "            unlabel_spt.to(device), unlabel_qry.to(device)\n",
    "            if args[\"spy_gan_num\"]:\n",
    "                accs,loss_q = maml(x_spt, y_spt, x_qry, y_qry,step,unlabel_spt_image=unlabel_spt, unlabel_qry_image=unlabel_qry,gan_spt=gan_spt, gan_qry=gan_qry)\n",
    "            else:\n",
    "                accs,loss_q = maml(x_spt, y_spt, x_qry, y_qry,step,unlabel_spt_image=unlabel_spt, unlabel_qry_image=unlabel_qry)\n",
    "\n",
    "        writer.add_scalar(\"Loss/train_loss\", loss_q, step)\n",
    "        if \"total_query_nway\" in accs:\n",
    "            writer.add_scalar(\"Accuracy/train_total_query_nway\", accs[\"total_query_nway\"][-1], step)\n",
    "        if \"label_query_nway_recall\" in accs:\n",
    "            writer.add_scalar(\"Accuracy/train_label_query_nway_recall\", accs[\"label_query_nway_recall\"][-1], step)\n",
    "        if \"distractor_query_nway_recall\" in accs:\n",
    "            writer.add_scalar(\"Accuracy/train_distractor_query_nway_recall\", accs[\"distractor_query_nway_recall\"][-1], step)\n",
    "        if \"query_nway_recall\" in accs:\n",
    "            writer.add_scalar(\"Accuracy/train_query_nway_recall\", accs[\"query_nway_recall\"][-1], step)\n",
    "        if \"gan_query_nway_recall\" in accs:\n",
    "            writer.add_scalar(\"Accuracy/train_gan_query_nway_recall\", accs[\"gan_query_nway_recall\"][-1], step)\n",
    "        if \"query_nway\" in accs:\n",
    "            writer.add_scalar(\"Accuracy/train_query_nway_recall\", accs[\"query_nway_recall\"][-1], step)\n",
    "        if step % 100 == 0:\n",
    "            print(\"step:\", step, \"\\ttraining acc:\", accs)\n",
    "        if step % 500 == 0:  # evaluation\n",
    "            db_test = DataLoader(test_data_generator, 1, shuffle=True, num_workers=1, pin_memory=True)\n",
    "            accs_all_test = {\n",
    "                            \"total_query_nway\":[],\n",
    "                            \"distractor_query_nway_recall\":[],\n",
    "                            \"query_nway_recall\":[],\n",
    "                            \"label_query_nway_recall\":[],\n",
    "                            \"gan_query_nway\":[]\n",
    "            }\n",
    "            for test_data in db_test:\n",
    "\n",
    "                if len(test_data) == 4:\n",
    "                    x_spt, y_spt, x_qry, y_qry = test_data\n",
    "                    x_spt, y_spt, x_qry, y_qry = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device), \\\n",
    "                                                 x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device)\n",
    "\n",
    "                    accs = maml.finetunning(x_spt, y_spt, x_qry, y_qry)\n",
    "\n",
    "                else:\n",
    "                    x_spt, y_spt, x_qry, y_qry, unlabel_spt, unlabel_qry = test_data\n",
    "                    x_spt, y_spt, x_qry, y_qry, unlabel_spt, unlabel_qry = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device), \\\n",
    "                                                 x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device),\\\n",
    "                                                unlabel_spt.squeeze(0).to(device), unlabel_qry.squeeze(0).to(device)\n",
    "\n",
    "\n",
    "                    accs = maml.finetunning(x_spt, y_spt, x_qry, y_qry, unlabel_spt, unlabel_qry)\n",
    "                if \"total_query_nway\" in accs:\n",
    "                    accs_all_test[\"total_query_nway\"].append(accs[\"total_query_nway\"])\n",
    "                if \"label_query_nway_recall\" in accs:\n",
    "                    accs_all_test[\"label_query_nway_recall\"].append(accs[\"label_query_nway_recall\"])\n",
    "                if \"distractor_query_nway_recall\" in accs:\n",
    "                    accs_all_test[\"distractor_query_nway_recall\"].append(accs[\"distractor_query_nway_recall\"])\n",
    "                if \"gan_query_nway\" in accs:\n",
    "                    accs_all_test[\"gan_query_nway\"].append(accs[\"gan_query_nway\"])\n",
    "                if \"query_nway_recall\" in accs:\n",
    "                    accs_all_test[\"query_nway_recall\"].append(accs[\"query_nway_recall\"])\n",
    "            # [b, update_step+1]\n",
    "            if \"total_query_nway\" in accs:\n",
    "                accs[\"total_query_nway\"] = np.array(accs_all_test[\"total_query_nway\"]).mean(axis=0).astype(np.float16)\n",
    "                writer.add_scalar(\"Accuracy/test_total_query_nway_accuracy\", accs[\"total_query_nway\"][-1], step)\n",
    "            if \"label_query_nway_recall\" in accs:\n",
    "                accs[\"label_query_nway_recall\"] = np.array(accs_all_test[\"label_query_nway_recall\"]).mean(axis=0).astype(np.float16)\n",
    "                \n",
    "                writer.add_scalar(\"Accuracy/test_label_query_nway_accuracy\", accs[\"label_query_nway_recall\"][-1], step)\n",
    "            if \"distractor_query_nway_recall\" in accs:\n",
    "                accs[\"distractor_query_nway_recall\"] = np.array(accs_all_test[\"distractor_query_nway_recall\"]).mean(axis=0).astype(np.float16)\n",
    "                writer.add_scalar(\"Accuracy/test_distractor_query_nway_recall_accuracy\", accs[\"distractor_query_nway_recall\"][-1], step)\n",
    "            if \"gan_query_nway\" in accs:\n",
    "                accs[\"gan_query_nway\"] = np.array(accs_all_test[\"gan_query_nway\"]).mean(axis=0).astype(np.float16)\n",
    "                writer.add_scalar(\"Accuracy/test_gan_query_nway_accuracy\", accs[\"gan_query_nway\"][-1], step)\n",
    "            if \"query_nway_recall\" in accs:\n",
    "                accs[\"query_nway_recall\"] = np.array(accs_all_test[\"query_nway_recall\"]).mean(axis=0).astype(np.float16)\n",
    "                writer.add_scalar(\"Accuracy/test_query_nway_accuracy\", accs[\"query_nway_recall\"][-1], step)\n",
    "\n",
    "            print(\"Test acc:\", accs)\n",
    "\n",
    "            torch.save(maml.state_dict(), \"model_results/\" + path + \"/model_step\" + str(step) + \".pt\")\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89da01-d3e8-42d5-bd52-00321dcd2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_data in db_test:\n",
    "\n",
    "    if len(test_data) == 4:\n",
    "        x_spt, y_spt, x_qry, y_qry = test_data\n",
    "        x_spt, y_spt, x_qry, y_qry = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device), \\\n",
    "                                     x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device)\n",
    "    else:\n",
    "        x_spt, y_spt, x_qry, y_qry, unlabel_spt, unlabel_qry = test_data\n",
    "        x_spt, y_spt, x_qry, y_qry, unlabel_spt, unlabel_qry = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device), \\\n",
    "                                     x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device),\\\n",
    "                                    unlabel_spt.squeeze(0).to(device), unlabel_qry.squeeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df969b2-8b84-42be-b214-e020d124c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_step_test = 10\n",
    "corrects = {}\n",
    "corrects[\"total_query_nway\"] = np.zeros(10 + 1)\n",
    "\n",
    "unlabel_querysz = unlabel_qry.size(0)\n",
    "\n",
    "net = maml.net\n",
    "spt_image = x_spt\n",
    "spt_label = y_spt\n",
    "qry_image = x_qry\n",
    "qry_label = y_qry\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    total_logits_q = net(qry_image,net.parameters() , bn_training=False)\n",
    "    total_pred_q = F.softmax(total_logits_q, dim=1).argmax(dim=1)\n",
    "    total_q_correct = torch.eq(total_pred_q, qry_label).sum().item()\n",
    "    corrects[\"total_query_nway\"][0] += total_q_correct\n",
    "    loss_q = F.cross_entropy(total_logits_q, qry_label)\n",
    "    \n",
    "logits = net(spt_image)\n",
    "loss = F.cross_entropy(logits, spt_label)\n",
    "\n",
    "grad = torch.autograd.grad(loss, net.parameters())\n",
    "fast_weights = list(map(lambda p: p[1] - 0.0001 * p[0], zip(grad, net.parameters())))\n",
    "\n",
    "# this is the loss and accuracy before first update\n",
    "\n",
    "\n",
    "\n",
    "# this is the loss and accuracy after the first update\n",
    "with torch.no_grad():\n",
    "\n",
    "    total_logits_q = net(qry_image,fast_weights , bn_training=False)\n",
    "    total_pred_q = F.softmax(total_logits_q, dim=1).argmax(dim=1)\n",
    "\n",
    "    total_q_correct = torch.eq(total_pred_q, qry_label).sum().item()\n",
    "    corrects[\"total_query_nway\"][1] += total_q_correct\n",
    "    loss_q = F.cross_entropy(total_logits_q, qry_label)\n",
    "\n",
    "\n",
    "for k in range(1, 10):\n",
    "    # 1. run the i-th task and compute loss for k=1~K-1\n",
    "    logits = net(spt_image, fast_weights, bn_training=True)\n",
    "    loss = F.cross_entropy(logits, spt_label)\n",
    "    # 2. compute grad on theta_pi\n",
    "    grad = torch.autograd.grad(loss, fast_weights)\n",
    "    # 3. theta_pi = theta_pi - train_lr * grad\n",
    "    fast_weights = list(map(lambda p: p[1] - 0.0001 * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "    logits_q = net(qry_image, fast_weights, bn_training=True)\n",
    "    # loss_q will be overwritten and just keep the loss_q on last update step.\n",
    "    loss_q = F.cross_entropy(logits_q, qry_label)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        total_logits_q = net(qry_image,fast_weights , bn_training=False)\n",
    "\n",
    "        total_pred_q = F.softmax(total_logits_q, dim=1).argmax(dim=1)\n",
    "        # print(F.softmax(total_logits_q, dim=1))\n",
    "        total_q_correct = torch.eq(total_pred_q, qry_label).sum().item()\n",
    "        corrects[\"total_query_nway\"][k+1] += total_q_correct\n",
    "        loss_q = F.cross_entropy(total_logits_q, qry_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdee566-1964-4108-8157-a93615e2dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3c481c-2cd8-4811-9a5c-e669d80b20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "invTrans = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                                     std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "                                transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
    "                                                     std = [ 1., 1., 1. ])])\n",
    "inv_tensor = invTrans(qry_image)\n",
    "for i in range(len(inv_tensor)):\n",
    "\n",
    "\n",
    "    img = (inv_tensor[i])\n",
    "    # img = (img*255).long()\n",
    "    plt.figure()\n",
    "    plt.imshow(img.cpu().numpy().transpose(1,2,0))\n",
    "    plt.show()\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43bbd0-767f-442e-826d-cb93076db042",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca10bb1-2744-47f7-93f8-3375e3e61e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769bd609-1888-4523-9336-9ed58ad53222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "values = np.array([1,2,3,1,2,4,5,6,3,2,1])\n",
    "searchval = 3\n",
    "np.where(values == searchval)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18240835-1b73-463f-b7c4-d6e50becf8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice([5, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2804a2-3ff1-4a7f-9d7e-1e579ac0e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_spt[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc915c4a-7c38-43e1-b835-b9600e0953b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "invTrans = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                                     std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "                                transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
    "                                                     std = [ 1., 1., 1. ])])\n",
    "inv_tensor = invTrans(x_spt[0])\n",
    "for i in range(len(x_spt[0])):\n",
    "    print(i)\n",
    "    i = (x_spt[2][i])\n",
    "    # i = (i*255).long()\n",
    "    plt.figure()\n",
    "    plt.imshow(i.cpu().numpy().transpose(1,2,0))\n",
    "    plt.show()\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a84662-4834-4dac-9314-53ab8cfee4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_spt[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ec928-60f1-4f8a-8ffb-6e29a58e698c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metagan",
   "language": "python",
   "name": "metagan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
