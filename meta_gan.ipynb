{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9034beb-c9ed-4806-bef5-c1334415cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch\n",
    "from    torch import nn\n",
    "from    torch import optim\n",
    "from    torch.nn import functional as F\n",
    "from    torch.utils.data import TensorDataset, DataLoader\n",
    "from    torch import optim\n",
    "import  numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from   models.learner import Learner\n",
    "from models.generator import Generator\n",
    "from    copy import deepcopy\n",
    "import os\n",
    "from torchsummary import summary\n",
    "\n",
    "from utils.dataloader import train_data_gen , test_data_gen\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "# import tqdm.notebook as tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d53ff7f-c937-4105-a1f6-dfd8b919a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('configs/0613_maml_gen.json') as json_file:\n",
    "    args = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24889abc-a8cf-4ba6-b0f0-7b8e2611f95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 96000, 'n_way': 5, 'k_spt': 5, 'k_qry': 15, 'img_sz': 84, 'tasks_per_batch': 3, 'img_c': 3, 'meta_gen_lr': 0.0005, 'meta_discrim_lr': 0.0001, 'update_lr': 0.004, 'update_steps': 2, 'update_steps_test': 2, 'loss': 'cross_entropy', 'min_learning_rate': 1e-15, 'number_of_training_steps_per_iter': 4, 'multi_step_loss_num_epochs': 15, 'spy_gen_num': 5, 'qry_gen_num': 25, 'num_distractor': 0, 'batch_for_gradient': 50, 'no_save': 0, 'learn_inner_lr': 0, 'create_graph': 0, 'msl': 0, 'single_fast_test': 0, 'consine_schedule': 0, 'save_path': '0612_maml_gen'}\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e5f2933-f22d-442f-8001-aedb930cafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(\"images/\" + args[\"save_path\"]):\n",
    "#     shutil.rmtree(\"images/\" + args[\"save_path\"])\n",
    "    \n",
    "# if os.path.exists(\"data/\" + args[\"save_path\"]):\n",
    "#     shutil.rmtree(\"data/\" + args[\"save_path\"])\n",
    "    \n",
    "# if os.path.exists(\"save_models/\" + args[\"save_path\"]):\n",
    "#     shutil.rmtree(\"save_models/\" + args[\"save_path\"])\n",
    "    \n",
    "# if os.path.exists(\"runs/\" + args[\"save_path\"]):\n",
    "#     shutil.rmtree(\"runs/\" + args[\"save_path\"])    \n",
    "\n",
    "writer = SummaryWriter('runs/' + args[\"save_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5219417-95fd-442b-810e-3ae324fde36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    if not os.path.exists(\"images/\" + path):\n",
    "        os.makedirs(\"images/\" + path)\n",
    "        \n",
    "    if not os.path.exists(\"data/\" + path):\n",
    "        os.makedirs(\"data/\" + path)\n",
    "        \n",
    "    if not os.path.exists(\"save_models/\" + path):\n",
    "        os.makedirs(\"save_models/\" + path)        \n",
    "\n",
    "def save_imgs(path, imgs, step):\n",
    "\n",
    "    some_imgs = np.reshape(imgs, [imgs.shape[0]*imgs.shape[1], -1])[0:50]\n",
    "\n",
    "    # save png of imgs\n",
    "    i = 0\n",
    "    for flat_img in some_imgs:\n",
    "        img = flat_img.reshape(3,84,84).swapaxes(0,1).swapaxes(1,2)\n",
    "        im = ((img - np.min(img))*255/(np.max(img - np.min(img)))).astype(np.uint8)\n",
    "        if i < 15:\n",
    "            plt.subplot(5, 3, i+1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(im)\n",
    "        i += 1\n",
    "    plt.savefig(\"images/\" + path + \"/images_step\" + str(step) + \".png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8c83a7d-2d03-4a4d-9590-54f2f78bf957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load datasets/BelgiumTSC\n",
      "load complete time 0.33933115005493164\n",
      "load datasets/ArTS\n",
      "load complete time 0.3034861087799072\n",
      "load datasets/chinese_traffic_sign\n",
      "load complete time 0.6109473705291748\n",
      "load datasets/CVL\n",
      "load complete time 0.42543721199035645\n",
      "load datasets/FullJCNN2013\n",
      "load complete time 0.22013282775878906\n",
      "load datasets/logo_2k\n",
      "load complete time 0.9927217960357666\n",
      "load datasets/GTSRB\n",
      "load complete time 0.0888059139251709\n",
      "load datasets/DFG\n",
      "load complete time 0.03560972213745117\n"
     ]
    }
   ],
   "source": [
    "train_data_generator = train_data_gen(args)\n",
    "test_data_generator = test_data_gen(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94799fe8-a071-4f5e-9068-ab7abb8e7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = 64\n",
    "discriminator_config = [\n",
    "    ('conv2d', [ndf, 3, 4, 4, 2, 1]),\n",
    "    ('leakyrelu', [0.2,True]),\n",
    "    # ('bn', [ndf]),\n",
    "    \n",
    "    ('conv2d', [ndf*2, ndf, 4, 4, 2, 1]),\n",
    "    ('bn', [ndf*2]),\n",
    "    ('leakyrelu', [0.2,True]),\n",
    "\n",
    "    ('conv2d', [ndf*4, ndf*2, 4, 4, 2, 1]),\n",
    "    ('bn', [ndf*4]),\n",
    "    ('leakyrelu', [0.2,True]),\n",
    "    \n",
    "    \n",
    "    ('conv2d', [ndf*8, ndf*4, 4, 4, 2, 1]),\n",
    "    ('bn', [ndf*8]),\n",
    "    ('leakyrelu', [0.2,True]),\n",
    "    \n",
    "    ('conv2d', [1,ndf*8 , 2, 2, 1, 0]),\n",
    "    ('flatten', []),\n",
    "    ('linear',[6, 16]),\n",
    "    ('softmax',[])\n",
    "]\n",
    "nz = 100\n",
    "ngf = 64\n",
    "gen_config = [\n",
    "    ('convert_z',[]),\n",
    "    ('convt2d',[nz,ngf*8,4,4,1,0]),\n",
    "    ('bn',[ngf * 8]),\n",
    "    ('leakyrelu', [.2, True]),  \n",
    "    \n",
    "    ('convt2d',[ngf*8,ngf*4,4,4,2,0]),\n",
    "    ('bn',[ngf * 4]),\n",
    "    ('leakyrelu', [.2, True]),  \n",
    "    \n",
    "    ('convt2d',[ngf*4,ngf*2,4,4,2,0]),\n",
    "    ('bn',[ngf * 2]),\n",
    "    ('leakyrelu', [.2, True]),  \n",
    "    \n",
    "    ('convt2d',[ngf*2,ngf,3,3,2,1]),\n",
    "    ('bn',[ngf]),\n",
    "    ('leakyrelu', [.2, True]),      \n",
    "    \n",
    "    ('convt2d',[ngf,3,3,3,2,1]),\n",
    "    ('convt2d',[3,3,2,2,1,1]),\n",
    "    (\"tanh\",[])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce16fb30-6933-4607-ac3c-8712f6e9b36e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Meta(nn.Module):\n",
    "    \"\"\"\n",
    "    Meta Learner with GAN incorporated\n",
    "    \"\"\"\n",
    "    def __init__(self, args, discriminator_config, gen_config):\n",
    "        \"\"\"\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(Meta, self).__init__()\n",
    "        \n",
    "        cuda = torch.cuda.is_available()\n",
    "        self.FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor \n",
    "        self.LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "        self.total_epochs = args[\"epoch\"]   \n",
    "        # model parameters config\n",
    "        self.meta_gen_lr = args[\"meta_gen_lr\"]\n",
    "        self.meta_discrim_lr = args[\"meta_discrim_lr\"]\n",
    "        \n",
    "        self.update_lr = args[\"update_lr\"]\n",
    "        self.consine_schedule = args[\"consine_schedule\"]\n",
    "        self.update_steps = args[\"update_steps\"]\n",
    "        self.update_steps_test = args[\"update_steps_test\"]\n",
    "\n",
    "        # dataset config\n",
    "        self.img_c = args[\"img_c\"]\n",
    "        self.img_sz = args[\"img_sz\"]        \n",
    "        self.n_way = args[\"n_way\"]\n",
    "        self.k_spt = args[\"k_spt\"]\n",
    "        self.k_qry = args[\"k_qry\"]\n",
    "        self.MSL = args[\"msl\"]\n",
    "        # generator num\n",
    "        self.spy_gen_num = args[\"spy_gen_num\"]\n",
    "        self.qry_gen_num = args[\"qry_gen_num\"]\n",
    "        # query gan batch\n",
    "        self.batch_for_gradient = args[\"batch_for_gradient\"]\n",
    "        self.fix_noise = torch.randn(self.qry_gen_num, nz,1,1, device=device)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        # load model\n",
    "        self.generator = Generator(gen_config, self.img_c, self.img_sz)\n",
    "        self.discrim_net = Learner(discriminator_config, self.img_c, self.img_sz)\n",
    "        beta1 = 0.0\n",
    "        beta2 = 0.0\n",
    "        \n",
    "        self.meta_gen_optim = optim.Adam(self.generator.parameters(), lr=self.meta_gen_lr,betas=(beta1, 0.9))\n",
    "        self.meta_d_optim = optim.Adam(self.discrim_net.parameters(), lr=self.meta_discrim_lr,betas=(beta2, 0.9))\n",
    "        if self.consine_schedule:\n",
    "            self.min_learning_rate = 1e-8\n",
    "            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=self.meta_d_optim, T_max=self.total_epochs,\n",
    "                                                                  eta_min=self.min_learning_rate)\n",
    "        self.real_value = 1\n",
    "        self.fake_value = 0\n",
    "        self.discrim_fake = 5\n",
    "    def cross_entropy(self, output,label):\n",
    "        output = torch.log(output)\n",
    "        loss = F.nll_loss(output,label)\n",
    "        return loss\n",
    "    \n",
    "    def pred(self, x, weights=None, nets=None, nway=True, discrim=True, conditions=False):\n",
    "        if weights == None:\n",
    "            discrim_weights = self.discrim_net.parameters()\n",
    "        else:\n",
    "            discrim_weights = weights\n",
    "\n",
    "        discrim_logits = self.discrim_net(x, vars=discrim_weights, bn_training=True) if discrim else None\n",
    "          \n",
    "        return discrim_logits\n",
    "\n",
    "    def get_num_corrects(self, y, x=None, weights=None):\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            discrim_logits = self.pred(x, weights=weights)\n",
    "            pred_q = discrim_logits.argmax(dim=1)\n",
    "\n",
    "            nway_correct = torch.eq(pred_q, y).sum().item()\n",
    "            \n",
    "            other = torch.tensor([5]*len(discrim_logits)).cuda()\n",
    "            other_correct = torch.eq(pred_q, other).sum().item()\n",
    "        return nway_correct, other_correct\n",
    "\n",
    "        \n",
    "    def update_weights(self, net_losses, net_weights,learned_lrs, gen=False):\n",
    "        if gen:\n",
    "            update_lr = self.gen_update_lr\n",
    "        else:\n",
    "            update_lr = self.update_lr\n",
    "        # grad = torch.autograd.grad(net_losses, net_weights, retain_graph=True, create_graph=self.create_graph)\n",
    "        grad = torch.autograd.grad(net_losses, net_weights)\n",
    "        weights = list(map(lambda p: p[1] - update_lr * p[0], zip(grad, net_weights)))\n",
    "\n",
    "        return weights\n",
    "    \n",
    "    def meta_test(self,qry_img,qry_label,discrim_weight,gen_weight):\n",
    "        ### discriminator train\n",
    "        q_real_discrim_logits = self.pred(qry_img, weights=discrim_weight)\n",
    "\n",
    "        real_discrim_loss_q = self.cross_entropy(q_real_discrim_logits, qry_label)\n",
    "\n",
    "        discrim_fake_label = torch.full((self.qry_gen_num,), self.discrim_fake, dtype=torch.long, device=device) \n",
    "        noise = torch.randn(self.qry_gen_num, nz,1,1, device=device)\n",
    "        q_gen = torch.empty(0,3,84,84).cuda()\n",
    "        if self.qry_gen_num < self.batch_for_gradient:\n",
    "\n",
    "            q_gen = self.generator(qry_img, noise , vars=gen_weight)\n",
    "        else:\n",
    "            for i in range(self.qry_gen_num//self.batch_for_gradient):\n",
    "\n",
    "                noise_tmp = noise[i*self.batch_for_gradient:(i+1)*self.batch_for_gradient]\n",
    "\n",
    "                q_gen = torch.cat([q_gen,self.generator(qry_img[i*self.batch_for_gradient:(i+1)*self.batch_for_gradient], noise_tmp , vars=gen_weight)])\n",
    "        q_fake_discrim_logits = self.pred(q_gen.detach(), weights=discrim_weight)\n",
    "        fake_discrim_loss_q = self.cross_entropy(q_fake_discrim_logits, discrim_fake_label)\n",
    "        if torch.isnan(fake_discrim_loss_q):\n",
    "            print(\"fake d loss error\")\n",
    "        if torch.isnan(real_discrim_loss_q):\n",
    "            print(\"real d loss error\")\n",
    "        d_loss_q = (fake_discrim_loss_q + real_discrim_loss_q)\n",
    "        \n",
    "        ### generator train\n",
    "        gen_fake_label = torch.full((self.qry_gen_num,), self.real_value, dtype=torch.float, device=device)\n",
    "        gen_q_discrim = 1 - self.pred(q_gen, weights=discrim_weight)[:,-1]\n",
    "        g_loss_q = self.criterion(gen_q_discrim, gen_fake_label)\n",
    "        if torch.isnan(g_loss_q):\n",
    "            print(\"g loss error\")\n",
    "            \n",
    "        return d_loss_q, g_loss_q\n",
    "\n",
    "    def single_task_forward(self, x_spt, y_spt, x_qry, y_qry, update_steps,nets=None, images=False):\n",
    "        \n",
    "        corrects = {key: np.zeros(update_steps + 1) for key in \n",
    "                        [\n",
    "                        \"query_nway\", # number of meta-test (query) images correctly discriminated\n",
    "                        \"predict_other\",\n",
    "                        \"gen_discrim\", # number of generated images correctly discriminated\n",
    "                        ]}\n",
    "\n",
    "        support_sz, c_, h, w = x_spt.size()\n",
    "        nz = 100\n",
    "\n",
    "        discrim_weights,gen_weights = [x.parameters() for x in nets]\n",
    "\n",
    "        # this is the meta-test loss and accuracy before first update\n",
    "\n",
    "        q_discrim,other = self.get_num_corrects(y=y_qry, weights=None, x=x_qry)\n",
    "        corrects[\"query_nway\"][0] += q_discrim\n",
    "        corrects[\"predict_other\"][0] += other\n",
    "        # run the i-th task and compute loss for k-th inner update\n",
    "        query_fake_label = torch.full((self.qry_gen_num,), self.discrim_fake, dtype=torch.long, device=device)\n",
    "        for k in range(1, update_steps + 1):\n",
    "            ## discrim loss\n",
    "            noise = torch.randn(self.spy_gen_num, nz , 1, 1, device=device)\n",
    "            x_gen = self.generator(x_spt, noise , vars=gen_weights)\n",
    "            \n",
    "            # update discrim weight\n",
    "\n",
    "            real_discrim_logits = self.pred(x_spt, weights=discrim_weights)\n",
    "\n",
    "            fake_discrim_logits = self.pred(x_gen, weights=discrim_weights)\n",
    "\n",
    "            fake_label = torch.full((self.spy_gen_num,), self.discrim_fake, dtype=torch.long, device=device)\n",
    "            \n",
    "            real_discrim_loss = self.cross_entropy(real_discrim_logits, y_spt)\n",
    "            fake_discrim_loss = self.cross_entropy(fake_discrim_logits,fake_label)\n",
    "            D_loss = fake_discrim_loss + real_discrim_loss\n",
    "\n",
    "            discrim_weights = self.update_weights(D_loss, discrim_weights,self.update_lr) \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                x_gen = self.generator(x_qry, self.fix_noise , vars=gen_weights) \n",
    "                gen_correct,_ = self.get_num_corrects(y=query_fake_label,x=x_gen, weights=discrim_weights)\n",
    "                corrects[\"gen_discrim\"][k-1] += gen_correct\n",
    "                \n",
    "                q_discrim_correct,other = self.get_num_corrects(y=y_qry, x=x_qry, weights=discrim_weights)\n",
    "                corrects['query_nway'][k] += q_discrim_correct\n",
    "                corrects[\"predict_other\"][k] += other\n",
    "            # meta-test nway and discrim accuracy\n",
    "            # [query_sz]\n",
    "\n",
    "\n",
    "        \n",
    "        # final gen-discrim and gen-nway accuracy\n",
    "        with torch.no_grad():\n",
    "            x_gen = self.generator(x_qry, self.fix_noise , vars=gen_weights)\n",
    "            gen_correct,_ = self.get_num_corrects(y=query_fake_label,x=x_gen, weights=discrim_weights)\n",
    "            corrects[\"gen_discrim\"][-1] += gen_correct\n",
    "        d_loss_q, g_loss_q = self.meta_test(x_qry,y_qry,discrim_weights,gen_weights)\n",
    "            \n",
    "        if images:\n",
    "            return d_loss_q,g_loss_q, corrects, x_gen\n",
    "        else:\n",
    "            return d_loss_q,g_loss_q, corrects\n",
    "\n",
    "    def forward(self, x_spt, y_spt, x_qry, y_qry,step):\n",
    "        \"\"\"\n",
    "        :param x_spt:   [b, support_sz, c_, h, w]\n",
    "        :param y_spt:   [b, support_sz]\n",
    "        :param x_qry:   [b, query_sz, c_, h, w]\n",
    "        :param y_qry:   [b, query_sz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.current_epoch = step \n",
    "        tasks_per_batch, support_sz, c_, h, w = x_spt.size()\n",
    "        query_sz = x_qry.size(1)\n",
    "        g_loss_q = 0\n",
    "        d_loss_q = 0\n",
    "        gen_losses_q = [0 for _ in range(self.update_steps + 1)]\n",
    "        discrim_losses_q = [0 for _ in range(self.update_steps + 1)]\n",
    "        corrects = {key: np.zeros(self.update_steps + 1) for key in \n",
    "                        [\n",
    "                        \"query_nway\", # number of meta-test (query) images correctly discriminated\n",
    "                        \"predict_other\",\n",
    "                        \"gen_discrim\", # number of generated images correctly discriminated\n",
    "                        ]}\n",
    "        net = [self.discrim_net,self.generator]\n",
    "        for i in range(tasks_per_batch):\n",
    "            d_loss_q_tmp,g_loss_q_tmp, corrects_tmp = self.single_task_forward(x_spt[i], y_spt[i], x_qry[i], y_qry[i],self.update_steps,nets = net,images=False)\n",
    "            g_loss_q += g_loss_q_tmp\n",
    "            d_loss_q += d_loss_q_tmp\n",
    "            assert len(corrects_tmp.keys()) == len(corrects.keys())\n",
    "            for key in corrects.keys():\n",
    "                corrects[key] += corrects_tmp[key]\n",
    "            \n",
    "        # end of all tasks\n",
    "        # sum over final losses on query set across all tasks\n",
    "        g_loss_q /= tasks_per_batch\n",
    "        self.meta_gen_optim.zero_grad()\n",
    "        g_loss_q.backward()\n",
    "        self.meta_gen_optim.step()        \n",
    "\n",
    "        # optimize theta parameters\n",
    "        d_loss_q /= tasks_per_batch\n",
    "        self.meta_d_optim.zero_grad()\n",
    "        d_loss_q.backward()\n",
    "        self.meta_d_optim.step()\n",
    "        \n",
    "        accs = {}\n",
    "        accs[\"query_nway\"] = corrects[\"query_nway\"] / (tasks_per_batch * query_sz)\n",
    "        accs[\"predict_other\"] = corrects[\"predict_other\"] / (tasks_per_batch * query_sz)\n",
    "        accs[\"gen_discrim\"] = corrects[\"gen_discrim\"] / (tasks_per_batch * self.qry_gen_num)\n",
    "        return accs,d_loss_q,g_loss_q\n",
    "\n",
    "    def finetunning(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x_spt:   [support_sz, c_, h, w]\n",
    "        :param y_spt:   [support_sz]\n",
    "        :param x_qry:   [query_sz, c_, h, w]\n",
    "        :param y_qry:   [query_sz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        support_sz, c_, h, w = x_spt.size()\n",
    "\n",
    "        assert len(x_spt.shape) == 4\n",
    "\n",
    "        query_sz = x_qry.size(0)\n",
    "\n",
    "        # in order to not ruin the state of running_mean/variance and bn_weight/bias\n",
    "        # we finetunning on the copied model instead of self.net\n",
    "        \n",
    "        discrim_net = deepcopy(self.discrim_net)\n",
    "        generator = deepcopy(self.generator)\n",
    "        net = [self.discrim_net,self.generator]\n",
    "        d_loss_q,g_loss_q, corrects, imgs = self.single_task_forward(x_spt, y_spt, x_qry, y_qry,self.update_steps_test, nets=net,images=True)\n",
    "\n",
    "        del discrim_net\n",
    "        \n",
    "        accs[\"query_nway\"] = corrects[\"query_nway\"] / (query_sz)\n",
    "        accs[\"predict_other\"] = corrects[\"predict_other\"] / (query_sz)\n",
    "        accs[\"gen_discrim\"] = corrects[\"gen_discrim\"] / (self.qry_gen_num)\n",
    "\n",
    "        return accs, imgs,d_loss_q,g_loss_q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14bddb2-264f-4a65-965b-bb567e7197c4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/96000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "d loss: 2.363431215286255\n",
      "g loss: 1.2237510681152344\n",
      "accs {'query_nway': array([0.17333333, 0.34666667, 0.37777778]), 'predict_other': array([0.18222222, 0.17333333, 0.16444444]), 'gen_discrim': array([0.58666667, 0.77333333, 0.77333333])}\n",
      "d loss: 2.6459457620978357\n",
      "g loss: 0.8663842096924782\n",
      "Test acc: {'query_nway': array([0.16      , 0.53333333, 0.57333333]), 'predict_other': array([0.16      , 0.22666667, 0.26666667]), 'gen_discrim': array([0.08, 0.64, 0.64])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 101/96000 [00:23<6:41:00,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100\n",
      "d loss: 1.7167425155639648\n",
      "g loss: 3.443291187286377\n",
      "accs {'query_nway': array([0.18666667, 0.23111111, 0.29333333]), 'predict_other': array([0.00888889, 0.05777778, 0.06666667]), 'gen_discrim': array([0.90666667, 1.        , 1.        ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 202/96000 [00:39<3:35:30,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200\n",
      "d loss: 1.26929771900177\n",
      "g loss: 4.7000532150268555\n",
      "accs {'query_nway': array([0.23111111, 0.41333333, 0.53777778]), 'predict_other': array([0., 0., 0.]), 'gen_discrim': array([1., 1., 1.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 300/96000 [00:57<4:09:59,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 300\n",
      "d loss: 1.1243021488189697\n",
      "g loss: 4.804119110107422\n",
      "accs {'query_nway': array([0.24      , 0.43111111, 0.55111111]), 'predict_other': array([0.        , 0.01333333, 0.00888889]), 'gen_discrim': array([0.92, 1.  , 1.  ])}\n",
      "d loss: 1.3644985228776931\n",
      "g loss: 3.9385994017124175\n",
      "Test acc: {'query_nway': array([0.16      , 0.38666667, 0.49333333]), 'predict_other': array([0., 0., 0.]), 'gen_discrim': array([1., 1., 1.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                    | 402/96000 [01:17<3:37:17,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400\n",
      "d loss: 1.5945181846618652\n",
      "g loss: 2.6161673069000244\n",
      "accs {'query_nway': array([0.20444444, 0.12888889, 0.42666667]), 'predict_other': array([0.12888889, 0.63555556, 0.        ]), 'gen_discrim': array([1.        , 0.97333333, 0.97333333])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏                                    | 502/96000 [01:34<3:31:54,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500\n",
      "d loss: 1.6586192846298218\n",
      "g loss: 5.451716423034668\n",
      "accs {'query_nway': array([0.20888889, 0.44      , 0.51555556]), 'predict_other': array([0.02666667, 0.05333333, 0.03111111]), 'gen_discrim': array([0.90666667, 0.90666667, 0.90666667])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏                                    | 600/96000 [01:50<4:46:03,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 600\n",
      "d loss: 1.2928227186203003\n",
      "g loss: 6.15001106262207\n",
      "accs {'query_nway': array([0.19111111, 0.37333333, 0.46666667]), 'predict_other': array([0., 0., 0.]), 'gen_discrim': array([1., 1., 1.])}\n",
      "d loss: 1.1821458883583547\n",
      "g loss: 6.106702932715416\n",
      "Test acc: {'query_nway': array([0.33333333, 0.53333333, 0.61333333]), 'predict_other': array([0., 0., 0.]), 'gen_discrim': array([1., 1., 1.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                    | 702/96000 [02:11<3:37:55,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 700\n",
      "d loss: 1.6518462896347046\n",
      "g loss: 4.654877662658691\n",
      "accs {'query_nway': array([0.03555556, 0.25333333, 0.38222222]), 'predict_other': array([0.57333333, 0.02222222, 0.06666667]), 'gen_discrim': array([0.76      , 0.98666667, 0.98666667])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                    | 802/96000 [02:27<4:23:30,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 800\n",
      "d loss: 1.1354906558990479\n",
      "g loss: 4.807629108428955\n",
      "accs {'query_nway': array([0.20888889, 0.45333333, 0.56888889]), 'predict_other': array([0., 0., 0.]), 'gen_discrim': array([1., 1., 1.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                    | 900/96000 [02:46<4:09:04,  6.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 900\n",
      "d loss: 1.508542537689209\n",
      "g loss: 4.495633125305176\n",
      "accs {'query_nway': array([0.11111111, 0.45777778, 0.50222222]), 'predict_other': array([0.52888889, 0.01777778, 0.05333333]), 'gen_discrim': array([0.64      , 0.90666667, 0.90666667])}\n",
      "d loss: 1.3559754610061645\n",
      "g loss: 3.765403465926647\n",
      "Test acc: {'query_nway': array([0.        , 0.29333333, 0.68      ]), 'predict_other': array([0.85333333, 0.        , 0.        ]), 'gen_discrim': array([0.16, 0.92, 0.92])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                   | 1002/96000 [03:03<3:25:47,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000\n",
      "d loss: 1.0568253993988037\n",
      "g loss: 4.226287841796875\n",
      "accs {'query_nway': array([0.26222222, 0.48444444, 0.61333333]), 'predict_other': array([0., 0., 0.]), 'gen_discrim': array([1., 1., 1.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                   | 1102/96000 [03:18<3:34:06,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1100\n",
      "d loss: 1.2637505531311035\n",
      "g loss: 8.159392356872559\n",
      "accs {'query_nway': array([0.        , 0.30666667, 0.46666667]), 'predict_other': array([1., 0., 0.]), 'gen_discrim': array([1., 1., 1.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                   | 1200/96000 [03:35<5:19:15,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1200\n",
      "d loss: 1.0161058902740479\n",
      "g loss: 5.44084358215332\n",
      "accs {'query_nway': array([0.19555556, 0.51111111, 0.59555556]), 'predict_other': array([0., 0., 0.]), 'gen_discrim': array([1., 1., 1.])}\n",
      "d loss: 1.1787633046507835\n",
      "g loss: 5.65355912744999\n",
      "Test acc: {'query_nway': array([0.34666667, 0.50666667, 0.73333333]), 'predict_other': array([0., 0., 0.]), 'gen_discrim': array([1., 1., 1.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                   | 1302/96000 [03:55<3:45:05,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1300\n",
      "d loss: 1.3308796882629395\n",
      "g loss: 3.334371566772461\n",
      "accs {'query_nway': array([0.14222222, 0.16      , 0.48      ]), 'predict_other': array([0.11111111, 0.49333333, 0.00888889]), 'gen_discrim': array([1., 1., 1.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                   | 1402/96000 [04:11<3:34:18,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1400\n",
      "d loss: 1.4642432928085327\n",
      "g loss: 8.723167419433594\n",
      "accs {'query_nway': array([0.09777778, 0.32      , 0.41777778]), 'predict_other': array([0.04, 0.  , 0.  ]), 'gen_discrim': array([1., 1., 1.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▌                                   | 1500/96000 [04:29<3:33:16,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1500\n",
      "d loss: 1.1851251125335693\n",
      "g loss: 6.390166282653809\n",
      "accs {'query_nway': array([0.22222222, 0.46222222, 0.55555556]), 'predict_other': array([0., 0., 0.]), 'gen_discrim': array([1., 1., 1.])}\n",
      "d loss: 1.1669722333550454\n",
      "g loss: 5.367955774068832\n",
      "Test acc: {'query_nway': array([0.22666667, 0.37333333, 0.52      ]), 'predict_other': array([0., 0., 0.]), 'gen_discrim': array([1., 1., 1.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▌                                   | 1602/96000 [04:50<4:35:11,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1600\n",
      "d loss: 0.9553803205490112\n",
      "g loss: 6.40683650970459\n",
      "accs {'query_nway': array([0.22222222, 0.56      , 0.61333333]), 'predict_other': array([0., 0., 0.]), 'gen_discrim': array([1., 1., 1.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▋                                   | 1702/96000 [05:07<4:51:23,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1700\n",
      "d loss: 1.4141197204589844\n",
      "g loss: 9.670666694641113\n",
      "accs {'query_nway': array([0.18222222, 0.34222222, 0.42666667]), 'predict_other': array([0.00444444, 0.00444444, 0.00444444]), 'gen_discrim': array([1., 1., 1.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▋                                   | 1800/96000 [05:24<4:09:54,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1800\n",
      "d loss: 1.19931960105896\n",
      "g loss: 6.67488956451416\n",
      "accs {'query_nway': array([0.23555556, 0.44      , 0.56444444]), 'predict_other': array([0., 0., 0.]), 'gen_discrim': array([1., 1., 1.])}\n",
      "d loss: 1.0563895933330059\n",
      "g loss: 6.353534096479416\n",
      "Test acc: {'query_nway': array([0.13333333, 0.38666667, 0.46666667]), 'predict_other': array([0., 0., 0.]), 'gen_discrim': array([1., 1., 1.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▋                                   | 1902/96000 [05:44<4:18:18,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1900\n",
      "d loss: 0.7681993246078491\n",
      "g loss: 7.84945821762085\n",
      "accs {'query_nway': array([0.18666667, 0.47555556, 0.67555556]), 'predict_other': array([0., 0., 0.]), 'gen_discrim': array([1., 1., 1.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                   | 2001/96000 [06:04<6:18:25,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000\n",
      "d loss: 1.2263422012329102\n",
      "g loss: 9.844486236572266\n",
      "accs {'query_nway': array([0.21333333, 0.39555556, 0.52888889]), 'predict_other': array([0., 0., 0.]), 'gen_discrim': array([1., 1., 1.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                   | 2021/96000 [06:07<3:42:12,  7.05it/s]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mamlGAN = Meta(args, discriminator_config, gen_config).to(device)\n",
    "step = 0\n",
    "path = args[\"save_path\"]\n",
    "mkdir_p(path)\n",
    "best_acc = []\n",
    "\n",
    "with tqdm.tqdm(initial=step,\n",
    "                   total=int(args[\"epoch\"])) as pbar_train:\n",
    "    for _ in range(args[\"epoch\"] * args[\"tasks_per_batch\"]//6000):\n",
    "        # fetch meta_batchsz num of episode each time\n",
    "        train_dataloader = DataLoader(train_data_generator, args[\"tasks_per_batch\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "        for _, (x_spt, y_spt, x_qry, y_qry) in enumerate(train_dataloader):\n",
    "            x_spt, y_spt, x_qry, y_qry = x_spt.to(device), y_spt.to(device), x_qry.to(device), y_qry.to(device)\n",
    "\n",
    "            accs,d_loss,g_loss = mamlGAN(x_spt, y_spt, x_qry, y_qry,step)\n",
    "            # accs,d_loss = mamlGAN(x_spt, y_spt, x_qry, y_qry,step)\n",
    "            writer.add_scalar('Loss/train_d_loss', d_loss, step)\n",
    "            writer.add_scalar('Loss/train_g_loss', g_loss, step)\n",
    "            writer.add_scalar('Accuracy/query_nway', accs[\"query_nway\"][-1], step)\n",
    "            writer.add_scalar('Accuracy/gen_discrim', accs[\"gen_discrim\"][-1], step)\n",
    "            writer.add_scalar('Accuracy/predict_other', accs[\"predict_other\"][-1],step)\n",
    "            if step % 100 == 0:\n",
    "                print(\"step \" + str(step))\n",
    "                print('d loss:',d_loss.item())\n",
    "                print('g loss:',g_loss.item())\n",
    "                print(\"accs\",accs)\n",
    "\n",
    "\n",
    "            if step % 300 == 0:  # evaluation\n",
    "                db_test = DataLoader(test_data_generator, 1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "                accs_all_test = []\n",
    "                imgs_all_test = []\n",
    "                d_loss_all_test = []\n",
    "                g_loss_all_test = []\n",
    "                for x_spt, y_spt, x_qry, y_qry in db_test:\n",
    "                    x_spt, y_spt, x_qry, y_qry = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device), \\\n",
    "                                                 x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device)\n",
    "\n",
    "                    # accs, d_loss = mamlGAN.finetunning(x_spt, y_spt, x_qry, y_qry)\n",
    "                    accs, imgs,d_loss,g_loss = mamlGAN.finetunning(x_spt, y_spt, x_qry, y_qry)\n",
    "\n",
    "\n",
    "                    accs_all_test.append(accs)\n",
    "                    imgs_all_test.append(imgs.cpu().detach().numpy())\n",
    "                    d_loss_all_test.append(d_loss.item())\n",
    "                    g_loss_all_test.append(g_loss.item())\n",
    "\n",
    "                imgs_all_test = np.array(imgs_all_test)\n",
    "                # [b, update_step+1]\n",
    "                # accs = np.array(accs_all_test).mean(axis=0).astype(np.float16)\n",
    "                d_loss = np.mean(np.array(d_loss_all_test))\n",
    "                g_loss = np.mean(np.array(g_loss_all_test))\n",
    "\n",
    "                print('d loss:',d_loss)\n",
    "                print('g loss:',g_loss)\n",
    "                print('Test acc:', accs)    \n",
    "\n",
    "                writer.add_scalar('Loss/test_d_loss', d_loss, step)\n",
    "                writer.add_scalar('Loss/test_g_loss', g_loss, step)\n",
    "                writer.add_scalar('Accuracy/test_query_nway', accs[\"query_nway\"][-1], step)\n",
    "                writer.add_scalar('Accuracy/test_gen_discrim', accs[\"gen_discrim\"][-1], step)\n",
    "                writer.add_scalar('Accuracy/test_predict_other', accs[\"predict_other\"][-1],step)\n",
    "                if not len(best_acc):\n",
    "                    best_acc = accs\n",
    "                    best_epoch = step\n",
    "                    torch.save({'model_state_dict': mamlGAN.state_dict()}, \"save_models/\" + path + \"/best.pth\")\n",
    "                else:\n",
    "                    if max(accs) > max(best_acc):\n",
    "                        best_acc = accs\n",
    "                        best_epoch = step\n",
    "                        torch.save({'model_state_dict': mamlGAN.state_dict()}, \"save_models/\" + path + \"/best.pth\")\n",
    "                torch.save({'model_state_dict': mamlGAN.state_dict()}, \"save_models/\" + path + \"/model_step\" + str(step) + \".pth\")\n",
    "\n",
    "                save_imgs(path, imgs_all_test, step)\n",
    "\n",
    "            step = step + 1\n",
    "            pbar_train.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea2a244-3e93-44c2-b93c-5fe16ce8efa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metagan",
   "language": "python",
   "name": "metagan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
