{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9034beb-c9ed-4806-bef5-c1334415cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch\n",
    "from    torch import nn\n",
    "from    torch import optim\n",
    "from    torch.nn import functional as F\n",
    "from    torch.utils.data import TensorDataset, DataLoader\n",
    "from    torch import optim\n",
    "import  numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from    learner import Learner\n",
    "from generator import Generator\n",
    "from    copy import deepcopy\n",
    "from conditioner import Conditioner\n",
    "import os\n",
    "from torchsummary import summary\n",
    "\n",
    "from utils.dataloader import train_data_gen , test_data_gen\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "# import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d53ff7f-c937-4105-a1f6-dfd8b919a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('configs/gen8.json') as json_file:\n",
    "    args = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24889abc-a8cf-4ba6-b0f0-7b8e2611f95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 30000, 'n_way': 5, 'k_spt': 1, 'k_qry': 10, 'img_sz': 84, 'tasks_per_batch': 5, 'img_c': 3, 'meta_gen_lr': 0.0005, 'meta_discrim_lr': 0.0001, 'update_lr': 0.004, 'update_steps': 1, 'update_steps_test': 1, 'loss': 'cross_entropy', 'min_learning_rate': 1e-15, 'number_of_training_steps_per_iter': 4, 'multi_step_loss_num_epochs': 15, 'spy_gen_num': 5, 'qry_gen_num': 25, 'num_distractor': 0, 'batch_for_gradient': 50, 'no_save': 0, 'learn_inner_lr': 0, 'create_graph': 0, 'msl': 0, 'single_fast_test': 0, 'consine_schedule': 0, 'save_path': 'gen13'}\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e5f2933-f22d-442f-8001-aedb930cafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"images/\" + args[\"save_path\"]):\n",
    "    shutil.rmtree(\"images/\" + args[\"save_path\"])\n",
    "    \n",
    "if os.path.exists(\"data/\" + args[\"save_path\"]):\n",
    "    shutil.rmtree(\"data/\" + args[\"save_path\"])\n",
    "    \n",
    "if os.path.exists(\"save_models/\" + args[\"save_path\"]):\n",
    "    shutil.rmtree(\"save_models/\" + args[\"save_path\"])\n",
    "    \n",
    "if os.path.exists(\"runs/\" + args[\"save_path\"]):\n",
    "    shutil.rmtree(\"runs/\" + args[\"save_path\"])    \n",
    "\n",
    "writer = SummaryWriter('runs/' + args[\"save_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5219417-95fd-442b-810e-3ae324fde36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    if not os.path.exists(\"images/\" + path):\n",
    "        os.makedirs(\"images/\" + path)\n",
    "        \n",
    "    if not os.path.exists(\"data/\" + path):\n",
    "        os.makedirs(\"data/\" + path)\n",
    "        \n",
    "    if not os.path.exists(\"save_models/\" + path):\n",
    "        os.makedirs(\"save_models/\" + path)        \n",
    "\n",
    "\n",
    "def save_imgs(path, imgs, step):\n",
    "\n",
    "    some_imgs = np.reshape(imgs, [imgs.shape[0]*imgs.shape[1], -1])[0:50]\n",
    "\n",
    "    # save png of imgs\n",
    "    i = 0\n",
    "    for flat_img in some_imgs:\n",
    "        img = flat_img.reshape(3,84,84).swapaxes(0,1).swapaxes(1,2)\n",
    "        im = ((img - np.min(img))*255/(np.max(img - np.min(img)))).astype(np.uint8)\n",
    "        if i < 15:\n",
    "            plt.subplot(5, 3, i+1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(im)\n",
    "        i += 1\n",
    "    plt.savefig(\"images/\" + path + \"/images_step\" + str(step) + \".png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c83a7d-2d03-4a4d-9590-54f2f78bf957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load datasets/BelgiumTSC\n",
      "load complete time 0.3554813861846924\n",
      "load datasets/ArTS\n",
      "load complete time 0.3645918369293213\n",
      "load datasets/chinese_traffic_sign\n",
      "load complete time 0.6034379005432129\n",
      "load datasets/CVL\n",
      "load complete time 0.4320671558380127\n",
      "load datasets/FullJCNN2013\n",
      "load complete time 0.2325131893157959\n",
      "load datasets/logo_2k\n",
      "load complete time 1.0139472484588623\n",
      "load datasets/GTSRB\n",
      "load complete time 0.07374167442321777\n",
      "load datasets/DFG\n",
      "load complete time 0.030877351760864258\n"
     ]
    }
   ],
   "source": [
    "train_data_generator = train_data_gen(args)\n",
    "test_data_generator = test_data_gen(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94799fe8-a071-4f5e-9068-ab7abb8e7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = 64\n",
    "discriminator_config = [\n",
    "    ('conv2d', [ndf, 3, 4, 4, 2, 1]),\n",
    "    ('leakyrelu', [0.2,True]),\n",
    "    # ('bn', [ndf]),\n",
    "    \n",
    "    ('conv2d', [ndf*2, ndf, 4, 4, 2, 1]),\n",
    "    ('bn', [ndf*2]),\n",
    "    ('leakyrelu', [0.2,True]),\n",
    "\n",
    "    ('conv2d', [ndf*4, ndf*2, 4, 4, 2, 1]),\n",
    "    ('bn', [ndf*4]),\n",
    "    ('leakyrelu', [0.2,True]),\n",
    "    \n",
    "    \n",
    "    ('conv2d', [ndf*8, ndf*4, 4, 4, 2, 1]),\n",
    "    ('bn', [ndf*8]),\n",
    "    ('leakyrelu', [0.2,True]),\n",
    "    \n",
    "    ('conv2d', [1,ndf*8 , 2, 2, 1, 0]),\n",
    "    ('flatten', []),\n",
    "    ('linear',[6, 16]),\n",
    "    ('softmax',[])\n",
    "]\n",
    "\n",
    "nz = 100\n",
    "ngf = 64\n",
    "gen_config = [\n",
    "    ('convert_z',[]),\n",
    "    ('convt2d',[nz,ngf*8,4,4,1,0]),\n",
    "    ('bn',[ngf * 8]),\n",
    "    ('leakyrelu', [.2, True]),  \n",
    "    \n",
    "    ('convt2d',[ngf*8,ngf*4,4,4,2,0]),\n",
    "    ('bn',[ngf * 4]),\n",
    "    ('leakyrelu', [.2, True]),  \n",
    "    \n",
    "    ('convt2d',[ngf*4,ngf*2,4,4,2,0]),\n",
    "    ('bn',[ngf * 2]),\n",
    "    ('leakyrelu', [.2, True]),  \n",
    "    \n",
    "    ('convt2d',[ngf*2,ngf,3,3,2,1]),\n",
    "    ('bn',[ngf]),\n",
    "    ('leakyrelu', [.2, True]),      \n",
    "    \n",
    "    ('convt2d',[ngf,3,3,3,2,1]),\n",
    "    ('convt2d',[3,3,2,2,1,1]),\n",
    "    (\"tanh\",[])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce16fb30-6933-4607-ac3c-8712f6e9b36e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Meta(nn.Module):\n",
    "    \"\"\"\n",
    "    Meta Learner with GAN incorporated\n",
    "    \"\"\"\n",
    "    def __init__(self, args, discriminator_config, gen_config):\n",
    "        \"\"\"\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(Meta, self).__init__()\n",
    "        \n",
    "        cuda = torch.cuda.is_available()\n",
    "        self.FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor \n",
    "        self.LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "        self.total_epochs = args[\"epoch\"]   \n",
    "        # model parameters config\n",
    "        self.meta_gen_lr = args[\"meta_gen_lr\"]\n",
    "        self.meta_discrim_lr = args[\"meta_discrim_lr\"]\n",
    "        \n",
    "        self.update_lr = args[\"update_lr\"]\n",
    "        \n",
    "        self.update_steps = args[\"update_steps\"]\n",
    "        self.update_steps_test = args[\"update_steps_test\"]\n",
    "        \n",
    "        # dataset config\n",
    "        self.img_c = args[\"img_c\"]\n",
    "        self.img_sz = args[\"img_sz\"]        \n",
    "        self.n_way = args[\"n_way\"]\n",
    "        self.k_spt = args[\"k_spt\"]\n",
    "        self.k_qry = args[\"k_qry\"]\n",
    "        self.MSL = args[\"msl\"]\n",
    "        # generator num\n",
    "        self.spy_gen_num = args[\"spy_gen_num\"]\n",
    "        self.qry_gen_num = args[\"qry_gen_num\"]\n",
    "        # query gan batch\n",
    "        self.batch_for_gradient = args[\"batch_for_gradient\"]\n",
    "        self.fix_noise = torch.randn(self.batch_for_gradient, nz,1,1, device=device)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        # load model\n",
    "        self.generator = Generator(gen_config, self.img_c, self.img_sz)\n",
    "        self.discrim_net = Learner(discriminator_config, self.img_c, self.img_sz)\n",
    "        beta1 = 0.0\n",
    "        beta2 = 0.0\n",
    "\n",
    "\n",
    "        self.meta_gen_optim = optim.Adam(self.generator.parameters(), lr=self.meta_gen_lr,betas=(beta1, 0.9))\n",
    "        self.meta_d_optim = optim.Adam(self.discrim_net.parameters(), lr=self.meta_discrim_lr,betas=(beta2, 0.9))\n",
    "\n",
    "\n",
    "        self.real_value = 1\n",
    "        self.fake_value = 0\n",
    "    \n",
    "    def pred(self, x, weights=None, nets=None, nway=True, discrim=True, conditions=False):\n",
    "        if weights == None:\n",
    "            discrim_weights = self.discrim_net.parameters()\n",
    "        else:\n",
    "            discrim_weights = weights\n",
    "\n",
    "        discrim_logits = self.discrim_net(x, vars=discrim_weights, bn_training=True) if discrim else None\n",
    "          \n",
    "        return discrim_logits\n",
    "\n",
    "    def get_num_corrects(self, y, x=None, weights=None):\n",
    "            \n",
    "        with torch.no_grad():\n",
    "\n",
    "            discrim_logits = self.pred(x, weights=weights)[:,0]\n",
    "            nway_correct = (discrim_logits).mean().item()\n",
    "\n",
    "        return nway_correct\n",
    "\n",
    "    def update_weights(self, net_losses, net_weights,learned_lrs):\n",
    "\n",
    "        update_lr = self.update_lr\n",
    "        grad = torch.autograd.grad(net_losses, net_weights)\n",
    "        weights = list(map(lambda p: p[1] - update_lr * p[0], zip(grad, net_weights)))\n",
    "\n",
    "        return weights\n",
    "    \n",
    "    def meta_test(self,qry_img,qry_label,discrim_weight,gen_weight):\n",
    "        ### discriminator train\n",
    "        q_real_discrim_logits = self.pred(qry_img, weights=discrim_weight)[:,0]\n",
    "\n",
    "        real_discrim_loss_q = self.criterion(q_real_discrim_logits, qry_label)\n",
    "\n",
    "        discrim_fake_label = torch.full((self.qry_gen_num,), self.fake_value, dtype=torch.float, device=device) \n",
    "        noise = torch.randn(self.qry_gen_num, nz,1,1, device=device)\n",
    "        q_gen = torch.empty(0,3,84,84).cuda()\n",
    "        \n",
    "        if self.qry_gen_num < self.batch_for_gradient:\n",
    "            q_gen = self.generator(qry_img, noise , vars=gen_weight)\n",
    "        else:\n",
    "            for i in range(self.qry_gen_num//self.batch_for_gradient):\n",
    "                noise_tmp = noise[i*self.batch_for_gradient:(i+1)*self.batch_for_gradient]\n",
    "                q_gen = torch.cat([q_gen,self.generator(qry_img[i*self.batch_for_gradient:(i+1)*self.batch_for_gradient], noise_tmp , vars=gen_weight)])\n",
    "\n",
    "        q_fake_discrim_logits = self.pred(q_gen.detach(), weights=discrim_weight)[:,0]\n",
    "        fake_discrim_loss_q = self.criterion(q_fake_discrim_logits, discrim_fake_label)\n",
    "        d_loss_q = (fake_discrim_loss_q + real_discrim_loss_q)\n",
    "        \n",
    "        ### generator train\n",
    "        gen_fake_label = torch.full((self.qry_gen_num,), self.real_value, dtype=torch.float, device=device)\n",
    "        gen_q_discrim = self.pred(q_gen, weights=discrim_weight)[:,0]\n",
    "        g_loss_q = self.criterion(gen_q_discrim, gen_fake_label)\n",
    "        return d_loss_q, g_loss_q\n",
    "\n",
    "    def single_task_forward(self, x_spt, y_spt, x_qry, y_qry, update_steps,nets=None, images=False):\n",
    "        \n",
    "        corrects = {key: np.zeros(update_steps + 1) for key in \n",
    "                        [\n",
    "                        \"D(x)\",\n",
    "                        \"D(G(z))\"\n",
    "                        ]}\n",
    "\n",
    "        y_spt = torch.full((y_spt.size(0),), self.real_value, dtype=torch.float, device=device)\n",
    "        y_qry = torch.full((y_qry.size(0),), self.real_value, dtype=torch.float, device=device)\n",
    "        support_sz, c_, h, w = x_spt.size()\n",
    "        nz = 100\n",
    "\n",
    "        discrim_weights,gen_weights = [x.parameters() for x in nets]\n",
    "\n",
    "        # this is the meta-test loss and accuracy before first update\n",
    "\n",
    "        q_discrim = self.get_num_corrects(y=y_qry, weights=None, x=x_qry)\n",
    "        corrects[\"D(x)\"][0] += q_discrim\n",
    "        # run the i-th task and compute loss for k-th inner update\n",
    "        \n",
    "        for k in range(1, update_steps + 1):\n",
    "            ## discrim loss\n",
    "            noise = torch.randn(self.spy_gen_num, nz , 1, 1, device=device)\n",
    "            x_gen = self.generator(x_spt, noise , vars=gen_weights)\n",
    "            \n",
    "            # update discrim weight\n",
    "\n",
    "            real_discrim_logits = self.pred(x_spt, weights=discrim_weights)[:,0]\n",
    "            # real_discrim_logits = self.pred(x_spt, weights=discrim_weights)\n",
    "\n",
    "            fake_discrim_logits = self.pred(x_gen, weights=discrim_weights)[:,0]\n",
    "            # fake_discrim_logits = self.pred(x_gen, weights=discrim_weights)\n",
    "            \n",
    "            fake_label = torch.full((self.spy_gen_num,), self.fake_value, dtype=torch.float, device=device)\n",
    "            \n",
    "\n",
    "            real_discrim_loss = self.criterion(real_discrim_logits, y_spt)\n",
    "            fake_discrim_loss = self.criterion(fake_discrim_logits,fake_label)\n",
    "            D_loss = fake_discrim_loss + real_discrim_loss\n",
    "            # print(fake_discrim_loss.item(),real_discrim_loss.item())\n",
    "            discrim_weights = self.update_weights(D_loss, discrim_weights,self.update_lr) \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                x_gen = self.generator(x_qry, self.fix_noise , vars=gen_weights) \n",
    "                gen_correct = self.pred(x_gen, weights=discrim_weights)[:,0]\n",
    "                gen_correct = gen_correct.mean().item()\n",
    "                corrects[\"D(G(z))\"][k-1] += gen_correct\n",
    "                \n",
    "                q_discrim_correct = self.get_num_corrects(y=y_qry, x=x_qry, weights=discrim_weights)\n",
    "                corrects[\"D(x)\"][k] += q_discrim_correct\n",
    "#             # meta-test nway and discrim accuracy\n",
    "#             # [query_sz]\n",
    "        \n",
    "#         # final gen-discrim and gen-nway accuracy\n",
    "        with torch.no_grad():\n",
    "            x_gen = self.generator(x_qry, self.fix_noise , vars=gen_weights)\n",
    "            gen_correct = self.pred(x_gen, weights=discrim_weights)[:,0]\n",
    "            # gen_correct = self.pred(x_gen, weights=discrim_weights)\n",
    "            gen_correct = gen_correct.mean().item()\n",
    "            corrects[\"D(G(z))\"][-1] += gen_correct\n",
    "        d_loss_q, g_loss_q = self.meta_test(x_qry,y_qry,discrim_weights,gen_weights)\n",
    "            \n",
    "        if images:\n",
    "            return d_loss_q,g_loss_q, corrects, x_gen\n",
    "        else:\n",
    "            return d_loss_q,g_loss_q, corrects\n",
    "\n",
    "    def forward(self, x_spt, y_spt, x_qry, y_qry,step):\n",
    "        \"\"\"\n",
    "        :param x_spt:   [b, support_sz, c_, h, w]\n",
    "        :param y_spt:   [b, support_sz]\n",
    "        :param x_qry:   [b, query_sz, c_, h, w]\n",
    "        :param y_qry:   [b, query_sz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.current_epoch = step \n",
    "        tasks_per_batch, support_sz, c_, h, w = x_spt.size()\n",
    "        query_sz = x_qry.size(1)\n",
    "        g_loss_q = 0\n",
    "        d_loss_q = 0\n",
    "        gen_losses_q = [0 for _ in range(self.update_steps + 1)]\n",
    "        discrim_losses_q = [0 for _ in range(self.update_steps + 1)]\n",
    "        corrects = {key: np.zeros(self.update_steps + 1) for key in \n",
    "                        [\n",
    "                        \"D(x)\",\n",
    "                        \"D(G(z))\"\n",
    "                        ]}\n",
    "        net = [self.discrim_net,self.generator]\n",
    "        for i in range(tasks_per_batch):\n",
    "            d_loss_q_tmp,g_loss_q_tmp, corrects_tmp = self.single_task_forward(x_spt[i], y_spt[i], x_qry[i], y_qry[i],self.update_steps,nets = net,images=False)\n",
    "            g_loss_q += g_loss_q_tmp\n",
    "            d_loss_q += d_loss_q_tmp\n",
    "            assert len(corrects_tmp.keys()) == len(corrects.keys())\n",
    "            for key in corrects.keys():\n",
    "                corrects[key] += corrects_tmp[key]\n",
    "            \n",
    "        # end of all tasks\n",
    "        # sum over final losses on query set across all tasks\n",
    "        if step > 30:\n",
    "            g_loss_q /= tasks_per_batch\n",
    "            self.meta_gen_optim.zero_grad()\n",
    "            g_loss_q.backward()\n",
    "            self.meta_gen_optim.step()        \n",
    "\n",
    "        # optimize theta parameters\n",
    "        d_loss_q /= tasks_per_batch\n",
    "        self.meta_d_optim.zero_grad()\n",
    "        d_loss_q.backward()\n",
    "        self.meta_d_optim.step()\n",
    "        \n",
    "        accs = {}\n",
    "        accs[\"D(x)\"] = corrects[\"D(x)\"] / (tasks_per_batch)\n",
    "        accs[\"D(G(z))\"] = corrects[\"D(G(z))\"] / (tasks_per_batch)\n",
    "\n",
    "        return accs,d_loss_q,g_loss_q\n",
    "\n",
    "    def finetunning(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x_spt:   [support_sz, c_, h, w]\n",
    "        :param y_spt:   [support_sz]\n",
    "        :param x_qry:   [query_sz, c_, h, w]\n",
    "        :param y_qry:   [query_sz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        support_sz, c_, h, w = x_spt.size()\n",
    "\n",
    "        assert len(x_spt.shape) == 4\n",
    "\n",
    "        query_sz = x_qry.size(0)\n",
    "\n",
    "        # in order to not ruin the state of running_mean/variance and bn_weight/bias\n",
    "        # we finetunning on the copied model instead of self.net\n",
    "        \n",
    "        discrim_net = deepcopy(self.discrim_net)\n",
    "        generator = deepcopy(self.generator)\n",
    "        net = [self.discrim_net,self.generator]\n",
    "        d_loss_q,g_loss_q, corrects, imgs = self.single_task_forward(x_spt, y_spt, x_qry, y_qry,self.update_steps_test, nets=net,images=True)\n",
    "\n",
    "        del discrim_net\n",
    "        \n",
    "        accs[\"D(x)\"] = corrects[\"D(x)\"]\n",
    "        accs[\"D(G(z))\"] = corrects[\"D(G(z))\"]\n",
    "\n",
    "        return accs, imgs,d_loss_q,g_loss_q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e14bddb2-264f-4a65-965b-bb567e7197c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3af9f5aec84cfd9f51edb79105b151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "d loss: 1.4976173639297485\n",
      "g loss: 10.921062469482422\n",
      "accs {'D(x)': array([0.20729602, 0.47721277]), 'D(G(z))': array([0.22991294, 0.22991011])}\n",
      "d loss: 1.349763709306717\n",
      "g loss: 2.174746948480606\n",
      "Test acc: {'D(x)': array([0.30513278, 0.59015721]), 'D(G(z))': array([0.19835092, 0.19836073])}\n",
      "step 100\n",
      "d loss: 1.3975893259048462\n",
      "g loss: 0.7426688075065613\n",
      "accs {'D(x)': array([0.83565809, 0.79118344]), 'D(G(z))': array([0.5523163 , 0.55231531])}\n",
      "step 200\n",
      "d loss: 0.17730998992919922\n",
      "g loss: 4.299203395843506\n",
      "accs {'D(x)': array([0.95750504, 0.90808061]), 'D(G(z))': array([0.07030124, 0.07030432])}\n",
      "step 300\n",
      "d loss: 0.35638609528541565\n",
      "g loss: 2.512193441390991\n",
      "accs {'D(x)': array([0.95898908, 0.86937265]), 'D(G(z))': array([0.10200596, 0.10200369])}\n",
      "step 400\n",
      "d loss: 0.3572602868080139\n",
      "g loss: 2.267479658126831\n",
      "accs {'D(x)': array([0.95211376, 0.90674336]), 'D(G(z))': array([0.18995091, 0.18994861])}\n",
      "step 500\n",
      "d loss: 0.027400070801377296\n",
      "g loss: 4.040902614593506\n",
      "accs {'D(x)': array([0.99645239, 0.99648663]), 'D(G(z))': array([0.02677252, 0.02677389])}\n",
      "d loss: 0.019670700782444327\n",
      "g loss: 6.220403099060059\n",
      "Test acc: {'D(x)': array([0.99306029, 0.99418366]), 'D(G(z))': array([0.00155225, 0.00155256])}\n",
      "step 600\n",
      "d loss: 0.029198763892054558\n",
      "g loss: 5.202413558959961\n",
      "accs {'D(x)': array([0.9993466 , 0.98896252]), 'D(G(z))': array([0.02287457, 0.02287605])}\n",
      "step 700\n",
      "d loss: 0.08683285117149353\n",
      "g loss: 4.260411739349365\n",
      "accs {'D(x)': array([7.46633449e-06, 9.48601413e-01]), 'D(G(z))': array([0.01956207, 0.0195619 ])}\n",
      "step 800\n",
      "d loss: 0.0639820471405983\n",
      "g loss: 3.6491761207580566\n",
      "accs {'D(x)': array([1.91919563e-05, 9.84546649e-01]), 'D(G(z))': array([0.0510659 , 0.05106476])}\n",
      "step 900\n",
      "d loss: 0.7790738344192505\n",
      "g loss: 2.881760358810425\n",
      "accs {'D(x)': array([5.83487240e-05, 7.14911944e-01]), 'D(G(z))': array([0.16570352, 0.16570799])}\n",
      "step 1000\n",
      "d loss: 0.6449398398399353\n",
      "g loss: 2.4017107486724854\n",
      "accs {'D(x)': array([2.59851427e-06, 7.92549968e-01]), 'D(G(z))': array([0.26522722, 0.26521669])}\n",
      "d loss: 1.5303771883249282\n",
      "g loss: 2.303779861330986\n",
      "Test acc: {'D(x)': array([3.61978465e-08, 6.41165912e-01]), 'D(G(z))': array([0.21572195, 0.21572019])}\n",
      "step 1100\n",
      "d loss: 1.1877378225326538\n",
      "g loss: 0.6535100340843201\n",
      "accs {'D(x)': array([7.91716698e-06, 8.55897260e-01]), 'D(G(z))': array([0.57618006, 0.57617976])}\n",
      "step 1200\n",
      "d loss: 1.154224157333374\n",
      "g loss: 1.039918303489685\n",
      "accs {'D(x)': array([3.82859284e-04, 7.48374975e-01]), 'D(G(z))': array([0.41026136, 0.41026427])}\n",
      "step 1300\n",
      "d loss: 1.1810104846954346\n",
      "g loss: 1.0401654243469238\n",
      "accs {'D(x)': array([8.42858366e-05, 8.65159059e-01]), 'D(G(z))': array([0.53186617, 0.53186322])}\n",
      "step 1400\n",
      "d loss: 0.5907346606254578\n",
      "g loss: 1.7766879796981812\n",
      "accs {'D(x)': array([5.66845985e-06, 8.01961386e-01]), 'D(G(z))': array([0.21983541, 0.21984054])}\n",
      "step 1500\n",
      "d loss: 0.5479076504707336\n",
      "g loss: 3.9256973266601562\n",
      "accs {'D(x)': array([0.27106399, 0.81520358]), 'D(G(z))': array([0.15668201, 0.15668676])}\n",
      "d loss: 0.10518678771331906\n",
      "g loss: 6.404325914382935\n",
      "Test acc: {'D(x)': array([0.69638658, 0.99178982]), 'D(G(z))': array([0.1085256 , 0.10852737])}\n",
      "step 1600\n",
      "d loss: 0.9531031847000122\n",
      "g loss: 1.7921030521392822\n",
      "accs {'D(x)': array([0.98140998, 0.59953858]), 'D(G(z))': array([0.25537118, 0.2553707 ])}\n",
      "step 1700\n",
      "d loss: 0.7264959812164307\n",
      "g loss: 2.782033681869507\n",
      "accs {'D(x)': array([0.96133366, 0.74254587]), 'D(G(z))': array([0.06346758, 0.0634678 ])}\n",
      "step 1800\n",
      "d loss: 0.9705750346183777\n",
      "g loss: 1.1989713907241821\n",
      "accs {'D(x)': array([0.9753386 , 0.66966231]), 'D(G(z))': array([0.29853732, 0.29853848])}\n",
      "step 1900\n",
      "d loss: 1.1000629663467407\n",
      "g loss: 1.044871211051941\n",
      "accs {'D(x)': array([0.97940135, 0.67672299]), 'D(G(z))': array([0.36297318, 0.36297193])}\n",
      "step 2000\n",
      "d loss: 0.8672580718994141\n",
      "g loss: 1.2701267004013062\n",
      "accs {'D(x)': array([0.95288978, 0.74896224]), 'D(G(z))': array([0.30722834, 0.307229  ])}\n",
      "d loss: 0.6257744416594505\n",
      "g loss: 1.3426503896713258\n",
      "Test acc: {'D(x)': array([0.98896515, 0.82151592]), 'D(G(z))': array([0.24576978, 0.24578337])}\n",
      "step 2100\n",
      "d loss: 0.3372100591659546\n",
      "g loss: 2.285935163497925\n",
      "accs {'D(x)': array([0.99584997, 0.87206459]), 'D(G(z))': array([0.11431195, 0.11431587])}\n",
      "step 2200\n",
      "d loss: 0.34988823533058167\n",
      "g loss: 1.558451533317566\n",
      "accs {'D(x)': array([0.99721489, 0.94541029]), 'D(G(z))': array([0.22879077, 0.22878441])}\n",
      "step 2300\n",
      "d loss: 0.678202211856842\n",
      "g loss: 1.704949975013733\n",
      "accs {'D(x)': array([0.97816378, 0.83788733]), 'D(G(z))': array([0.32819376, 0.32819699])}\n",
      "step 2400\n",
      "d loss: 0.5821570754051208\n",
      "g loss: 1.269291639328003\n",
      "accs {'D(x)': array([0.99686562, 0.85612948]), 'D(G(z))': array([0.29873834, 0.29872877])}\n",
      "step 2500\n",
      "d loss: 0.6228157877922058\n",
      "g loss: 1.7876033782958984\n",
      "accs {'D(x)': array([0.99612733, 0.73015797]), 'D(G(z))': array([0.20244356, 0.20244211])}\n",
      "d loss: 1.135284934937954\n",
      "g loss: 1.047493003308773\n",
      "Test acc: {'D(x)': array([0.97999263, 0.48123693]), 'D(G(z))': array([0.32936084, 0.32935277])}\n",
      "step 2600\n",
      "d loss: 0.4151860773563385\n",
      "g loss: 1.7130635976791382\n",
      "accs {'D(x)': array([0.9927155 , 0.85940416]), 'D(G(z))': array([0.2727235 , 0.27271583])}\n",
      "step 2700\n",
      "d loss: 0.7727149128913879\n",
      "g loss: 0.8775619864463806\n",
      "accs {'D(x)': array([0.99884558, 0.92271743]), 'D(G(z))': array([0.44161879, 0.44162151])}\n",
      "step 2800\n",
      "d loss: 0.5080451369285583\n",
      "g loss: 2.4509832859039307\n",
      "accs {'D(x)': array([0.99024986, 0.75401983]), 'D(G(z))': array([0.12631476, 0.12632044])}\n",
      "step 2900\n",
      "d loss: 0.43818551301956177\n",
      "g loss: 1.5120753049850464\n",
      "accs {'D(x)': array([0.99904981, 0.88858651]), 'D(G(z))': array([0.19984626, 0.19985208])}\n",
      "step 3000\n",
      "d loss: 1.0486849546432495\n",
      "g loss: 0.7091359496116638\n",
      "accs {'D(x)': array([0.99908881, 0.935149  ]), 'D(G(z))': array([0.55786785, 0.55787451])}\n",
      "d loss: 1.8953433521091938\n",
      "g loss: 2.8293689012527468\n",
      "Test acc: {'D(x)': array([0.9340685 , 0.49703872]), 'D(G(z))': array([0.17143689, 0.17144223])}\n",
      "step 3100\n",
      "d loss: 1.0441043376922607\n",
      "g loss: 1.8445862531661987\n",
      "accs {'D(x)': array([0.99080758, 0.65848739]), 'D(G(z))': array([0.22432031, 0.22432374])}\n",
      "step 3200\n",
      "d loss: 0.2764382064342499\n",
      "g loss: 2.8560664653778076\n",
      "accs {'D(x)': array([0.99803102, 0.85274836]), 'D(G(z))': array([0.06533771, 0.06533688])}\n",
      "step 3300\n",
      "d loss: 0.44943687319755554\n",
      "g loss: 1.9551416635513306\n",
      "accs {'D(x)': array([0.99752463, 0.81168518]), 'D(G(z))': array([0.17732947, 0.17732929])}\n",
      "step 3400\n",
      "d loss: 0.9218358397483826\n",
      "g loss: 1.9906810522079468\n",
      "accs {'D(x)': array([0.99686354, 0.60852685]), 'D(G(z))': array([0.15168397, 0.15168719])}\n",
      "step 3500\n",
      "d loss: 0.5007565021514893\n",
      "g loss: 1.8878090381622314\n",
      "accs {'D(x)': array([0.99715868, 0.85204409]), 'D(G(z))': array([0.19891514, 0.19891679])}\n",
      "d loss: 1.390322069078684\n",
      "g loss: 2.2419658482074736\n",
      "Test acc: {'D(x)': array([0.99230862, 0.82639593]), 'D(G(z))': array([0.16333273, 0.16333097])}\n",
      "step 3600\n",
      "d loss: 0.6592750549316406\n",
      "g loss: 1.8493973016738892\n",
      "accs {'D(x)': array([0.99826655, 0.70763685]), 'D(G(z))': array([0.20291835, 0.2029177 ])}\n",
      "step 3700\n",
      "d loss: 0.7737099528312683\n",
      "g loss: 0.9581393599510193\n",
      "accs {'D(x)': array([0.99912945, 0.90631427]), 'D(G(z))': array([0.52160735, 0.52159564])}\n",
      "step 3800\n",
      "d loss: 1.2080402374267578\n",
      "g loss: 3.8754022121429443\n",
      "accs {'D(x)': array([0.98429139, 0.51333606]), 'D(G(z))': array([0.04439131, 0.04439009])}\n",
      "step 3900\n",
      "d loss: 0.7905018925666809\n",
      "g loss: 2.7711141109466553\n",
      "accs {'D(x)': array([0.99576602, 0.56027822]), 'D(G(z))': array([0.1042575 , 0.10425725])}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtasks_per_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m6000\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# fetch meta_batchsz num of episode each time\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_data_generator, args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtasks_per_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, (x_spt, y_spt, x_qry, y_qry) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     15\u001b[0m         x_spt, y_spt, x_qry, y_qry \u001b[38;5;241m=\u001b[39m x_spt\u001b[38;5;241m.\u001b[39mto(device), y_spt\u001b[38;5;241m.\u001b[39mto(device), x_qry\u001b[38;5;241m.\u001b[39mto(device), y_qry\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m         accs,d_loss,g_loss \u001b[38;5;241m=\u001b[39m mamlGAN(x_spt, y_spt, x_qry, y_qry,step)\n",
      "File \u001b[0;32m~/miniconda3/envs/metagan/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/metagan/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1207\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1207\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/metagan/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1163\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1165\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/metagan/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1011\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1014\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/metagan/lib/python3.8/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/miniconda3/envs/metagan/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mamlGAN = Meta(args, discriminator_config, gen_config).to(device)\n",
    "step = 0\n",
    "path = args[\"save_path\"]\n",
    "mkdir_p(path)\n",
    "best_acc = []\n",
    "\n",
    "with tqdm.tqdm(initial=step,\n",
    "                   total=int(args[\"epoch\"])) as pbar_train:\n",
    "    for _ in range(args[\"epoch\"] * args[\"tasks_per_batch\"]//6000):\n",
    "        # fetch meta_batchsz num of episode each time\n",
    "        train_dataloader = DataLoader(train_data_generator, args[\"tasks_per_batch\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "        for _, (x_spt, y_spt, x_qry, y_qry) in enumerate(train_dataloader):\n",
    "            x_spt, y_spt, x_qry, y_qry = x_spt.to(device), y_spt.to(device), x_qry.to(device), y_qry.to(device)\n",
    "\n",
    "            accs,d_loss,g_loss = mamlGAN(x_spt, y_spt, x_qry, y_qry,step)\n",
    "            # accs,d_loss = mamlGAN(x_spt, y_spt, x_qry, y_qry,step)\n",
    "            writer.add_scalar('Loss/train_d_loss', d_loss, step)\n",
    "            writer.add_scalar('Loss/train_g_loss', g_loss, step)\n",
    "            writer.add_scalar('Accuracy/\"train_D(x)', accs[\"D(x)\"][-1], step)\n",
    "            writer.add_scalar('Accuracy/\"train_D(G(z))', accs[\"D(G(z))\"][-1], step)\n",
    "            if step % 100 == 0:\n",
    "                print(\"step \" + str(step))\n",
    "                print('d loss:',d_loss.item())\n",
    "                print('g loss:',g_loss.item())\n",
    "                print(\"accs\",accs)\n",
    "\n",
    "\n",
    "            if step % 500 == 0:  # evaluation\n",
    "                db_test = DataLoader(test_data_generator, 1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "                accs_all_test = []\n",
    "                imgs_all_test = []\n",
    "                d_loss_all_test = []\n",
    "                g_loss_all_test = []\n",
    "                for x_spt, y_spt, x_qry, y_qry in db_test:\n",
    "                    x_spt, y_spt, x_qry, y_qry = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device), \\\n",
    "                                                 x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device)\n",
    "\n",
    "                    # accs, d_loss = mamlGAN.finetunning(x_spt, y_spt, x_qry, y_qry)\n",
    "                    accs, imgs,d_loss,g_loss = mamlGAN.finetunning(x_spt, y_spt, x_qry, y_qry)\n",
    "\n",
    "\n",
    "                    accs_all_test.append(accs)\n",
    "                    imgs_all_test.append(imgs.cpu().detach().numpy())\n",
    "                    d_loss_all_test.append(d_loss.item())\n",
    "                    g_loss_all_test.append(g_loss.item())\n",
    "\n",
    "                imgs_all_test = np.array(imgs_all_test)\n",
    "                # [b, update_step+1]\n",
    "                # accs = np.array(accs_all_test).mean(axis=0).astype(np.float16)\n",
    "                d_loss = np.mean(np.array(d_loss_all_test))\n",
    "                g_loss = np.mean(np.array(g_loss_all_test))\n",
    "\n",
    "                print('d loss:',d_loss)\n",
    "                print('g loss:',g_loss)\n",
    "                print('Test acc:', accs)    \n",
    "\n",
    "                writer.add_scalar('Loss/test_d_loss', d_loss, step)\n",
    "                writer.add_scalar('Loss/test_g_loss', g_loss, step)\n",
    "                writer.add_scalar('Accuracy/\"test_D(x)', accs[\"D(x)\"][-1], step)\n",
    "                writer.add_scalar('Accuracy/\"test_D(G(z))', accs[\"D(G(z))\"][-1], step)\n",
    "\n",
    "                if not len(best_acc):\n",
    "                    best_acc = accs\n",
    "                    best_epoch = step\n",
    "                    torch.save({'model_state_dict': mamlGAN.state_dict()}, \"save_models/\" + path + \"/best.pth\")\n",
    "                else:\n",
    "                    if max(accs) > max(best_acc):\n",
    "                        best_acc = accs\n",
    "                        best_epoch = step\n",
    "                        torch.save({'model_state_dict': mamlGAN.state_dict()}, \"save_models/\" + path + \"/best.pth\")\n",
    "                torch.save({'model_state_dict': mamlGAN.state_dict()}, \"save_models/\" + path + \"/model_step\" + str(step) + \".pth\")\n",
    "\n",
    "                save_imgs(path, imgs_all_test, step)\n",
    "\n",
    "            step = step + 1\n",
    "            pbar_train.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metagan",
   "language": "python",
   "name": "metagan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
