{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c437769e-2c40-4f99-a9e2-b087b7ce600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyc/miniconda3/envs/metagan/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import  torch\n",
    "from    torch import nn\n",
    "from    torch import optim\n",
    "from    torch.nn import functional as F\n",
    "from    torch.utils.data import TensorDataset, DataLoader\n",
    "from    torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torchsummary import summary\n",
    "\n",
    "import  numpy as np\n",
    "import json\n",
    "from    copy import deepcopy\n",
    "from utils.dataloader import train_data_gen,test_data_gen\n",
    "import argparse\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "082f0ead-b5e5-496d-8e1d-3f6e1e121a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'epoch':24000,\n",
    "        'n_way':5,\n",
    "        'k_spt':5,\n",
    "        'k_qry':10,\n",
    "        'img_sz':84,\n",
    "        \"tasks_per_batch\":5,\n",
    "        'img_c':3,\n",
    "        'task_num': 5,\n",
    "        'meta_lr':1e-3,\n",
    "        'update_lr':1e-3,\n",
    "        'update_step':5,\n",
    "        'update_step_test':5,\n",
    "        \"no_save\":False,\n",
    "        \"learn_inner_lr\":True,\n",
    "        'condition_discrim':False,\n",
    "        \"loss\":\"cross_entropy\",\n",
    "        \"create_graph\":False,\n",
    "        \"num_distractor\":0,\n",
    "        'save_path':'0409_conditional_result',\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df6f4e4f-4bc8-4a78-9bf2-e11fa0f3ef65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset/BelgiumTSC\n",
      "load complete time 4.051136016845703\n",
      "load dataset/ArTS\n",
      "load complete time 4.11678409576416\n",
      "load dataset/chinese_traffic_sign\n",
      "load complete time 0.6857068538665771\n",
      "load dataset/CVL\n",
      "load complete time 0.5210871696472168\n",
      "load dataset/FullJCNN2013\n",
      "load complete time 0.2944614887237549\n",
      "load dataset/logo_2k\n",
      "load complete time 1.2227318286895752\n",
      "load dataset/GTSRB\n",
      "load complete time 0.15367412567138672\n",
      "load dataset/DFG\n",
      "load complete time 0.060781002044677734\n"
     ]
    }
   ],
   "source": [
    "train_data_generator = train_data_gen(args)\n",
    "test_data_generator = test_data_gen(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5398c6-8c76-4978-9673-7154f2d8a152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Learner(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, imgc, imgsz):\n",
    "        \"\"\"\n",
    "\n",
    "        :param config: network config file, type:list of (string, list)\n",
    "        :param imgc: 1 or 3\n",
    "        :param imgsz:  28 or 84\n",
    "        \"\"\"\n",
    "        super(Learner, self).__init__()\n",
    "\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # this dict contains all tensors needed to be optimized\n",
    "        self.vars = nn.ParameterList()\n",
    "        # running_mean and running_var\n",
    "        self.vars_bn = nn.ParameterList()\n",
    "\n",
    "        for i, (name, param) in enumerate(self.config):\n",
    "            if name == 'conv2d':\n",
    "                # [ch_out, ch_in, kernelsz, kernelsz]\n",
    "                w = nn.Parameter(torch.ones(*param[:4]))\n",
    "                # gain=1 according to cbfin's implementation\n",
    "                torch.nn.init.kaiming_normal_(w)\n",
    "                self.vars.append(w)\n",
    "                # [ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "\n",
    "            elif name == 'convt2d':\n",
    "                # [ch_in, ch_out, kernelsz, kernelsz, stride, padding]\n",
    "                w = nn.Parameter(torch.ones(*param[:4]))\n",
    "                # gain=1 according to cbfin's implementation\n",
    "                torch.nn.init.kaiming_normal_(w)\n",
    "                self.vars.append(w)\n",
    "                # [ch_in, ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[1])))\n",
    "\n",
    "            elif name == 'linear':\n",
    "                # [ch_out, ch_in]\n",
    "                w = nn.Parameter(torch.ones(*param))\n",
    "                # gain=1 according to cbfinn's implementation\n",
    "                torch.nn.init.kaiming_normal_(w)\n",
    "                self.vars.append(w)\n",
    "                # [ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "\n",
    "            elif name == 'bn':\n",
    "                # [ch_out]\n",
    "                w = nn.Parameter(torch.ones(param[0]))\n",
    "                self.vars.append(w)\n",
    "                # [ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "\n",
    "                # must set requires_grad=False\n",
    "                running_mean = nn.Parameter(torch.zeros(param[0]), requires_grad=False)\n",
    "                running_var = nn.Parameter(torch.ones(param[0]), requires_grad=False)\n",
    "                self.vars_bn.extend([running_mean, running_var])\n",
    "\n",
    "\n",
    "            elif name in ['tanh', 'relu', 'upsample', 'avg_pool2d', 'max_pool2d',\n",
    "                          'flatten', 'reshape', 'leakyrelu', 'sigmoid']:\n",
    "                continue\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def extra_repr(self):\n",
    "        info = ''\n",
    "\n",
    "        for name, param in self.config:\n",
    "            if name == 'conv2d':\n",
    "                tmp = 'conv2d:(ch_in:%d, ch_out:%d, k:%dx%d, stride:%d, padding:%d)'\\\n",
    "                      %(param[1], param[0], param[2], param[3], param[4], param[5],)\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "            elif name == 'convt2d':\n",
    "                tmp = 'convTranspose2d:(ch_in:%d, ch_out:%d, k:%dx%d, stride:%d, padding:%d)'\\\n",
    "                      %(param[0], param[1], param[2], param[3], param[4], param[5],)\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "            elif name == 'linear':\n",
    "                tmp = 'linear:(in:%d, out:%d)'%(param[1], param[0])\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "            elif name == 'leakyrelu':\n",
    "                tmp = 'leakyrelu:(slope:%f)'%(param[0])\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "\n",
    "            elif name == 'avg_pool2d':\n",
    "                tmp = 'avg_pool2d:(k:%d, stride:%d, padding:%d)'%(param[0], param[1], param[2])\n",
    "                info += tmp + '\\n'\n",
    "            elif name == 'max_pool2d':\n",
    "                tmp = 'max_pool2d:(k:%d, stride:%d, padding:%d)'%(param[0], param[1], param[2])\n",
    "                info += tmp + '\\n'\n",
    "            elif name in ['flatten', 'tanh', 'relu', 'upsample', 'reshape', 'sigmoid', 'use_logits', 'bn']:\n",
    "                tmp = name + ':' + str(tuple(param))\n",
    "                info += tmp + '\\n'\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        return info\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, vars=None, bn_training=True):\n",
    "        \"\"\"\n",
    "        This function can be called by finetunning, however, in finetunning, we dont wish to update\n",
    "        running_mean/running_var. Thought weights/bias of bn is updated, it has been separated by fast_weights.\n",
    "        Indeed, to not update running_mean/running_var, we need set update_bn_statistics=False\n",
    "        but weight/bias will be updated and not dirty initial theta parameters via fast_weiths.\n",
    "        :param x: [b, 1, 28, 28]\n",
    "        :param vars:\n",
    "        :param bn_training: set False to not update\n",
    "        :return: x, loss, likelihood, kld\n",
    "        \"\"\"\n",
    "\n",
    "        if vars is None:\n",
    "            vars = self.vars\n",
    "\n",
    "        idx = 0\n",
    "        bn_idx = 0\n",
    "\n",
    "        for name, param in self.config:\n",
    "            if name == 'conv2d':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                # remember to keep synchrozied of forward_encoder and forward_decoder!\n",
    "                x = F.conv2d(x, w, b, stride=param[4], padding=param[5])\n",
    "                idx += 2\n",
    "                # print(name, param, '\\tout:', x.shape)\n",
    "            elif name == 'convt2d':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                # remember to keep synchrozied of forward_encoder and forward_decoder!\n",
    "                x = F.conv_transpose2d(x, w, b, stride=param[4], padding=param[5])\n",
    "                idx += 2\n",
    "                # print(name, param, '\\tout:', x.shape)\n",
    "            elif name == 'linear':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                x = F.linear(x, w, b)\n",
    "                idx += 2\n",
    "                # print('forward:', idx, x.norm().item())\n",
    "            elif name == 'bn':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                running_mean, running_var = self.vars_bn[bn_idx], self.vars_bn[bn_idx+1]\n",
    "                x = F.batch_norm(x, running_mean, running_var, weight=w, bias=b, training=bn_training)\n",
    "                idx += 2\n",
    "                bn_idx += 2\n",
    "\n",
    "            elif name == 'flatten':\n",
    "                # print(x.shape)\n",
    "                x = x.view(x.size(0), -1)\n",
    "            elif name == 'reshape':\n",
    "                # [b, 8] => [b, 2, 2, 2]\n",
    "                x = x.view(x.size(0), *param)\n",
    "            elif name == 'relu':\n",
    "                x = F.relu(x, inplace=param[0])\n",
    "            elif name == 'leakyrelu':\n",
    "                x = F.leaky_relu(x, negative_slope=param[0], inplace=param[1])\n",
    "            elif name == 'tanh':\n",
    "                x = F.tanh(x)\n",
    "            elif name == 'sigmoid':\n",
    "                x = torch.sigmoid(x)\n",
    "            elif name == 'upsample':\n",
    "                x = F.upsample_nearest(x, scale_factor=param[0])\n",
    "            elif name == 'max_pool2d':\n",
    "                x = F.max_pool2d(x, param[0], param[1], param[2])\n",
    "            elif name == 'avg_pool2d':\n",
    "                x = F.avg_pool2d(x, param[0], param[1], param[2])\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        # make sure variable is used properly\n",
    "        assert idx == len(vars)\n",
    "        assert bn_idx == len(self.vars_bn)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def zero_grad(self, vars=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param vars:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if vars == None:\n",
    "                for p in self.vars:\n",
    "                    if not p.grad == None:\n",
    "                        p.grad.zero_()\n",
    "            else:\n",
    "                for p in vars:\n",
    "                    if not p.grad ==  None:\n",
    "                        p.grad.zero_()\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        override this function since initial parameters will return with a generator.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dfb02bc-dec5-4af0-82bb-7fccfb40a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = [\n",
    "    ('conv2d', [32, 3, 3, 3, 1, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [32]),\n",
    "    ('max_pool2d', [2, 2, 0]),\n",
    "    ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [32]),\n",
    "    ('max_pool2d', [2, 2, 0]),\n",
    "    ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [32]),\n",
    "    ('max_pool2d', [2, 2, 0]),\n",
    "    ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [32]),\n",
    "    ('max_pool2d', [2, 1, 0]),\n",
    "    ('flatten', []),\n",
    "    ('linear', [5, 32 * 5 * 5])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "648b1c0c-a08b-47eb-bc3d-2d307bcfab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meta(nn.Module):\n",
    "    \"\"\"\n",
    "    Meta Learner\n",
    "    \"\"\"\n",
    "    def __init__(self, args, config):\n",
    "        \"\"\"\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(Meta, self).__init__()\n",
    "\n",
    "        self.update_lr = args[\"update_lr\"]\n",
    "        self.meta_lr = args[\"meta_lr\"]\n",
    "        self.n_way = args[\"n_way\"]\n",
    "        self.k_spt = args[\"k_spt\"]\n",
    "        self.k_qry = args[\"k_qry\"]\n",
    "        self.task_num = args[\"task_num\"]\n",
    "        self.update_step = args[\"update_step\"]\n",
    "        self.update_step_test = args[\"update_step_test\"]\n",
    "\n",
    "\n",
    "        self.net = Learner(config, args[\"img_c\"], args[\"img_sz\"])\n",
    "        self.meta_optim = optim.Adam(self.net.parameters(), lr=self.meta_lr)\n",
    "\n",
    "    def forward(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        \"\"\"\n",
    "        :param x_spt:   [b, setsz, c_, h, w]\n",
    "        :param y_spt:   [b, setsz]\n",
    "        :param x_qry:   [b, querysz, c_, h, w]\n",
    "        :param y_qry:   [b, querysz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        task_num, setsz, c_, h, w = x_spt.size()\n",
    "        querysz = x_qry.size(1)\n",
    "\n",
    "        losses_q = [0 for _ in range(self.update_step + 1)]  # losses_q[i] is the loss on step i\n",
    "        corrects = [0 for _ in range(self.update_step + 1)]\n",
    "\n",
    "\n",
    "        for i in range(task_num):\n",
    "\n",
    "            # 1. run the i-th task and compute loss for k=0\n",
    "            logits = self.net(x_spt[i], vars=None, bn_training=True)\n",
    "            loss = F.cross_entropy(logits, y_spt[i])\n",
    "            grad = torch.autograd.grad(loss, self.net.parameters())\n",
    "            fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, self.net.parameters())))\n",
    "\n",
    "            # this is the loss and accuracy before first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                logits_q = self.net(x_qry[i], self.net.parameters(), bn_training=True)\n",
    "                loss_q = F.cross_entropy(logits_q, y_qry[i])\n",
    "                losses_q[0] += loss_q\n",
    "\n",
    "                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_q, y_qry[i]).sum().item()\n",
    "                corrects[0] = corrects[0] + correct\n",
    "\n",
    "            # this is the loss and accuracy after the first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                logits_q = self.net(x_qry[i], fast_weights, bn_training=True)\n",
    "                loss_q = F.cross_entropy(logits_q, y_qry[i])\n",
    "                losses_q[1] += loss_q\n",
    "                # [setsz]\n",
    "                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_q, y_qry[i]).sum().item()\n",
    "                corrects[1] = corrects[1] + correct\n",
    "\n",
    "            for k in range(1, self.update_step):\n",
    "                # 1. run the i-th task and compute loss for k=1~K-1\n",
    "                logits = self.net(x_spt[i], fast_weights, bn_training=True)\n",
    "                loss = F.cross_entropy(logits, y_spt[i])\n",
    "                # 2. compute grad on theta_pi\n",
    "                grad = torch.autograd.grad(loss, fast_weights)\n",
    "                # 3. theta_pi = theta_pi - train_lr * grad\n",
    "                fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "                logits_q = self.net(x_qry[i], fast_weights, bn_training=True)\n",
    "                # loss_q will be overwritten and just keep the loss_q on last update step.\n",
    "                loss_q = F.cross_entropy(logits_q, y_qry[i])\n",
    "                losses_q[k + 1] += loss_q\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                    correct = torch.eq(pred_q, y_qry[i]).sum().item()  # convert to numpy\n",
    "                    corrects[k + 1] = corrects[k + 1] + correct\n",
    "\n",
    "\n",
    "\n",
    "        # end of all tasks\n",
    "        # sum over all losses on query set across all tasks\n",
    "        loss_q = losses_q[-1] / task_num\n",
    "\n",
    "        # optimize theta parameters\n",
    "        self.meta_optim.zero_grad()\n",
    "        loss_q.backward()\n",
    "\n",
    "        self.meta_optim.step()\n",
    "\n",
    "        \n",
    "        accs = np.array(corrects) / (querysz * task_num)\n",
    "\n",
    "        return accs,losses_q\n",
    "\n",
    "\n",
    "    def finetunning(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        \"\"\"\n",
    "        :param x_spt:   [setsz, c_, h, w]\n",
    "        :param y_spt:   [setsz]\n",
    "        :param x_qry:   [querysz, c_, h, w]\n",
    "        :param y_qry:   [querysz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert len(x_spt.shape) == 4\n",
    "\n",
    "        querysz = x_qry.size(0)\n",
    "\n",
    "        corrects = [0 for _ in range(self.update_step_test + 1)]\n",
    "\n",
    "        # in order to not ruin the state of running_mean/variance and bn_weight/bias\n",
    "        # we finetunning on the copied model instead of self.net\n",
    "        net = deepcopy(self.net)\n",
    "\n",
    "        # 1. run the i-th task and compute loss for k=0\n",
    "        logits = net(x_spt)\n",
    "        loss = F.cross_entropy(logits, y_spt)\n",
    "        grad = torch.autograd.grad(loss, net.parameters())\n",
    "        fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, net.parameters())))\n",
    "\n",
    "        # this is the loss and accuracy before first update\n",
    "        with torch.no_grad():\n",
    "            # [setsz, nway]\n",
    "            logits_q = net(x_qry, net.parameters(), bn_training=True)\n",
    "            # [setsz]\n",
    "            pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "            # scalar\n",
    "            correct = torch.eq(pred_q, y_qry).sum().item()\n",
    "            corrects[0] = corrects[0] + correct\n",
    "\n",
    "        # this is the loss and accuracy after the first update\n",
    "        with torch.no_grad():\n",
    "            # [setsz, nway]\n",
    "            logits_q = net(x_qry, fast_weights, bn_training=True)\n",
    "            # [setsz]\n",
    "            pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "            # scalar\n",
    "            correct = torch.eq(pred_q, y_qry).sum().item()\n",
    "            corrects[1] = corrects[1] + correct\n",
    "\n",
    "        for k in range(1, self.update_step_test):\n",
    "            # 1. run the i-th task and compute loss for k=1~K-1\n",
    "            logits = net(x_spt, fast_weights, bn_training=True)\n",
    "            loss = F.cross_entropy(logits, y_spt)\n",
    "            # 2. compute grad on theta_pi\n",
    "            grad = torch.autograd.grad(loss, fast_weights)\n",
    "            # 3. theta_pi = theta_pi - train_lr * grad\n",
    "            fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "            logits_q = net(x_qry, fast_weights, bn_training=True)\n",
    "            # loss_q will be overwritten and just keep the loss_q on last update step.\n",
    "            loss_q = F.cross_entropy(logits_q, y_qry)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_q, y_qry).sum().item()  # convert to numpy\n",
    "                corrects[k + 1] = corrects[k + 1] + correct\n",
    "\n",
    "\n",
    "        del net\n",
    "\n",
    "        accs = np.array(corrects) / querysz\n",
    "\n",
    "        return accs,loss_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4acc925-df3a-4a0f-9143-094d7fe3c0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:15,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss [8.448969841003418, 7.932387828826904, 7.520358562469482, 7.1920671463012695, 6.930163860321045, 6.717977046966553],acc [0.18  0.264 0.364 0.404 0.436 0.472]\n",
      "\n",
      "Updated Model Parameter Theta\n",
      "\n",
      "Sampling Next Batch of Tasks \n",
      "\n",
      "---------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [04:13,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200: Loss [8.549342155456543, 7.836533546447754, 7.297307968139648, 6.888352394104004, 6.572206497192383, 6.322166442871094],acc [0.152 0.284 0.38  0.424 0.464 0.472]\n",
      "\n",
      "Updated Model Parameter Theta\n",
      "\n",
      "Sampling Next Batch of Tasks \n",
      "\n",
      "---------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [06:37,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300: Loss [8.512151718139648, 7.916592597961426, 7.464907169342041, 7.125998497009277, 6.871276378631592, 6.681093692779541],acc [0.232 0.26  0.32  0.352 0.384 0.4  ]\n",
      "\n",
      "Updated Model Parameter Theta\n",
      "\n",
      "Sampling Next Batch of Tasks \n",
      "\n",
      "---------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [08:49,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400: Loss [8.505376815795898, 7.821447372436523, 7.3149237632751465, 6.942846775054932, 6.664690971374512, 6.449995517730713],acc [0.176 0.264 0.352 0.396 0.428 0.464]\n",
      "\n",
      "Updated Model Parameter Theta\n",
      "\n",
      "Sampling Next Batch of Tasks \n",
      "\n",
      "---------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "499it [10:49,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: Loss [8.589656829833984, 7.879627704620361, 7.3749871253967285, 7.0132222175598145, 6.749013900756836, 6.548984527587891],acc [0.148 0.232 0.296 0.364 0.38  0.38 ]\n",
      "\n",
      "Updated Model Parameter Theta\n",
      "\n",
      "Sampling Next Batch of Tasks \n",
      "\n",
      "---------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     21\u001b[0m     x_spt, y_spt, x_qry, y_qry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(test_dataloader)\n\u001b[0;32m---> 22\u001b[0m     accs,loss \u001b[38;5;241m=\u001b[39m \u001b[43mmeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinetunning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_spt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_spt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_qry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_qry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     loss \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m loss]\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta test: Epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: Loss \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m,acc \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch,loss,accs))\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mMeta.finetunning\u001b[0;34m(self, x_spt, y_spt, x_qry, y_qry)\u001b[0m\n\u001b[1;32m    122\u001b[0m net \u001b[38;5;241m=\u001b[39m deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# 1. run the i-th task and compute loss for k=0\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_spt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, y_spt)\n\u001b[1;32m    127\u001b[0m grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(loss, net\u001b[38;5;241m.\u001b[39mparameters())\n",
      "File \u001b[0;32m~/miniconda3/envs/metagan/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mLearner.forward\u001b[0;34m(self, x, vars, bn_training)\u001b[0m\n\u001b[1;32m    134\u001b[0m w, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mvars\u001b[39m[idx], \u001b[38;5;28mvars\u001b[39m[idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# remember to keep synchrozied of forward_encoder and forward_decoder!\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# print(name, param, '\\tout:', x.shape)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "epoch = 0\n",
    "meta =Meta(args,config).to(device)\n",
    "for _ in range(args[\"epoch\"] // 10000):     \n",
    "\n",
    "    train_dataloader = DataLoader(train_data_generator, args[\"tasks_per_batch\"], shuffle=True, num_workers=1, pin_memory=True)\n",
    "    test_dataloader = iter(DataLoader(test_data_generator, args[\"tasks_per_batch\"], shuffle=True, num_workers=1, pin_memory=True))\n",
    "    for _, (x_spt, y_spt, x_qry, y_qry) in tqdm.tqdm(enumerate(train_dataloader)):\n",
    "        x_spt, y_spt, x_qry, y_qry = x_spt.to(device), y_spt.to(device), x_qry.to(device), y_qry.to(device)\n",
    "        accs,loss = meta(x_spt, y_spt, x_qry, y_qry)\n",
    "        loss = [x.item() for x in loss]\n",
    "        epoch = epoch + 1\n",
    "        \n",
    "        if epoch%100==0:\n",
    "            print(\"Epoch {}: Loss {},acc {}\\n\".format(epoch,loss,accs))\n",
    "            print('Updated Model Parameter Theta\\n')\n",
    "            print('Sampling Next Batch of Tasks \\n')\n",
    "            print('---------------------------------\\n')\n",
    "        \n",
    "        if (epoch % 500 == 0):\n",
    "            x_spt, y_spt, x_qry, y_qry = next(test_dataloader)\n",
    "            accs,loss = meta.finetunning(x_spt[0], y_spt[0], x_qry[0], y_qry[0])\n",
    "            loss = [x.item() for x in loss]\n",
    "            print(\"meta test: Epoch {}: Loss {},acc {}\\n\".format(epoch,loss,accs))\n",
    "            print('Updated Model Parameter Theta\\n')\n",
    "            print('Sampling Next Batch of Tasks \\n')\n",
    "            print('---------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae41178-5688-4270-b195-66840d0685d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metagan",
   "language": "python",
   "name": "metagan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
